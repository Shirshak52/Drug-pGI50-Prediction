{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8bd2555-5c6e-4a73-be36-c637e121dbde",
   "metadata": {},
   "source": [
    "# Initial Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba522b06-3bb8-4944-88b5-ec728a3dd895",
   "metadata": {},
   "source": [
    "## Setup Environment and Project Path Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cfbd48-ceb7-4107-a3e7-720588151435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch threads: 16\n",
      "PyTorch interop threads: 16\n",
      "Project root added to sys.path: C:\\Users\\Acer\\Desktop\\Projects for Data Science\\Drug Gi50 Value Prediction\n"
     ]
    }
   ],
   "source": [
    "# General CPU Usage Optimization\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '16'\n",
    "os.environ['MKL_NUM_THREADS'] = '16'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '16'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '16'\n",
    "\n",
    "import time\n",
    "\n",
    "# PyTorch-specific CPU Usage Optimization\n",
    "import torch\n",
    "try:\n",
    "    torch.set_num_threads(16)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: Could not set torch.set_num_threads.\\n{e}\")\n",
    "\n",
    "try:\n",
    "    torch.set_num_interop_threads(16)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: Could not set torch.set_num_interop_threads.\\n{e}\")\n",
    "\n",
    "print(f\"PyTorch threads: {torch.get_num_threads()}\")\n",
    "print(f\"PyTorch interop threads: {torch.get_num_interop_threads()}\")\n",
    "\n",
    "# Configure Project Path for Module Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate up to the project root directory\n",
    "project_root = Path(current_dir).parent.resolve()\n",
    "\n",
    "# Add the project root to sys.path if it's not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root added to sys.path: {project_root}\")\n",
    "\n",
    "# General Utility for Timestamps\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498e4ed-618b-45c9-ab02-79404947c941",
   "metadata": {},
   "source": [
    "## Import Core Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "607b77a9-8ab7-4dde-b15c-cae9a6a0e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem  # For basic molecule handling\n",
    "from rdkit.Chem import AllChem  # For atom features like Gasteiger charges, and other utilities\n",
    "\n",
    "# PyTorch Core for Neural Networks\n",
    "import torch.nn as nn  # Neural network modules like Linear, ReLU, MSELoss\n",
    "import torch.nn.functional as F  # Functional interface for activations, e.g. F.ReLU\n",
    "import torch.optim as optim  # Optimization functions like Adam, AdamW, etc.\n",
    "from torch.optim import lr_scheduler  # Learning rate scheduling\n",
    "\n",
    "# PyTorch Geometric for Graph Neural Networks\n",
    "from torch_geometric.data import Data # The graph data object in PyG\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader # PyG DataLoader for graphs\n",
    "import torch_geometric.nn as pyg_nn # Common GNN layers (e.g., GCNConv, GraphSAGEConv)\n",
    "import torch_geometric.utils as pyg_utils # Utility functions for graph manipulation\n",
    "\n",
    "# GNN Model Class\n",
    "from src.models.gnn_models import GNN\n",
    "\n",
    "# Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Mixed Precision Training (for GPU-accelerated training)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Git commit ID for final model filename (for reproducibility)\n",
    "import subprocess\n",
    "\n",
    "# For graph visualization (optional)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73369f-d2e5-42ad-98e1-5d1cfba0d789",
   "metadata": {},
   "source": [
    "## Import Utility Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16ce2845-e6d6-4546-929f-568de3729445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm.notebook found and enabled for pandas.\n"
     ]
    }
   ],
   "source": [
    "# Progress bars\n",
    "tqdm_notebook_available = False\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "    tqdm.pandas() # Enable tqdm for pandas apply\n",
    "    tqdm_notebook_available = True\n",
    "    print(\"tqdm.notebook found and enabled for pandas.\")\n",
    "except ImportError:\n",
    "    print(\"tqdm.notebook not found. Install with 'pip install tqdm'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afdaa5-b8f4-4c1c-9231-edf19fbc344c",
   "metadata": {},
   "source": [
    "## Define Device (GPU/CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "385334b2-5c91-4353-a650-b5e6c9b74825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939c1c3-4217-4c59-837e-0f59067088b6",
   "metadata": {},
   "source": [
    "## Set Final Model Save Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd70cd4-d396-4621-a342-7d6876c6f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best final GNN model will be saved in: ..\\models\\gnn\n"
     ]
    }
   ],
   "source": [
    "gnn_models_base_dir = Path(\"../models/gnn\")\n",
    "gnn_models_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"The best final GNN model will be saved in: {gnn_models_base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af314e-5d67-4625-a656-cfb57a00d6e4",
   "metadata": {},
   "source": [
    "# Load Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d4c581-37f9-42ef-a421-e67646669758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data splits from ..\\data\\splits...\n",
      "Data splits loaded successfully.\n",
      "X_train shape: (13119, 2268)\n",
      "X_val shape: (2812, 2268)\n",
      "X_test shape: (2812, 2268)\n",
      "y_train shape: (13119, 1)\n",
      "y_val shape: (2812, 1)\n",
      "y_test shape: (2812, 1)\n",
      "\n",
      "First 5 rows of X_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molregno</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307646</td>\n",
       "      <td>COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C</td>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081122</td>\n",
       "      <td>COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC</td>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2199496</td>\n",
       "      <td>COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2221960</td>\n",
       "      <td>O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO</td>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2879093</td>\n",
       "      <td>Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...</td>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   molregno                                   canonical_smiles  \\\n",
       "0   2307646               COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C   \n",
       "1   2081122       COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC   \n",
       "2   2199496  COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...   \n",
       "3   2221960           O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO   \n",
       "4   2879093  Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...   \n",
       "\n",
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  ...  morgan_fp_2038  \\\n",
       "0        0.494176  0.476742  12.560000  328.371  ...               0   \n",
       "1        0.459195  0.604738  12.923077  353.374  ...               0   \n",
       "2       -0.173158  0.359463  15.909091  447.535  ...               0   \n",
       "3       -0.686457  0.479732  11.217391  375.222  ...               0   \n",
       "4       -3.116139  0.437556  16.121212  472.879  ...               0   \n",
       "\n",
       "   morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  morgan_fp_2042  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  morgan_fp_2046  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2047  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 2268 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of y_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pGI50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>5.734742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>7.164746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>4.928428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>6.882724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>6.094208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pGI50\n",
       "14387  5.734742\n",
       "12543  7.164746\n",
       "12810  4.928428\n",
       "13172  6.882724\n",
       "18712  6.094208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits_dir = Path(\"../data/splits\")\n",
    "print(f\"\\nLoading data splits from {splits_dir}...\")\n",
    "\n",
    "try:\n",
    "    X_train = pd.read_parquet(splits_dir / \"X_train.parquet\")\n",
    "    X_val = pd.read_parquet(splits_dir / \"X_val.parquet\")\n",
    "    X_test = pd.read_parquet(splits_dir / \"X_test.parquet\")\n",
    "    \n",
    "    y_train = pd.read_parquet(splits_dir / \"y_train.parquet\")\n",
    "    y_val = pd.read_parquet(splits_dir / \"y_val.parquet\")\n",
    "    y_test = pd.read_parquet(splits_dir / \"y_test.parquet\")\n",
    "    print(\"Data splits loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or more split files not found in '{splits_dir}'.\")\n",
    "    print(\"Please ensure you have run '02_Split_Features.ipynb' to generate and save the splits.\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Display first few rows to verify data\n",
    "print(\"\\nFirst 5 rows of X_train:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of y_train:\")\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c86cd-f9d1-4b3f-ac55-99cae504d03a",
   "metadata": {},
   "source": [
    "# Prepare Data for GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf767878-0371-4b4e-acb6-63085a6ef138",
   "metadata": {},
   "source": [
    "## Extract Global Features of Each Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4eecff-86f8-4cdc-b34d-d67ac68c7702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Excracting global features of each molecule ---\n",
      "Identified 2266 global feature columns for GNN.\n",
      "Global feature columns: ['num_activities', 'MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAmideBonds', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumAtomStereoCenters', 'NumBridgeheadAtoms', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumHeterocycles', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'NumSpiroAtoms', 'NumUnspecifiedAtomStereoCenters', 'Phi', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea', 'morgan_fp_0', 'morgan_fp_1', 'morgan_fp_2', 'morgan_fp_3', 'morgan_fp_4', 'morgan_fp_5', 'morgan_fp_6', 'morgan_fp_7', 'morgan_fp_8', 'morgan_fp_9', 'morgan_fp_10', 'morgan_fp_11', 'morgan_fp_12', 'morgan_fp_13', 'morgan_fp_14', 'morgan_fp_15', 'morgan_fp_16', 'morgan_fp_17', 'morgan_fp_18', 'morgan_fp_19', 'morgan_fp_20', 'morgan_fp_21', 'morgan_fp_22', 'morgan_fp_23', 'morgan_fp_24', 'morgan_fp_25', 'morgan_fp_26', 'morgan_fp_27', 'morgan_fp_28', 'morgan_fp_29', 'morgan_fp_30', 'morgan_fp_31', 'morgan_fp_32', 'morgan_fp_33', 'morgan_fp_34', 'morgan_fp_35', 'morgan_fp_36', 'morgan_fp_37', 'morgan_fp_38', 'morgan_fp_39', 'morgan_fp_40', 'morgan_fp_41', 'morgan_fp_42', 'morgan_fp_43', 'morgan_fp_44', 'morgan_fp_45', 'morgan_fp_46', 'morgan_fp_47', 'morgan_fp_48', 'morgan_fp_49', 'morgan_fp_50', 'morgan_fp_51', 'morgan_fp_52', 'morgan_fp_53', 'morgan_fp_54', 'morgan_fp_55', 'morgan_fp_56', 'morgan_fp_57', 'morgan_fp_58', 'morgan_fp_59', 'morgan_fp_60', 'morgan_fp_61', 'morgan_fp_62', 'morgan_fp_63', 'morgan_fp_64', 'morgan_fp_65', 'morgan_fp_66', 'morgan_fp_67', 'morgan_fp_68', 'morgan_fp_69', 'morgan_fp_70', 'morgan_fp_71', 'morgan_fp_72', 'morgan_fp_73', 'morgan_fp_74', 'morgan_fp_75', 'morgan_fp_76', 'morgan_fp_77', 'morgan_fp_78', 'morgan_fp_79', 'morgan_fp_80', 'morgan_fp_81', 'morgan_fp_82', 'morgan_fp_83', 'morgan_fp_84', 'morgan_fp_85', 'morgan_fp_86', 'morgan_fp_87', 'morgan_fp_88', 'morgan_fp_89', 'morgan_fp_90', 'morgan_fp_91', 'morgan_fp_92', 'morgan_fp_93', 'morgan_fp_94', 'morgan_fp_95', 'morgan_fp_96', 'morgan_fp_97', 'morgan_fp_98', 'morgan_fp_99', 'morgan_fp_100', 'morgan_fp_101', 'morgan_fp_102', 'morgan_fp_103', 'morgan_fp_104', 'morgan_fp_105', 'morgan_fp_106', 'morgan_fp_107', 'morgan_fp_108', 'morgan_fp_109', 'morgan_fp_110', 'morgan_fp_111', 'morgan_fp_112', 'morgan_fp_113', 'morgan_fp_114', 'morgan_fp_115', 'morgan_fp_116', 'morgan_fp_117', 'morgan_fp_118', 'morgan_fp_119', 'morgan_fp_120', 'morgan_fp_121', 'morgan_fp_122', 'morgan_fp_123', 'morgan_fp_124', 'morgan_fp_125', 'morgan_fp_126', 'morgan_fp_127', 'morgan_fp_128', 'morgan_fp_129', 'morgan_fp_130', 'morgan_fp_131', 'morgan_fp_132', 'morgan_fp_133', 'morgan_fp_134', 'morgan_fp_135', 'morgan_fp_136', 'morgan_fp_137', 'morgan_fp_138', 'morgan_fp_139', 'morgan_fp_140', 'morgan_fp_141', 'morgan_fp_142', 'morgan_fp_143', 'morgan_fp_144', 'morgan_fp_145', 'morgan_fp_146', 'morgan_fp_147', 'morgan_fp_148', 'morgan_fp_149', 'morgan_fp_150', 'morgan_fp_151', 'morgan_fp_152', 'morgan_fp_153', 'morgan_fp_154', 'morgan_fp_155', 'morgan_fp_156', 'morgan_fp_157', 'morgan_fp_158', 'morgan_fp_159', 'morgan_fp_160', 'morgan_fp_161', 'morgan_fp_162', 'morgan_fp_163', 'morgan_fp_164', 'morgan_fp_165', 'morgan_fp_166', 'morgan_fp_167', 'morgan_fp_168', 'morgan_fp_169', 'morgan_fp_170', 'morgan_fp_171', 'morgan_fp_172', 'morgan_fp_173', 'morgan_fp_174', 'morgan_fp_175', 'morgan_fp_176', 'morgan_fp_177', 'morgan_fp_178', 'morgan_fp_179', 'morgan_fp_180', 'morgan_fp_181', 'morgan_fp_182', 'morgan_fp_183', 'morgan_fp_184', 'morgan_fp_185', 'morgan_fp_186', 'morgan_fp_187', 'morgan_fp_188', 'morgan_fp_189', 'morgan_fp_190', 'morgan_fp_191', 'morgan_fp_192', 'morgan_fp_193', 'morgan_fp_194', 'morgan_fp_195', 'morgan_fp_196', 'morgan_fp_197', 'morgan_fp_198', 'morgan_fp_199', 'morgan_fp_200', 'morgan_fp_201', 'morgan_fp_202', 'morgan_fp_203', 'morgan_fp_204', 'morgan_fp_205', 'morgan_fp_206', 'morgan_fp_207', 'morgan_fp_208', 'morgan_fp_209', 'morgan_fp_210', 'morgan_fp_211', 'morgan_fp_212', 'morgan_fp_213', 'morgan_fp_214', 'morgan_fp_215', 'morgan_fp_216', 'morgan_fp_217', 'morgan_fp_218', 'morgan_fp_219', 'morgan_fp_220', 'morgan_fp_221', 'morgan_fp_222', 'morgan_fp_223', 'morgan_fp_224', 'morgan_fp_225', 'morgan_fp_226', 'morgan_fp_227', 'morgan_fp_228', 'morgan_fp_229', 'morgan_fp_230', 'morgan_fp_231', 'morgan_fp_232', 'morgan_fp_233', 'morgan_fp_234', 'morgan_fp_235', 'morgan_fp_236', 'morgan_fp_237', 'morgan_fp_238', 'morgan_fp_239', 'morgan_fp_240', 'morgan_fp_241', 'morgan_fp_242', 'morgan_fp_243', 'morgan_fp_244', 'morgan_fp_245', 'morgan_fp_246', 'morgan_fp_247', 'morgan_fp_248', 'morgan_fp_249', 'morgan_fp_250', 'morgan_fp_251', 'morgan_fp_252', 'morgan_fp_253', 'morgan_fp_254', 'morgan_fp_255', 'morgan_fp_256', 'morgan_fp_257', 'morgan_fp_258', 'morgan_fp_259', 'morgan_fp_260', 'morgan_fp_261', 'morgan_fp_262', 'morgan_fp_263', 'morgan_fp_264', 'morgan_fp_265', 'morgan_fp_266', 'morgan_fp_267', 'morgan_fp_268', 'morgan_fp_269', 'morgan_fp_270', 'morgan_fp_271', 'morgan_fp_272', 'morgan_fp_273', 'morgan_fp_274', 'morgan_fp_275', 'morgan_fp_276', 'morgan_fp_277', 'morgan_fp_278', 'morgan_fp_279', 'morgan_fp_280', 'morgan_fp_281', 'morgan_fp_282', 'morgan_fp_283', 'morgan_fp_284', 'morgan_fp_285', 'morgan_fp_286', 'morgan_fp_287', 'morgan_fp_288', 'morgan_fp_289', 'morgan_fp_290', 'morgan_fp_291', 'morgan_fp_292', 'morgan_fp_293', 'morgan_fp_294', 'morgan_fp_295', 'morgan_fp_296', 'morgan_fp_297', 'morgan_fp_298', 'morgan_fp_299', 'morgan_fp_300', 'morgan_fp_301', 'morgan_fp_302', 'morgan_fp_303', 'morgan_fp_304', 'morgan_fp_305', 'morgan_fp_306', 'morgan_fp_307', 'morgan_fp_308', 'morgan_fp_309', 'morgan_fp_310', 'morgan_fp_311', 'morgan_fp_312', 'morgan_fp_313', 'morgan_fp_314', 'morgan_fp_315', 'morgan_fp_316', 'morgan_fp_317', 'morgan_fp_318', 'morgan_fp_319', 'morgan_fp_320', 'morgan_fp_321', 'morgan_fp_322', 'morgan_fp_323', 'morgan_fp_324', 'morgan_fp_325', 'morgan_fp_326', 'morgan_fp_327', 'morgan_fp_328', 'morgan_fp_329', 'morgan_fp_330', 'morgan_fp_331', 'morgan_fp_332', 'morgan_fp_333', 'morgan_fp_334', 'morgan_fp_335', 'morgan_fp_336', 'morgan_fp_337', 'morgan_fp_338', 'morgan_fp_339', 'morgan_fp_340', 'morgan_fp_341', 'morgan_fp_342', 'morgan_fp_343', 'morgan_fp_344', 'morgan_fp_345', 'morgan_fp_346', 'morgan_fp_347', 'morgan_fp_348', 'morgan_fp_349', 'morgan_fp_350', 'morgan_fp_351', 'morgan_fp_352', 'morgan_fp_353', 'morgan_fp_354', 'morgan_fp_355', 'morgan_fp_356', 'morgan_fp_357', 'morgan_fp_358', 'morgan_fp_359', 'morgan_fp_360', 'morgan_fp_361', 'morgan_fp_362', 'morgan_fp_363', 'morgan_fp_364', 'morgan_fp_365', 'morgan_fp_366', 'morgan_fp_367', 'morgan_fp_368', 'morgan_fp_369', 'morgan_fp_370', 'morgan_fp_371', 'morgan_fp_372', 'morgan_fp_373', 'morgan_fp_374', 'morgan_fp_375', 'morgan_fp_376', 'morgan_fp_377', 'morgan_fp_378', 'morgan_fp_379', 'morgan_fp_380', 'morgan_fp_381', 'morgan_fp_382', 'morgan_fp_383', 'morgan_fp_384', 'morgan_fp_385', 'morgan_fp_386', 'morgan_fp_387', 'morgan_fp_388', 'morgan_fp_389', 'morgan_fp_390', 'morgan_fp_391', 'morgan_fp_392', 'morgan_fp_393', 'morgan_fp_394', 'morgan_fp_395', 'morgan_fp_396', 'morgan_fp_397', 'morgan_fp_398', 'morgan_fp_399', 'morgan_fp_400', 'morgan_fp_401', 'morgan_fp_402', 'morgan_fp_403', 'morgan_fp_404', 'morgan_fp_405', 'morgan_fp_406', 'morgan_fp_407', 'morgan_fp_408', 'morgan_fp_409', 'morgan_fp_410', 'morgan_fp_411', 'morgan_fp_412', 'morgan_fp_413', 'morgan_fp_414', 'morgan_fp_415', 'morgan_fp_416', 'morgan_fp_417', 'morgan_fp_418', 'morgan_fp_419', 'morgan_fp_420', 'morgan_fp_421', 'morgan_fp_422', 'morgan_fp_423', 'morgan_fp_424', 'morgan_fp_425', 'morgan_fp_426', 'morgan_fp_427', 'morgan_fp_428', 'morgan_fp_429', 'morgan_fp_430', 'morgan_fp_431', 'morgan_fp_432', 'morgan_fp_433', 'morgan_fp_434', 'morgan_fp_435', 'morgan_fp_436', 'morgan_fp_437', 'morgan_fp_438', 'morgan_fp_439', 'morgan_fp_440', 'morgan_fp_441', 'morgan_fp_442', 'morgan_fp_443', 'morgan_fp_444', 'morgan_fp_445', 'morgan_fp_446', 'morgan_fp_447', 'morgan_fp_448', 'morgan_fp_449', 'morgan_fp_450', 'morgan_fp_451', 'morgan_fp_452', 'morgan_fp_453', 'morgan_fp_454', 'morgan_fp_455', 'morgan_fp_456', 'morgan_fp_457', 'morgan_fp_458', 'morgan_fp_459', 'morgan_fp_460', 'morgan_fp_461', 'morgan_fp_462', 'morgan_fp_463', 'morgan_fp_464', 'morgan_fp_465', 'morgan_fp_466', 'morgan_fp_467', 'morgan_fp_468', 'morgan_fp_469', 'morgan_fp_470', 'morgan_fp_471', 'morgan_fp_472', 'morgan_fp_473', 'morgan_fp_474', 'morgan_fp_475', 'morgan_fp_476', 'morgan_fp_477', 'morgan_fp_478', 'morgan_fp_479', 'morgan_fp_480', 'morgan_fp_481', 'morgan_fp_482', 'morgan_fp_483', 'morgan_fp_484', 'morgan_fp_485', 'morgan_fp_486', 'morgan_fp_487', 'morgan_fp_488', 'morgan_fp_489', 'morgan_fp_490', 'morgan_fp_491', 'morgan_fp_492', 'morgan_fp_493', 'morgan_fp_494', 'morgan_fp_495', 'morgan_fp_496', 'morgan_fp_497', 'morgan_fp_498', 'morgan_fp_499', 'morgan_fp_500', 'morgan_fp_501', 'morgan_fp_502', 'morgan_fp_503', 'morgan_fp_504', 'morgan_fp_505', 'morgan_fp_506', 'morgan_fp_507', 'morgan_fp_508', 'morgan_fp_509', 'morgan_fp_510', 'morgan_fp_511', 'morgan_fp_512', 'morgan_fp_513', 'morgan_fp_514', 'morgan_fp_515', 'morgan_fp_516', 'morgan_fp_517', 'morgan_fp_518', 'morgan_fp_519', 'morgan_fp_520', 'morgan_fp_521', 'morgan_fp_522', 'morgan_fp_523', 'morgan_fp_524', 'morgan_fp_525', 'morgan_fp_526', 'morgan_fp_527', 'morgan_fp_528', 'morgan_fp_529', 'morgan_fp_530', 'morgan_fp_531', 'morgan_fp_532', 'morgan_fp_533', 'morgan_fp_534', 'morgan_fp_535', 'morgan_fp_536', 'morgan_fp_537', 'morgan_fp_538', 'morgan_fp_539', 'morgan_fp_540', 'morgan_fp_541', 'morgan_fp_542', 'morgan_fp_543', 'morgan_fp_544', 'morgan_fp_545', 'morgan_fp_546', 'morgan_fp_547', 'morgan_fp_548', 'morgan_fp_549', 'morgan_fp_550', 'morgan_fp_551', 'morgan_fp_552', 'morgan_fp_553', 'morgan_fp_554', 'morgan_fp_555', 'morgan_fp_556', 'morgan_fp_557', 'morgan_fp_558', 'morgan_fp_559', 'morgan_fp_560', 'morgan_fp_561', 'morgan_fp_562', 'morgan_fp_563', 'morgan_fp_564', 'morgan_fp_565', 'morgan_fp_566', 'morgan_fp_567', 'morgan_fp_568', 'morgan_fp_569', 'morgan_fp_570', 'morgan_fp_571', 'morgan_fp_572', 'morgan_fp_573', 'morgan_fp_574', 'morgan_fp_575', 'morgan_fp_576', 'morgan_fp_577', 'morgan_fp_578', 'morgan_fp_579', 'morgan_fp_580', 'morgan_fp_581', 'morgan_fp_582', 'morgan_fp_583', 'morgan_fp_584', 'morgan_fp_585', 'morgan_fp_586', 'morgan_fp_587', 'morgan_fp_588', 'morgan_fp_589', 'morgan_fp_590', 'morgan_fp_591', 'morgan_fp_592', 'morgan_fp_593', 'morgan_fp_594', 'morgan_fp_595', 'morgan_fp_596', 'morgan_fp_597', 'morgan_fp_598', 'morgan_fp_599', 'morgan_fp_600', 'morgan_fp_601', 'morgan_fp_602', 'morgan_fp_603', 'morgan_fp_604', 'morgan_fp_605', 'morgan_fp_606', 'morgan_fp_607', 'morgan_fp_608', 'morgan_fp_609', 'morgan_fp_610', 'morgan_fp_611', 'morgan_fp_612', 'morgan_fp_613', 'morgan_fp_614', 'morgan_fp_615', 'morgan_fp_616', 'morgan_fp_617', 'morgan_fp_618', 'morgan_fp_619', 'morgan_fp_620', 'morgan_fp_621', 'morgan_fp_622', 'morgan_fp_623', 'morgan_fp_624', 'morgan_fp_625', 'morgan_fp_626', 'morgan_fp_627', 'morgan_fp_628', 'morgan_fp_629', 'morgan_fp_630', 'morgan_fp_631', 'morgan_fp_632', 'morgan_fp_633', 'morgan_fp_634', 'morgan_fp_635', 'morgan_fp_636', 'morgan_fp_637', 'morgan_fp_638', 'morgan_fp_639', 'morgan_fp_640', 'morgan_fp_641', 'morgan_fp_642', 'morgan_fp_643', 'morgan_fp_644', 'morgan_fp_645', 'morgan_fp_646', 'morgan_fp_647', 'morgan_fp_648', 'morgan_fp_649', 'morgan_fp_650', 'morgan_fp_651', 'morgan_fp_652', 'morgan_fp_653', 'morgan_fp_654', 'morgan_fp_655', 'morgan_fp_656', 'morgan_fp_657', 'morgan_fp_658', 'morgan_fp_659', 'morgan_fp_660', 'morgan_fp_661', 'morgan_fp_662', 'morgan_fp_663', 'morgan_fp_664', 'morgan_fp_665', 'morgan_fp_666', 'morgan_fp_667', 'morgan_fp_668', 'morgan_fp_669', 'morgan_fp_670', 'morgan_fp_671', 'morgan_fp_672', 'morgan_fp_673', 'morgan_fp_674', 'morgan_fp_675', 'morgan_fp_676', 'morgan_fp_677', 'morgan_fp_678', 'morgan_fp_679', 'morgan_fp_680', 'morgan_fp_681', 'morgan_fp_682', 'morgan_fp_683', 'morgan_fp_684', 'morgan_fp_685', 'morgan_fp_686', 'morgan_fp_687', 'morgan_fp_688', 'morgan_fp_689', 'morgan_fp_690', 'morgan_fp_691', 'morgan_fp_692', 'morgan_fp_693', 'morgan_fp_694', 'morgan_fp_695', 'morgan_fp_696', 'morgan_fp_697', 'morgan_fp_698', 'morgan_fp_699', 'morgan_fp_700', 'morgan_fp_701', 'morgan_fp_702', 'morgan_fp_703', 'morgan_fp_704', 'morgan_fp_705', 'morgan_fp_706', 'morgan_fp_707', 'morgan_fp_708', 'morgan_fp_709', 'morgan_fp_710', 'morgan_fp_711', 'morgan_fp_712', 'morgan_fp_713', 'morgan_fp_714', 'morgan_fp_715', 'morgan_fp_716', 'morgan_fp_717', 'morgan_fp_718', 'morgan_fp_719', 'morgan_fp_720', 'morgan_fp_721', 'morgan_fp_722', 'morgan_fp_723', 'morgan_fp_724', 'morgan_fp_725', 'morgan_fp_726', 'morgan_fp_727', 'morgan_fp_728', 'morgan_fp_729', 'morgan_fp_730', 'morgan_fp_731', 'morgan_fp_732', 'morgan_fp_733', 'morgan_fp_734', 'morgan_fp_735', 'morgan_fp_736', 'morgan_fp_737', 'morgan_fp_738', 'morgan_fp_739', 'morgan_fp_740', 'morgan_fp_741', 'morgan_fp_742', 'morgan_fp_743', 'morgan_fp_744', 'morgan_fp_745', 'morgan_fp_746', 'morgan_fp_747', 'morgan_fp_748', 'morgan_fp_749', 'morgan_fp_750', 'morgan_fp_751', 'morgan_fp_752', 'morgan_fp_753', 'morgan_fp_754', 'morgan_fp_755', 'morgan_fp_756', 'morgan_fp_757', 'morgan_fp_758', 'morgan_fp_759', 'morgan_fp_760', 'morgan_fp_761', 'morgan_fp_762', 'morgan_fp_763', 'morgan_fp_764', 'morgan_fp_765', 'morgan_fp_766', 'morgan_fp_767', 'morgan_fp_768', 'morgan_fp_769', 'morgan_fp_770', 'morgan_fp_771', 'morgan_fp_772', 'morgan_fp_773', 'morgan_fp_774', 'morgan_fp_775', 'morgan_fp_776', 'morgan_fp_777', 'morgan_fp_778', 'morgan_fp_779', 'morgan_fp_780', 'morgan_fp_781', 'morgan_fp_782', 'morgan_fp_783', 'morgan_fp_784', 'morgan_fp_785', 'morgan_fp_786', 'morgan_fp_787', 'morgan_fp_788', 'morgan_fp_789', 'morgan_fp_790', 'morgan_fp_791', 'morgan_fp_792', 'morgan_fp_793', 'morgan_fp_794', 'morgan_fp_795', 'morgan_fp_796', 'morgan_fp_797', 'morgan_fp_798', 'morgan_fp_799', 'morgan_fp_800', 'morgan_fp_801', 'morgan_fp_802', 'morgan_fp_803', 'morgan_fp_804', 'morgan_fp_805', 'morgan_fp_806', 'morgan_fp_807', 'morgan_fp_808', 'morgan_fp_809', 'morgan_fp_810', 'morgan_fp_811', 'morgan_fp_812', 'morgan_fp_813', 'morgan_fp_814', 'morgan_fp_815', 'morgan_fp_816', 'morgan_fp_817', 'morgan_fp_818', 'morgan_fp_819', 'morgan_fp_820', 'morgan_fp_821', 'morgan_fp_822', 'morgan_fp_823', 'morgan_fp_824', 'morgan_fp_825', 'morgan_fp_826', 'morgan_fp_827', 'morgan_fp_828', 'morgan_fp_829', 'morgan_fp_830', 'morgan_fp_831', 'morgan_fp_832', 'morgan_fp_833', 'morgan_fp_834', 'morgan_fp_835', 'morgan_fp_836', 'morgan_fp_837', 'morgan_fp_838', 'morgan_fp_839', 'morgan_fp_840', 'morgan_fp_841', 'morgan_fp_842', 'morgan_fp_843', 'morgan_fp_844', 'morgan_fp_845', 'morgan_fp_846', 'morgan_fp_847', 'morgan_fp_848', 'morgan_fp_849', 'morgan_fp_850', 'morgan_fp_851', 'morgan_fp_852', 'morgan_fp_853', 'morgan_fp_854', 'morgan_fp_855', 'morgan_fp_856', 'morgan_fp_857', 'morgan_fp_858', 'morgan_fp_859', 'morgan_fp_860', 'morgan_fp_861', 'morgan_fp_862', 'morgan_fp_863', 'morgan_fp_864', 'morgan_fp_865', 'morgan_fp_866', 'morgan_fp_867', 'morgan_fp_868', 'morgan_fp_869', 'morgan_fp_870', 'morgan_fp_871', 'morgan_fp_872', 'morgan_fp_873', 'morgan_fp_874', 'morgan_fp_875', 'morgan_fp_876', 'morgan_fp_877', 'morgan_fp_878', 'morgan_fp_879', 'morgan_fp_880', 'morgan_fp_881', 'morgan_fp_882', 'morgan_fp_883', 'morgan_fp_884', 'morgan_fp_885', 'morgan_fp_886', 'morgan_fp_887', 'morgan_fp_888', 'morgan_fp_889', 'morgan_fp_890', 'morgan_fp_891', 'morgan_fp_892', 'morgan_fp_893', 'morgan_fp_894', 'morgan_fp_895', 'morgan_fp_896', 'morgan_fp_897', 'morgan_fp_898', 'morgan_fp_899', 'morgan_fp_900', 'morgan_fp_901', 'morgan_fp_902', 'morgan_fp_903', 'morgan_fp_904', 'morgan_fp_905', 'morgan_fp_906', 'morgan_fp_907', 'morgan_fp_908', 'morgan_fp_909', 'morgan_fp_910', 'morgan_fp_911', 'morgan_fp_912', 'morgan_fp_913', 'morgan_fp_914', 'morgan_fp_915', 'morgan_fp_916', 'morgan_fp_917', 'morgan_fp_918', 'morgan_fp_919', 'morgan_fp_920', 'morgan_fp_921', 'morgan_fp_922', 'morgan_fp_923', 'morgan_fp_924', 'morgan_fp_925', 'morgan_fp_926', 'morgan_fp_927', 'morgan_fp_928', 'morgan_fp_929', 'morgan_fp_930', 'morgan_fp_931', 'morgan_fp_932', 'morgan_fp_933', 'morgan_fp_934', 'morgan_fp_935', 'morgan_fp_936', 'morgan_fp_937', 'morgan_fp_938', 'morgan_fp_939', 'morgan_fp_940', 'morgan_fp_941', 'morgan_fp_942', 'morgan_fp_943', 'morgan_fp_944', 'morgan_fp_945', 'morgan_fp_946', 'morgan_fp_947', 'morgan_fp_948', 'morgan_fp_949', 'morgan_fp_950', 'morgan_fp_951', 'morgan_fp_952', 'morgan_fp_953', 'morgan_fp_954', 'morgan_fp_955', 'morgan_fp_956', 'morgan_fp_957', 'morgan_fp_958', 'morgan_fp_959', 'morgan_fp_960', 'morgan_fp_961', 'morgan_fp_962', 'morgan_fp_963', 'morgan_fp_964', 'morgan_fp_965', 'morgan_fp_966', 'morgan_fp_967', 'morgan_fp_968', 'morgan_fp_969', 'morgan_fp_970', 'morgan_fp_971', 'morgan_fp_972', 'morgan_fp_973', 'morgan_fp_974', 'morgan_fp_975', 'morgan_fp_976', 'morgan_fp_977', 'morgan_fp_978', 'morgan_fp_979', 'morgan_fp_980', 'morgan_fp_981', 'morgan_fp_982', 'morgan_fp_983', 'morgan_fp_984', 'morgan_fp_985', 'morgan_fp_986', 'morgan_fp_987', 'morgan_fp_988', 'morgan_fp_989', 'morgan_fp_990', 'morgan_fp_991', 'morgan_fp_992', 'morgan_fp_993', 'morgan_fp_994', 'morgan_fp_995', 'morgan_fp_996', 'morgan_fp_997', 'morgan_fp_998', 'morgan_fp_999', 'morgan_fp_1000', 'morgan_fp_1001', 'morgan_fp_1002', 'morgan_fp_1003', 'morgan_fp_1004', 'morgan_fp_1005', 'morgan_fp_1006', 'morgan_fp_1007', 'morgan_fp_1008', 'morgan_fp_1009', 'morgan_fp_1010', 'morgan_fp_1011', 'morgan_fp_1012', 'morgan_fp_1013', 'morgan_fp_1014', 'morgan_fp_1015', 'morgan_fp_1016', 'morgan_fp_1017', 'morgan_fp_1018', 'morgan_fp_1019', 'morgan_fp_1020', 'morgan_fp_1021', 'morgan_fp_1022', 'morgan_fp_1023', 'morgan_fp_1024', 'morgan_fp_1025', 'morgan_fp_1026', 'morgan_fp_1027', 'morgan_fp_1028', 'morgan_fp_1029', 'morgan_fp_1030', 'morgan_fp_1031', 'morgan_fp_1032', 'morgan_fp_1033', 'morgan_fp_1034', 'morgan_fp_1035', 'morgan_fp_1036', 'morgan_fp_1037', 'morgan_fp_1038', 'morgan_fp_1039', 'morgan_fp_1040', 'morgan_fp_1041', 'morgan_fp_1042', 'morgan_fp_1043', 'morgan_fp_1044', 'morgan_fp_1045', 'morgan_fp_1046', 'morgan_fp_1047', 'morgan_fp_1048', 'morgan_fp_1049', 'morgan_fp_1050', 'morgan_fp_1051', 'morgan_fp_1052', 'morgan_fp_1053', 'morgan_fp_1054', 'morgan_fp_1055', 'morgan_fp_1056', 'morgan_fp_1057', 'morgan_fp_1058', 'morgan_fp_1059', 'morgan_fp_1060', 'morgan_fp_1061', 'morgan_fp_1062', 'morgan_fp_1063', 'morgan_fp_1064', 'morgan_fp_1065', 'morgan_fp_1066', 'morgan_fp_1067', 'morgan_fp_1068', 'morgan_fp_1069', 'morgan_fp_1070', 'morgan_fp_1071', 'morgan_fp_1072', 'morgan_fp_1073', 'morgan_fp_1074', 'morgan_fp_1075', 'morgan_fp_1076', 'morgan_fp_1077', 'morgan_fp_1078', 'morgan_fp_1079', 'morgan_fp_1080', 'morgan_fp_1081', 'morgan_fp_1082', 'morgan_fp_1083', 'morgan_fp_1084', 'morgan_fp_1085', 'morgan_fp_1086', 'morgan_fp_1087', 'morgan_fp_1088', 'morgan_fp_1089', 'morgan_fp_1090', 'morgan_fp_1091', 'morgan_fp_1092', 'morgan_fp_1093', 'morgan_fp_1094', 'morgan_fp_1095', 'morgan_fp_1096', 'morgan_fp_1097', 'morgan_fp_1098', 'morgan_fp_1099', 'morgan_fp_1100', 'morgan_fp_1101', 'morgan_fp_1102', 'morgan_fp_1103', 'morgan_fp_1104', 'morgan_fp_1105', 'morgan_fp_1106', 'morgan_fp_1107', 'morgan_fp_1108', 'morgan_fp_1109', 'morgan_fp_1110', 'morgan_fp_1111', 'morgan_fp_1112', 'morgan_fp_1113', 'morgan_fp_1114', 'morgan_fp_1115', 'morgan_fp_1116', 'morgan_fp_1117', 'morgan_fp_1118', 'morgan_fp_1119', 'morgan_fp_1120', 'morgan_fp_1121', 'morgan_fp_1122', 'morgan_fp_1123', 'morgan_fp_1124', 'morgan_fp_1125', 'morgan_fp_1126', 'morgan_fp_1127', 'morgan_fp_1128', 'morgan_fp_1129', 'morgan_fp_1130', 'morgan_fp_1131', 'morgan_fp_1132', 'morgan_fp_1133', 'morgan_fp_1134', 'morgan_fp_1135', 'morgan_fp_1136', 'morgan_fp_1137', 'morgan_fp_1138', 'morgan_fp_1139', 'morgan_fp_1140', 'morgan_fp_1141', 'morgan_fp_1142', 'morgan_fp_1143', 'morgan_fp_1144', 'morgan_fp_1145', 'morgan_fp_1146', 'morgan_fp_1147', 'morgan_fp_1148', 'morgan_fp_1149', 'morgan_fp_1150', 'morgan_fp_1151', 'morgan_fp_1152', 'morgan_fp_1153', 'morgan_fp_1154', 'morgan_fp_1155', 'morgan_fp_1156', 'morgan_fp_1157', 'morgan_fp_1158', 'morgan_fp_1159', 'morgan_fp_1160', 'morgan_fp_1161', 'morgan_fp_1162', 'morgan_fp_1163', 'morgan_fp_1164', 'morgan_fp_1165', 'morgan_fp_1166', 'morgan_fp_1167', 'morgan_fp_1168', 'morgan_fp_1169', 'morgan_fp_1170', 'morgan_fp_1171', 'morgan_fp_1172', 'morgan_fp_1173', 'morgan_fp_1174', 'morgan_fp_1175', 'morgan_fp_1176', 'morgan_fp_1177', 'morgan_fp_1178', 'morgan_fp_1179', 'morgan_fp_1180', 'morgan_fp_1181', 'morgan_fp_1182', 'morgan_fp_1183', 'morgan_fp_1184', 'morgan_fp_1185', 'morgan_fp_1186', 'morgan_fp_1187', 'morgan_fp_1188', 'morgan_fp_1189', 'morgan_fp_1190', 'morgan_fp_1191', 'morgan_fp_1192', 'morgan_fp_1193', 'morgan_fp_1194', 'morgan_fp_1195', 'morgan_fp_1196', 'morgan_fp_1197', 'morgan_fp_1198', 'morgan_fp_1199', 'morgan_fp_1200', 'morgan_fp_1201', 'morgan_fp_1202', 'morgan_fp_1203', 'morgan_fp_1204', 'morgan_fp_1205', 'morgan_fp_1206', 'morgan_fp_1207', 'morgan_fp_1208', 'morgan_fp_1209', 'morgan_fp_1210', 'morgan_fp_1211', 'morgan_fp_1212', 'morgan_fp_1213', 'morgan_fp_1214', 'morgan_fp_1215', 'morgan_fp_1216', 'morgan_fp_1217', 'morgan_fp_1218', 'morgan_fp_1219', 'morgan_fp_1220', 'morgan_fp_1221', 'morgan_fp_1222', 'morgan_fp_1223', 'morgan_fp_1224', 'morgan_fp_1225', 'morgan_fp_1226', 'morgan_fp_1227', 'morgan_fp_1228', 'morgan_fp_1229', 'morgan_fp_1230', 'morgan_fp_1231', 'morgan_fp_1232', 'morgan_fp_1233', 'morgan_fp_1234', 'morgan_fp_1235', 'morgan_fp_1236', 'morgan_fp_1237', 'morgan_fp_1238', 'morgan_fp_1239', 'morgan_fp_1240', 'morgan_fp_1241', 'morgan_fp_1242', 'morgan_fp_1243', 'morgan_fp_1244', 'morgan_fp_1245', 'morgan_fp_1246', 'morgan_fp_1247', 'morgan_fp_1248', 'morgan_fp_1249', 'morgan_fp_1250', 'morgan_fp_1251', 'morgan_fp_1252', 'morgan_fp_1253', 'morgan_fp_1254', 'morgan_fp_1255', 'morgan_fp_1256', 'morgan_fp_1257', 'morgan_fp_1258', 'morgan_fp_1259', 'morgan_fp_1260', 'morgan_fp_1261', 'morgan_fp_1262', 'morgan_fp_1263', 'morgan_fp_1264', 'morgan_fp_1265', 'morgan_fp_1266', 'morgan_fp_1267', 'morgan_fp_1268', 'morgan_fp_1269', 'morgan_fp_1270', 'morgan_fp_1271', 'morgan_fp_1272', 'morgan_fp_1273', 'morgan_fp_1274', 'morgan_fp_1275', 'morgan_fp_1276', 'morgan_fp_1277', 'morgan_fp_1278', 'morgan_fp_1279', 'morgan_fp_1280', 'morgan_fp_1281', 'morgan_fp_1282', 'morgan_fp_1283', 'morgan_fp_1284', 'morgan_fp_1285', 'morgan_fp_1286', 'morgan_fp_1287', 'morgan_fp_1288', 'morgan_fp_1289', 'morgan_fp_1290', 'morgan_fp_1291', 'morgan_fp_1292', 'morgan_fp_1293', 'morgan_fp_1294', 'morgan_fp_1295', 'morgan_fp_1296', 'morgan_fp_1297', 'morgan_fp_1298', 'morgan_fp_1299', 'morgan_fp_1300', 'morgan_fp_1301', 'morgan_fp_1302', 'morgan_fp_1303', 'morgan_fp_1304', 'morgan_fp_1305', 'morgan_fp_1306', 'morgan_fp_1307', 'morgan_fp_1308', 'morgan_fp_1309', 'morgan_fp_1310', 'morgan_fp_1311', 'morgan_fp_1312', 'morgan_fp_1313', 'morgan_fp_1314', 'morgan_fp_1315', 'morgan_fp_1316', 'morgan_fp_1317', 'morgan_fp_1318', 'morgan_fp_1319', 'morgan_fp_1320', 'morgan_fp_1321', 'morgan_fp_1322', 'morgan_fp_1323', 'morgan_fp_1324', 'morgan_fp_1325', 'morgan_fp_1326', 'morgan_fp_1327', 'morgan_fp_1328', 'morgan_fp_1329', 'morgan_fp_1330', 'morgan_fp_1331', 'morgan_fp_1332', 'morgan_fp_1333', 'morgan_fp_1334', 'morgan_fp_1335', 'morgan_fp_1336', 'morgan_fp_1337', 'morgan_fp_1338', 'morgan_fp_1339', 'morgan_fp_1340', 'morgan_fp_1341', 'morgan_fp_1342', 'morgan_fp_1343', 'morgan_fp_1344', 'morgan_fp_1345', 'morgan_fp_1346', 'morgan_fp_1347', 'morgan_fp_1348', 'morgan_fp_1349', 'morgan_fp_1350', 'morgan_fp_1351', 'morgan_fp_1352', 'morgan_fp_1353', 'morgan_fp_1354', 'morgan_fp_1355', 'morgan_fp_1356', 'morgan_fp_1357', 'morgan_fp_1358', 'morgan_fp_1359', 'morgan_fp_1360', 'morgan_fp_1361', 'morgan_fp_1362', 'morgan_fp_1363', 'morgan_fp_1364', 'morgan_fp_1365', 'morgan_fp_1366', 'morgan_fp_1367', 'morgan_fp_1368', 'morgan_fp_1369', 'morgan_fp_1370', 'morgan_fp_1371', 'morgan_fp_1372', 'morgan_fp_1373', 'morgan_fp_1374', 'morgan_fp_1375', 'morgan_fp_1376', 'morgan_fp_1377', 'morgan_fp_1378', 'morgan_fp_1379', 'morgan_fp_1380', 'morgan_fp_1381', 'morgan_fp_1382', 'morgan_fp_1383', 'morgan_fp_1384', 'morgan_fp_1385', 'morgan_fp_1386', 'morgan_fp_1387', 'morgan_fp_1388', 'morgan_fp_1389', 'morgan_fp_1390', 'morgan_fp_1391', 'morgan_fp_1392', 'morgan_fp_1393', 'morgan_fp_1394', 'morgan_fp_1395', 'morgan_fp_1396', 'morgan_fp_1397', 'morgan_fp_1398', 'morgan_fp_1399', 'morgan_fp_1400', 'morgan_fp_1401', 'morgan_fp_1402', 'morgan_fp_1403', 'morgan_fp_1404', 'morgan_fp_1405', 'morgan_fp_1406', 'morgan_fp_1407', 'morgan_fp_1408', 'morgan_fp_1409', 'morgan_fp_1410', 'morgan_fp_1411', 'morgan_fp_1412', 'morgan_fp_1413', 'morgan_fp_1414', 'morgan_fp_1415', 'morgan_fp_1416', 'morgan_fp_1417', 'morgan_fp_1418', 'morgan_fp_1419', 'morgan_fp_1420', 'morgan_fp_1421', 'morgan_fp_1422', 'morgan_fp_1423', 'morgan_fp_1424', 'morgan_fp_1425', 'morgan_fp_1426', 'morgan_fp_1427', 'morgan_fp_1428', 'morgan_fp_1429', 'morgan_fp_1430', 'morgan_fp_1431', 'morgan_fp_1432', 'morgan_fp_1433', 'morgan_fp_1434', 'morgan_fp_1435', 'morgan_fp_1436', 'morgan_fp_1437', 'morgan_fp_1438', 'morgan_fp_1439', 'morgan_fp_1440', 'morgan_fp_1441', 'morgan_fp_1442', 'morgan_fp_1443', 'morgan_fp_1444', 'morgan_fp_1445', 'morgan_fp_1446', 'morgan_fp_1447', 'morgan_fp_1448', 'morgan_fp_1449', 'morgan_fp_1450', 'morgan_fp_1451', 'morgan_fp_1452', 'morgan_fp_1453', 'morgan_fp_1454', 'morgan_fp_1455', 'morgan_fp_1456', 'morgan_fp_1457', 'morgan_fp_1458', 'morgan_fp_1459', 'morgan_fp_1460', 'morgan_fp_1461', 'morgan_fp_1462', 'morgan_fp_1463', 'morgan_fp_1464', 'morgan_fp_1465', 'morgan_fp_1466', 'morgan_fp_1467', 'morgan_fp_1468', 'morgan_fp_1469', 'morgan_fp_1470', 'morgan_fp_1471', 'morgan_fp_1472', 'morgan_fp_1473', 'morgan_fp_1474', 'morgan_fp_1475', 'morgan_fp_1476', 'morgan_fp_1477', 'morgan_fp_1478', 'morgan_fp_1479', 'morgan_fp_1480', 'morgan_fp_1481', 'morgan_fp_1482', 'morgan_fp_1483', 'morgan_fp_1484', 'morgan_fp_1485', 'morgan_fp_1486', 'morgan_fp_1487', 'morgan_fp_1488', 'morgan_fp_1489', 'morgan_fp_1490', 'morgan_fp_1491', 'morgan_fp_1492', 'morgan_fp_1493', 'morgan_fp_1494', 'morgan_fp_1495', 'morgan_fp_1496', 'morgan_fp_1497', 'morgan_fp_1498', 'morgan_fp_1499', 'morgan_fp_1500', 'morgan_fp_1501', 'morgan_fp_1502', 'morgan_fp_1503', 'morgan_fp_1504', 'morgan_fp_1505', 'morgan_fp_1506', 'morgan_fp_1507', 'morgan_fp_1508', 'morgan_fp_1509', 'morgan_fp_1510', 'morgan_fp_1511', 'morgan_fp_1512', 'morgan_fp_1513', 'morgan_fp_1514', 'morgan_fp_1515', 'morgan_fp_1516', 'morgan_fp_1517', 'morgan_fp_1518', 'morgan_fp_1519', 'morgan_fp_1520', 'morgan_fp_1521', 'morgan_fp_1522', 'morgan_fp_1523', 'morgan_fp_1524', 'morgan_fp_1525', 'morgan_fp_1526', 'morgan_fp_1527', 'morgan_fp_1528', 'morgan_fp_1529', 'morgan_fp_1530', 'morgan_fp_1531', 'morgan_fp_1532', 'morgan_fp_1533', 'morgan_fp_1534', 'morgan_fp_1535', 'morgan_fp_1536', 'morgan_fp_1537', 'morgan_fp_1538', 'morgan_fp_1539', 'morgan_fp_1540', 'morgan_fp_1541', 'morgan_fp_1542', 'morgan_fp_1543', 'morgan_fp_1544', 'morgan_fp_1545', 'morgan_fp_1546', 'morgan_fp_1547', 'morgan_fp_1548', 'morgan_fp_1549', 'morgan_fp_1550', 'morgan_fp_1551', 'morgan_fp_1552', 'morgan_fp_1553', 'morgan_fp_1554', 'morgan_fp_1555', 'morgan_fp_1556', 'morgan_fp_1557', 'morgan_fp_1558', 'morgan_fp_1559', 'morgan_fp_1560', 'morgan_fp_1561', 'morgan_fp_1562', 'morgan_fp_1563', 'morgan_fp_1564', 'morgan_fp_1565', 'morgan_fp_1566', 'morgan_fp_1567', 'morgan_fp_1568', 'morgan_fp_1569', 'morgan_fp_1570', 'morgan_fp_1571', 'morgan_fp_1572', 'morgan_fp_1573', 'morgan_fp_1574', 'morgan_fp_1575', 'morgan_fp_1576', 'morgan_fp_1577', 'morgan_fp_1578', 'morgan_fp_1579', 'morgan_fp_1580', 'morgan_fp_1581', 'morgan_fp_1582', 'morgan_fp_1583', 'morgan_fp_1584', 'morgan_fp_1585', 'morgan_fp_1586', 'morgan_fp_1587', 'morgan_fp_1588', 'morgan_fp_1589', 'morgan_fp_1590', 'morgan_fp_1591', 'morgan_fp_1592', 'morgan_fp_1593', 'morgan_fp_1594', 'morgan_fp_1595', 'morgan_fp_1596', 'morgan_fp_1597', 'morgan_fp_1598', 'morgan_fp_1599', 'morgan_fp_1600', 'morgan_fp_1601', 'morgan_fp_1602', 'morgan_fp_1603', 'morgan_fp_1604', 'morgan_fp_1605', 'morgan_fp_1606', 'morgan_fp_1607', 'morgan_fp_1608', 'morgan_fp_1609', 'morgan_fp_1610', 'morgan_fp_1611', 'morgan_fp_1612', 'morgan_fp_1613', 'morgan_fp_1614', 'morgan_fp_1615', 'morgan_fp_1616', 'morgan_fp_1617', 'morgan_fp_1618', 'morgan_fp_1619', 'morgan_fp_1620', 'morgan_fp_1621', 'morgan_fp_1622', 'morgan_fp_1623', 'morgan_fp_1624', 'morgan_fp_1625', 'morgan_fp_1626', 'morgan_fp_1627', 'morgan_fp_1628', 'morgan_fp_1629', 'morgan_fp_1630', 'morgan_fp_1631', 'morgan_fp_1632', 'morgan_fp_1633', 'morgan_fp_1634', 'morgan_fp_1635', 'morgan_fp_1636', 'morgan_fp_1637', 'morgan_fp_1638', 'morgan_fp_1639', 'morgan_fp_1640', 'morgan_fp_1641', 'morgan_fp_1642', 'morgan_fp_1643', 'morgan_fp_1644', 'morgan_fp_1645', 'morgan_fp_1646', 'morgan_fp_1647', 'morgan_fp_1648', 'morgan_fp_1649', 'morgan_fp_1650', 'morgan_fp_1651', 'morgan_fp_1652', 'morgan_fp_1653', 'morgan_fp_1654', 'morgan_fp_1655', 'morgan_fp_1656', 'morgan_fp_1657', 'morgan_fp_1658', 'morgan_fp_1659', 'morgan_fp_1660', 'morgan_fp_1661', 'morgan_fp_1662', 'morgan_fp_1663', 'morgan_fp_1664', 'morgan_fp_1665', 'morgan_fp_1666', 'morgan_fp_1667', 'morgan_fp_1668', 'morgan_fp_1669', 'morgan_fp_1670', 'morgan_fp_1671', 'morgan_fp_1672', 'morgan_fp_1673', 'morgan_fp_1674', 'morgan_fp_1675', 'morgan_fp_1676', 'morgan_fp_1677', 'morgan_fp_1678', 'morgan_fp_1679', 'morgan_fp_1680', 'morgan_fp_1681', 'morgan_fp_1682', 'morgan_fp_1683', 'morgan_fp_1684', 'morgan_fp_1685', 'morgan_fp_1686', 'morgan_fp_1687', 'morgan_fp_1688', 'morgan_fp_1689', 'morgan_fp_1690', 'morgan_fp_1691', 'morgan_fp_1692', 'morgan_fp_1693', 'morgan_fp_1694', 'morgan_fp_1695', 'morgan_fp_1696', 'morgan_fp_1697', 'morgan_fp_1698', 'morgan_fp_1699', 'morgan_fp_1700', 'morgan_fp_1701', 'morgan_fp_1702', 'morgan_fp_1703', 'morgan_fp_1704', 'morgan_fp_1705', 'morgan_fp_1706', 'morgan_fp_1707', 'morgan_fp_1708', 'morgan_fp_1709', 'morgan_fp_1710', 'morgan_fp_1711', 'morgan_fp_1712', 'morgan_fp_1713', 'morgan_fp_1714', 'morgan_fp_1715', 'morgan_fp_1716', 'morgan_fp_1717', 'morgan_fp_1718', 'morgan_fp_1719', 'morgan_fp_1720', 'morgan_fp_1721', 'morgan_fp_1722', 'morgan_fp_1723', 'morgan_fp_1724', 'morgan_fp_1725', 'morgan_fp_1726', 'morgan_fp_1727', 'morgan_fp_1728', 'morgan_fp_1729', 'morgan_fp_1730', 'morgan_fp_1731', 'morgan_fp_1732', 'morgan_fp_1733', 'morgan_fp_1734', 'morgan_fp_1735', 'morgan_fp_1736', 'morgan_fp_1737', 'morgan_fp_1738', 'morgan_fp_1739', 'morgan_fp_1740', 'morgan_fp_1741', 'morgan_fp_1742', 'morgan_fp_1743', 'morgan_fp_1744', 'morgan_fp_1745', 'morgan_fp_1746', 'morgan_fp_1747', 'morgan_fp_1748', 'morgan_fp_1749', 'morgan_fp_1750', 'morgan_fp_1751', 'morgan_fp_1752', 'morgan_fp_1753', 'morgan_fp_1754', 'morgan_fp_1755', 'morgan_fp_1756', 'morgan_fp_1757', 'morgan_fp_1758', 'morgan_fp_1759', 'morgan_fp_1760', 'morgan_fp_1761', 'morgan_fp_1762', 'morgan_fp_1763', 'morgan_fp_1764', 'morgan_fp_1765', 'morgan_fp_1766', 'morgan_fp_1767', 'morgan_fp_1768', 'morgan_fp_1769', 'morgan_fp_1770', 'morgan_fp_1771', 'morgan_fp_1772', 'morgan_fp_1773', 'morgan_fp_1774', 'morgan_fp_1775', 'morgan_fp_1776', 'morgan_fp_1777', 'morgan_fp_1778', 'morgan_fp_1779', 'morgan_fp_1780', 'morgan_fp_1781', 'morgan_fp_1782', 'morgan_fp_1783', 'morgan_fp_1784', 'morgan_fp_1785', 'morgan_fp_1786', 'morgan_fp_1787', 'morgan_fp_1788', 'morgan_fp_1789', 'morgan_fp_1790', 'morgan_fp_1791', 'morgan_fp_1792', 'morgan_fp_1793', 'morgan_fp_1794', 'morgan_fp_1795', 'morgan_fp_1796', 'morgan_fp_1797', 'morgan_fp_1798', 'morgan_fp_1799', 'morgan_fp_1800', 'morgan_fp_1801', 'morgan_fp_1802', 'morgan_fp_1803', 'morgan_fp_1804', 'morgan_fp_1805', 'morgan_fp_1806', 'morgan_fp_1807', 'morgan_fp_1808', 'morgan_fp_1809', 'morgan_fp_1810', 'morgan_fp_1811', 'morgan_fp_1812', 'morgan_fp_1813', 'morgan_fp_1814', 'morgan_fp_1815', 'morgan_fp_1816', 'morgan_fp_1817', 'morgan_fp_1818', 'morgan_fp_1819', 'morgan_fp_1820', 'morgan_fp_1821', 'morgan_fp_1822', 'morgan_fp_1823', 'morgan_fp_1824', 'morgan_fp_1825', 'morgan_fp_1826', 'morgan_fp_1827', 'morgan_fp_1828', 'morgan_fp_1829', 'morgan_fp_1830', 'morgan_fp_1831', 'morgan_fp_1832', 'morgan_fp_1833', 'morgan_fp_1834', 'morgan_fp_1835', 'morgan_fp_1836', 'morgan_fp_1837', 'morgan_fp_1838', 'morgan_fp_1839', 'morgan_fp_1840', 'morgan_fp_1841', 'morgan_fp_1842', 'morgan_fp_1843', 'morgan_fp_1844', 'morgan_fp_1845', 'morgan_fp_1846', 'morgan_fp_1847', 'morgan_fp_1848', 'morgan_fp_1849', 'morgan_fp_1850', 'morgan_fp_1851', 'morgan_fp_1852', 'morgan_fp_1853', 'morgan_fp_1854', 'morgan_fp_1855', 'morgan_fp_1856', 'morgan_fp_1857', 'morgan_fp_1858', 'morgan_fp_1859', 'morgan_fp_1860', 'morgan_fp_1861', 'morgan_fp_1862', 'morgan_fp_1863', 'morgan_fp_1864', 'morgan_fp_1865', 'morgan_fp_1866', 'morgan_fp_1867', 'morgan_fp_1868', 'morgan_fp_1869', 'morgan_fp_1870', 'morgan_fp_1871', 'morgan_fp_1872', 'morgan_fp_1873', 'morgan_fp_1874', 'morgan_fp_1875', 'morgan_fp_1876', 'morgan_fp_1877', 'morgan_fp_1878', 'morgan_fp_1879', 'morgan_fp_1880', 'morgan_fp_1881', 'morgan_fp_1882', 'morgan_fp_1883', 'morgan_fp_1884', 'morgan_fp_1885', 'morgan_fp_1886', 'morgan_fp_1887', 'morgan_fp_1888', 'morgan_fp_1889', 'morgan_fp_1890', 'morgan_fp_1891', 'morgan_fp_1892', 'morgan_fp_1893', 'morgan_fp_1894', 'morgan_fp_1895', 'morgan_fp_1896', 'morgan_fp_1897', 'morgan_fp_1898', 'morgan_fp_1899', 'morgan_fp_1900', 'morgan_fp_1901', 'morgan_fp_1902', 'morgan_fp_1903', 'morgan_fp_1904', 'morgan_fp_1905', 'morgan_fp_1906', 'morgan_fp_1907', 'morgan_fp_1908', 'morgan_fp_1909', 'morgan_fp_1910', 'morgan_fp_1911', 'morgan_fp_1912', 'morgan_fp_1913', 'morgan_fp_1914', 'morgan_fp_1915', 'morgan_fp_1916', 'morgan_fp_1917', 'morgan_fp_1918', 'morgan_fp_1919', 'morgan_fp_1920', 'morgan_fp_1921', 'morgan_fp_1922', 'morgan_fp_1923', 'morgan_fp_1924', 'morgan_fp_1925', 'morgan_fp_1926', 'morgan_fp_1927', 'morgan_fp_1928', 'morgan_fp_1929', 'morgan_fp_1930', 'morgan_fp_1931', 'morgan_fp_1932', 'morgan_fp_1933', 'morgan_fp_1934', 'morgan_fp_1935', 'morgan_fp_1936', 'morgan_fp_1937', 'morgan_fp_1938', 'morgan_fp_1939', 'morgan_fp_1940', 'morgan_fp_1941', 'morgan_fp_1942', 'morgan_fp_1943', 'morgan_fp_1944', 'morgan_fp_1945', 'morgan_fp_1946', 'morgan_fp_1947', 'morgan_fp_1948', 'morgan_fp_1949', 'morgan_fp_1950', 'morgan_fp_1951', 'morgan_fp_1952', 'morgan_fp_1953', 'morgan_fp_1954', 'morgan_fp_1955', 'morgan_fp_1956', 'morgan_fp_1957', 'morgan_fp_1958', 'morgan_fp_1959', 'morgan_fp_1960', 'morgan_fp_1961', 'morgan_fp_1962', 'morgan_fp_1963', 'morgan_fp_1964', 'morgan_fp_1965', 'morgan_fp_1966', 'morgan_fp_1967', 'morgan_fp_1968', 'morgan_fp_1969', 'morgan_fp_1970', 'morgan_fp_1971', 'morgan_fp_1972', 'morgan_fp_1973', 'morgan_fp_1974', 'morgan_fp_1975', 'morgan_fp_1976', 'morgan_fp_1977', 'morgan_fp_1978', 'morgan_fp_1979', 'morgan_fp_1980', 'morgan_fp_1981', 'morgan_fp_1982', 'morgan_fp_1983', 'morgan_fp_1984', 'morgan_fp_1985', 'morgan_fp_1986', 'morgan_fp_1987', 'morgan_fp_1988', 'morgan_fp_1989', 'morgan_fp_1990', 'morgan_fp_1991', 'morgan_fp_1992', 'morgan_fp_1993', 'morgan_fp_1994', 'morgan_fp_1995', 'morgan_fp_1996', 'morgan_fp_1997', 'morgan_fp_1998', 'morgan_fp_1999', 'morgan_fp_2000', 'morgan_fp_2001', 'morgan_fp_2002', 'morgan_fp_2003', 'morgan_fp_2004', 'morgan_fp_2005', 'morgan_fp_2006', 'morgan_fp_2007', 'morgan_fp_2008', 'morgan_fp_2009', 'morgan_fp_2010', 'morgan_fp_2011', 'morgan_fp_2012', 'morgan_fp_2013', 'morgan_fp_2014', 'morgan_fp_2015', 'morgan_fp_2016', 'morgan_fp_2017', 'morgan_fp_2018', 'morgan_fp_2019', 'morgan_fp_2020', 'morgan_fp_2021', 'morgan_fp_2022', 'morgan_fp_2023', 'morgan_fp_2024', 'morgan_fp_2025', 'morgan_fp_2026', 'morgan_fp_2027', 'morgan_fp_2028', 'morgan_fp_2029', 'morgan_fp_2030', 'morgan_fp_2031', 'morgan_fp_2032', 'morgan_fp_2033', 'morgan_fp_2034', 'morgan_fp_2035', 'morgan_fp_2036', 'morgan_fp_2037', 'morgan_fp_2038', 'morgan_fp_2039', 'morgan_fp_2040', 'morgan_fp_2041', 'morgan_fp_2042', 'morgan_fp_2043', 'morgan_fp_2044', 'morgan_fp_2045', 'morgan_fp_2046', 'morgan_fp_2047']\n",
      "\n",
      "Extracted global features for each split:\n",
      "X_train_global_features shape: (13119, 2266)\n",
      "X_val_global_features shape: (2812, 2266)\n",
      "X_test_global_features shape: (2812, 2266)\n",
      "\n",
      "First 5 rows of X_train_global_features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>312.243</td>\n",
       "      <td>328.121178</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>334.222</td>\n",
       "      <td>353.126323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>418.303</td>\n",
       "      <td>447.215806</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>360.102</td>\n",
       "      <td>374.026604</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>453.727</td>\n",
       "      <td>472.111375</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  HeavyAtomMolWt  ExactMolWt  \\\n",
       "0        0.494176  0.476742  12.560000  328.371         312.243  328.121178   \n",
       "1        0.459195  0.604738  12.923077  353.374         334.222  353.126323   \n",
       "2       -0.173158  0.359463  15.909091  447.535         418.303  447.215806   \n",
       "3       -0.686457  0.479732  11.217391  375.222         360.102  374.026604   \n",
       "4       -3.116139  0.437556  16.121212  472.879         453.727  472.111375   \n",
       "\n",
       "   ...  morgan_fp_2038  morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  \\\n",
       "0  ...               0               0               0               0   \n",
       "1  ...               0               0               0               0   \n",
       "2  ...               0               0               0               0   \n",
       "3  ...               0               0               0               0   \n",
       "4  ...               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2042  morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2046  morgan_fp_2047  \n",
       "0               0               0  \n",
       "1               0               0  \n",
       "2               0               0  \n",
       "3               0               0  \n",
       "4               0               0  \n",
       "\n",
       "[5 rows x 2266 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original X DataFrames (with molregno and canonical_smiles) are retained and ready for graph construction:\n",
      "X_train shape: (13119, 2268)\n",
      "X_val shape: (2812, 2268)\n",
      "X_test shape: (2812, 2268)\n",
      "y_train shape: (13119, 1), y_val shape: (2812, 1), y_test shape: (2812, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molregno</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307646</td>\n",
       "      <td>COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C</td>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081122</td>\n",
       "      <td>COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC</td>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2199496</td>\n",
       "      <td>COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2221960</td>\n",
       "      <td>O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO</td>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2879093</td>\n",
       "      <td>Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...</td>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   molregno                                   canonical_smiles  \\\n",
       "0   2307646               COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C   \n",
       "1   2081122       COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC   \n",
       "2   2199496  COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...   \n",
       "3   2221960           O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO   \n",
       "4   2879093  Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...   \n",
       "\n",
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  ...  morgan_fp_2038  \\\n",
       "0        0.494176  0.476742  12.560000  328.371  ...               0   \n",
       "1        0.459195  0.604738  12.923077  353.374  ...               0   \n",
       "2       -0.173158  0.359463  15.909091  447.535  ...               0   \n",
       "3       -0.686457  0.479732  11.217391  375.222  ...               0   \n",
       "4       -3.116139  0.437556  16.121212  472.879  ...               0   \n",
       "\n",
       "   morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  morgan_fp_2042  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  morgan_fp_2046  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2047  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 2268 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pGI50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>5.734742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>7.164746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>4.928428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>6.882724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>6.094208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pGI50\n",
       "14387  5.734742\n",
       "12543  7.164746\n",
       "12810  4.928428\n",
       "13172  6.882724\n",
       "18712  6.094208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n--- Excracting global features of each molecule ---\")\n",
    "\n",
    "# Identify columns for global features\n",
    "# These are all columns in X_train EXCEPT 'molregno' and 'canonical_smiles'\n",
    "global_feature_columns = X_train.drop(columns=['molregno', 'canonical_smiles'], errors='ignore').columns.tolist()\n",
    "\n",
    "print(f\"Identified {len(global_feature_columns)} global feature columns for GNN.\")\n",
    "print(f\"Global feature columns: {global_feature_columns}\")\n",
    "\n",
    "# Extract global features into new DataFrames\n",
    "# These DataFrames will be the source for data.global_features in GNN Data objects\n",
    "X_train_global_features = X_train[global_feature_columns]\n",
    "X_val_global_features = X_val[global_feature_columns]\n",
    "X_test_global_features = X_test[global_feature_columns]\n",
    "\n",
    "print(\"\\nExtracted global features for each split:\")\n",
    "print(f\"X_train_global_features shape: {X_train_global_features.shape}\")\n",
    "print(f\"X_val_global_features shape: {X_val_global_features.shape}\")\n",
    "print(f\"X_test_global_features shape: {X_test_global_features.shape}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of X_train_global_features:\")\n",
    "display(X_train_global_features.head())\n",
    "\n",
    "\n",
    "# Confirm original X DataFrames (with molregno and canonical_smiles) are still available\n",
    "# These will be used for iterating and building individual graph objects.\n",
    "print(\"\\nOriginal X DataFrames (with molregno and canonical_smiles) are retained and ready for graph construction:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_val shape: {y_val.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "display(X_train.head()) # Show that original X_train still has molregno and canonical_smiles\n",
    "display(y_train.head()) # Show the pGI50 target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff4a6c-ef8d-4aef-863c-22bc039fe563",
   "metadata": {},
   "source": [
    "## Create Graph Objects of Each Molecule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8982d07-be78-4701-a6bc-45ebcd87fd11",
   "metadata": {},
   "source": [
    "### Define Helper Function to Create Graph Object of One Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c331d7-1a8a-47dc-b1f4-99b7429ae398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_pyg_data(mol, pgi50_value, global_features_vector, molregno, smiles_string):\n",
    "    if mol is None:\n",
    "        return None  # Handle cases where SMILES parsing fails\n",
    "\n",
    "    # Compute Gasteiger charges (how electron-dense the area occupied by this atom is, crucial for interactions)\n",
    "    try:\n",
    "        AllChem.ComputeGasteigerCharges(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute Gasteiger charges for molregno {molregno}: {e}\")\n",
    "        # If computation fails, atoms will default to 0.0 for this property\n",
    "        pass\n",
    "\n",
    "    # Node Features (x): Atom Properties\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        # Initialize a list for this atom's features\n",
    "        features = []\n",
    "\n",
    "        # Atomic Number (int, not one-hot coded)\n",
    "        features.append(atom.GetAtomicNum())\n",
    "\n",
    "        # Basic Connectivity\n",
    "        features.append(atom.GetDegree())  # Num of directly-bonded heavy (non-Hydrogen) atoms\n",
    "        features.append(atom.GetTotalDegree())  # Total numb of neighbors (including all Hydrogens)\n",
    "\n",
    "        # Charge and Valence\n",
    "        features.append(atom.GetFormalCharge())  # Formal charge (integer charge based on bonding rules)\n",
    "        features.append(atom.GetNumExplicitHs())  # Number of explicitly defined hydrogens attached\n",
    "        features.append(atom.GetNumImplicitHs())  # Number of hydrogens implicitly defined by valence\n",
    "        features.append(atom.GetTotalNumHs())  # Total number of hydrogens attached (explicit + implicit)\n",
    "        features.append(atom.GetValence(Chem.ValenceType.IMPLICIT))  # Implicit Valence: Number of bonds formed by implicit hydrogens\n",
    "        features.append(atom.GetValence(Chem.ValenceType.EXPLICIT))  # Explicit Valence: Sum of bond orders (1 for single, 2 for double, etc.) to explicitly defined atoms\n",
    "        features.append(atom.GetTotalValence())  # Total Valence: Total number of bonds (sum of explicit & implicit valence)\n",
    "\n",
    "        # Hybridization (convert enum to int) (e.g., sp3, sp2)\n",
    "        features.append(int(atom.GetHybridization()))\n",
    "\n",
    "        # Aromaticity and Ring Information (boolean converted to int)\n",
    "        features.append(int(atom.GetIsAromatic()))        # Whether the atom is part of an aromatic system\n",
    "        features.append(int(atom.IsInRing()))             # Whether the atom is in ANY ring structure\n",
    "        features.append(int(atom.IsInRingSize(3)))        # Whether the atom is in a 3-membered ring\n",
    "        features.append(int(atom.IsInRingSize(4)))        # Whether the atom is in a 4-membered ring\n",
    "        features.append(int(atom.IsInRingSize(5)))        # Whether the atom is in a 5-membered ring\n",
    "        features.append(int(atom.IsInRingSize(6)))        # Whether the atom is in a 6-membered ring\n",
    "        features.append(int(atom.IsInRingSize(7)))        # Whether the atom is in a 7-membered ring\n",
    "        features.append(int(atom.IsInRingSize(8)))        # Whether the atom is in an 8-membered ring\n",
    "\n",
    "        # Chirality (convert enum to int)(stereochemical information, crucial for biological activity)\n",
    "        features.append(int(atom.GetChiralTag()))\n",
    "\n",
    "         # Partial Charges (from Gasteiger calculation)\n",
    "        gasteiger_charge = 0.0\n",
    "        if atom.HasProp('_GasteigerCharge'):\n",
    "            try:\n",
    "                gasteiger_charge = float(atom.GetProp('_GasteigerCharge'))\n",
    "            except ValueError:\n",
    "                pass # Handle potential 'nan' or non-float values gracefully\n",
    "        features.append(gasteiger_charge)\n",
    "\n",
    "        # Add to the list of all atom features for this molecule\n",
    "        atom_features.append(features)\n",
    "        \n",
    "    # Convert the list of lists to a PyTorch tensor\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "\n",
    "    # Edge Index (edge_index): Bond connectivity\n",
    "    edge_indices = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append([i, j])\n",
    "        edge_indices.append([j, i]) # Add reverse edge for undirected graph\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Handle molecules with no bonds (single atom, e.g., for [Ne])\n",
    "    if edge_index.numel() == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long) # Create an empty edge_index tensor\n",
    "\n",
    "    # Graph-level Target (y): pGI50\n",
    "    y = torch.tensor([pgi50_value], dtype=torch.float)\n",
    "\n",
    "    # 4. Global Features (global_features)\n",
    "    try:\n",
    "        global_features_vector = global_features_vector.astype(float)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting global_features_vector to float for molregno {molregno}: {e}\")\n",
    "        \n",
    "    global_features_tensor = torch.tensor(global_features_vector, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    # Create the PyTorch Geometric Data object\n",
    "    data = Data(x=x,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                global_features=global_features_tensor,\n",
    "                molregno=molregno,\n",
    "                smiles=smiles_string)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3b691-faa4-494d-b48b-49685b04d746",
   "metadata": {},
   "source": [
    "### Apply Helper Function on Data to Create Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c56a6f9-50ad-4596-8935-38c2af5e1a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating PyG Data objects for Training Set ---\n",
      "Type of X_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of y_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of X_train_global_features: <class 'pandas.core.frame.DataFrame'>\n",
      "Length of train_df after concatenation: 13119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5877529cbb9425bbf9c93953bdf1ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Train Molecules:   0%|          | 0/13119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 13119 / 13119 graph objects for the training set.\n",
      "Total training graphs: 13119\n",
      "\n",
      "--- Creating PyG Data objects for Validation Set ---\n",
      "Length of val_df after concatenation: 2812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fb6b11cca64aeaa2dbf8ba24e0c4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Validation Molecules:   0%|          | 0/2812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 2812 / 2812 graph objects for the validation set.\n",
      "Total validation graphs: 2812\n",
      "\n",
      "--- Creating PyG Data objects for Test Set ---\n",
      "Length of test_df after concatenation: 2812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8517eba41bb47308aac7a64d6d55d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Test Molecules:   0%|          | 0/2812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 2812 / 2812 graph objects for the test set.\n",
      "Total test graphs: 2812\n"
     ]
    }
   ],
   "source": [
    "train_data_list = []\n",
    "val_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# Process Training Data\n",
    "print(\"\\n--- Creating PyG Data objects for Training Set ---\")\n",
    "\n",
    "print(f\"Type of X_train: {type(X_train)}\")\n",
    "print(f\"Type of y_train: {type(y_train)}\")\n",
    "print(f\"Type of X_train_global_features: {type(X_train_global_features)}\")\n",
    "\n",
    "# Ensure X_train, y_train, X_train_global_features have the same index for alignment\n",
    "train_df = pd.concat([X_train.reset_index(drop=True),\n",
    "                      y_train.reset_index(drop=True),\n",
    "                      X_train_global_features.reset_index(drop=True)],\n",
    "                     axis=1)\n",
    "print(f\"Length of train_df after concatenation: {len(train_df)}\")\n",
    "\n",
    "successful_train_graphs = 0\n",
    "for index, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing Train Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    \n",
    "    # Extract global features based on the column names extracted after loading data splits\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    # Convert SMILES to RDKit Mol object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Create PyG Data object\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0: # Ensure valid mol and has nodes\n",
    "        train_data_list.append(pyg_data)\n",
    "        successful_train_graphs += 1\n",
    "    else:\n",
    "        print(f\"Warning: Could not process SMILES: {smiles} (Molregno: {molregno})\")\n",
    "\n",
    "print(f\"Successfully created {successful_train_graphs} / {len(train_df)} graph objects for the training set.\")\n",
    "print(f\"Total training graphs: {len(train_data_list)}\")\n",
    "\n",
    "\n",
    "# Process Validation Data\n",
    "print(\"\\n--- Creating PyG Data objects for Validation Set ---\")\n",
    "val_df = pd.concat([X_val.reset_index(drop=True),\n",
    "                    y_val.reset_index(drop=True),\n",
    "                    X_val_global_features.reset_index(drop=True)],\n",
    "                   axis=1)\n",
    "print(f\"Length of val_df after concatenation: {len(val_df)}\")\n",
    "\n",
    "successful_val_graphs = 0\n",
    "for index, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Processing Validation Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0:\n",
    "        val_data_list.append(pyg_data)\n",
    "        successful_val_graphs += 1\n",
    "\n",
    "print(f\"Successfully created {successful_val_graphs} / {len(val_df)} graph objects for the validation set.\")\n",
    "print(f\"Total validation graphs: {len(val_data_list)}\")\n",
    "\n",
    "\n",
    "# Process Test Data\n",
    "print(\"\\n--- Creating PyG Data objects for Test Set ---\")\n",
    "test_df = pd.concat([X_test.reset_index(drop=True),\n",
    "                     y_test.reset_index(drop=True),\n",
    "                     X_test_global_features.reset_index(drop=True)],\n",
    "                    axis=1)\n",
    "print(f\"Length of test_df after concatenation: {len(test_df)}\")\n",
    "\n",
    "successful_test_graphs = 0\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Test Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0:\n",
    "        test_data_list.append(pyg_data)\n",
    "        successful_test_graphs += 1\n",
    "\n",
    "print(f\"Successfully created {successful_test_graphs} / {len(test_df)} graph objects for the test set.\")\n",
    "print(f\"Total test graphs: {len(test_data_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835fa21-03a3-42ff-9df3-4d33a258ea09",
   "metadata": {},
   "source": [
    "#### Verify Creation of Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9ff332-5b18-4665-8b6c-dcd4948bb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [6.0, 6.0, 6.033141613006592, 6.033141613006592, 6.033141613006592]\n",
      "    Global features (data.global_features) min: -3.1400\n",
      "    Global features (data.global_features) max: 1072410.2500\n",
      "    Global features (data.global_features) mean: 474.7916\n",
      "    Global features (data.global_features) std: 22525.9316\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2ba18-2eac-40de-abed-6e864d14f225",
   "metadata": {},
   "source": [
    "### Standardize Global Features of Each Molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4617613f-0ddd-4ca9-b99b-005acf0e4878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global features in torch_geometric.data.Data objects have been scaled!\n"
     ]
    }
   ],
   "source": [
    "# Collect all global features to fit the scaler\n",
    "# Concatenate all global_features tensors. Each data.global_features is already (1, feature_dim),\n",
    "# so torch.cat(..., dim=0) will result in (num_total_graphs, feature_dim).\n",
    "list_of_global_features_tensors = [data.global_features for data in train_data_list + val_data_list]\n",
    "all_global_features_combined = torch.cat(list_of_global_features_tensors, dim=0).cpu().numpy()\n",
    "\n",
    "# Initialize and fit the scaler on the combined global features from training and validation sets\n",
    "global_feature_scaler = StandardScaler()\n",
    "global_feature_scaler.fit(all_global_features_combined)\n",
    "\n",
    "# Apply scaling to the 'global_features' in Data objects for all splits\n",
    "for data_list in [train_data_list, val_data_list, test_data_list]:\n",
    "    for data in data_list:\n",
    "        # Ensure it's numpy before scaling, then back to torch\n",
    "        original_global_features_np = data.global_features.cpu().numpy()\n",
    "        scaled_global_features_np = global_feature_scaler.transform(original_global_features_np)\n",
    "        # Put it back on the correct device\n",
    "        data.global_features = torch.tensor(scaled_global_features_np, dtype=torch.float32).to(data.global_features.device)\n",
    "\n",
    "print(\"\\nGlobal features in torch_geometric.data.Data objects have been scaled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c07769-10ea-426a-810d-8cb48c2b0c03",
   "metadata": {},
   "source": [
    "#### Verify Scaling of Global Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42567868-2a41-42be-af81-96be63072dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [0.038460150361061096, 0.038460150361061096, -2.2056941986083984, -2.2056941986083984, -2.2056941986083984]\n",
      "    Global features (data.global_features) min: -2.2057\n",
      "    Global features (data.global_features) max: 16.9898\n",
      "    Global features (data.global_features) mean: -0.0328\n",
      "    Global features (data.global_features) std: 0.9213\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd7f3f-57a4-47d2-85df-3485e21df7a2",
   "metadata": {},
   "source": [
    "### Save Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af3d05e3-8fe7-4ec2-a05c-71d6dc567cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed graph data saved to: ..\\data\\splits\\pyg_data_graphs\n",
      "Train data list size: 13119\n",
      "Validation data list size: 2812\n",
      "Test data list size: 2812\n"
     ]
    }
   ],
   "source": [
    "# Directory for saving the processed graph data\n",
    "save_dir = Path('../data/splits/pyg_data_graphs')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the full file paths\n",
    "train_data_path = save_dir / 'train_data_list.pt'\n",
    "val_data_path = save_dir / 'val_data_list.pt'\n",
    "test_data_path = save_dir / 'test_data_list.pt'\n",
    "\n",
    "# Save the lists of Data objects\n",
    "torch.save(train_data_list, train_data_path)\n",
    "torch.save(val_data_list, val_data_path)\n",
    "torch.save(test_data_list, test_data_path)\n",
    "\n",
    "print(f\"Processed graph data saved to: {save_dir}\")\n",
    "print(f\"Train data list size: {len(train_data_list)}\")\n",
    "print(f\"Validation data list size: {len(val_data_list)}\")\n",
    "print(f\"Test data list size: {len(test_data_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046262d-755c-4d54-86e8-33b883e70dce",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f189570-12e8-4380-8cd2-ccd3c52985f8",
   "metadata": {},
   "source": [
    "## Load Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6919bfb2-bb97-4f6a-995f-25842c26ef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13119 training graphs.\n",
      "Loaded 2812 validation graphs.\n",
      "Loaded 2812 test graphs.\n"
     ]
    }
   ],
   "source": [
    "# Directory where the graph objects are saved\n",
    "load_dir = Path('../data/splits/pyg_data_graphs')\n",
    "\n",
    "# Define the full file paths\n",
    "train_data_path = load_dir / 'train_data_list.pt'\n",
    "val_data_path = load_dir / 'val_data_list.pt'\n",
    "test_data_path = load_dir / 'test_data_list.pt'\n",
    "\n",
    "# Load the lists of Data objects\n",
    "try:\n",
    "    train_data_list = torch.load(train_data_path, weights_only=False)\n",
    "    val_data_list = torch.load(val_data_path, weights_only=False)\n",
    "    test_data_list = torch.load(test_data_path, weights_only=False)\n",
    "\n",
    "    print(f\"Loaded {len(train_data_list)} training graphs.\")\n",
    "    print(f\"Loaded {len(val_data_list)} validation graphs.\")\n",
    "    print(f\"Loaded {len(test_data_list)} test graphs.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Processed data not found in {load_dir}. Please run the data processing and saving step first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25775b7b-f6bf-487c-bd1d-afa380af03c7",
   "metadata": {},
   "source": [
    "### Verify Loading of Graph Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62061207-247d-4bd6-a197-daaa1a6fb4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [0.038460150361061096, 0.038460150361061096, -2.2056941986083984, -2.2056941986083984, -2.2056941986083984]\n",
      "    Global features (data.global_features) min: -2.2057\n",
      "    Global features (data.global_features) max: 16.9898\n",
      "    Global features (data.global_features) mean: -0.0328\n",
      "    Global features (data.global_features) std: 0.9213\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e15dce-d1ff-42de-bc1c-ce38b7cf630a",
   "metadata": {},
   "source": [
    "## Optimize Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52b2f3-fb35-4d0d-b181-c7af49150abd",
   "metadata": {},
   "source": [
    "### Define Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b30682f4-705d-48b2-8da1-24a8afa2325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 128, 1024, log=True) # Number of neurons in hidden layer\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256]) # Batch size for DataLoaders\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 150, 600)  # Number of training epochs\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4) # Number of GNN layers\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5) # Dropout rate\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-3, log=True)\n",
    "\n",
    "    # Determine feature dimensions dynamically from loaded/created graph objects\n",
    "    # Ensure train_data_list is not empty before accessing its first element\n",
    "    if not train_data_list:\n",
    "        raise ValueError(\"train_data_list is empty. Cannot determine feature dimensions.\")\n",
    "\n",
    "    # node_feature_dim: Number of features per atom\n",
    "    # global_feature_dim: Number of global features per molecule\n",
    "    node_feature_dim = train_data_list[0].x.shape[1]\n",
    "    global_feature_dim = train_data_list[0].global_features.shape[1]\n",
    "\n",
    "    # Initialize model\n",
    "    model = GNN(\n",
    "        node_feature_dim=node_feature_dim,\n",
    "        global_feature_dim=global_feature_dim,\n",
    "        hidden_channels=hidden_channels,  # From Optuna trial\n",
    "        num_layers=num_layers,  # From Optuna trial\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss function and Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # PyTorch Geometric DataLoaders\n",
    "    num_workers = 0\n",
    "    train_loader = PyGDataLoader(train_data_list, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = PyGDataLoader(val_data_list, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    best_val_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 50 # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        start_epoch_time = time.time()\n",
    "        for batch_idx, data_batch in enumerate(train_loader):\n",
    "            data_batch = data_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch)\n",
    "                \n",
    "            # Ensure outputs and target are same shape for loss calculation\n",
    "            loss = criterion(outputs.view(-1), data_batch.y.view(-1)) # .view(-1) flattens to ensure shape compatibility\n",
    "\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"!!! WARNING: NaN/Inf in model outputs at epoch {epoch+1}, batch {batch_idx+1}\")\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"!!! WARNING: NaN/Inf in loss at epoch {epoch+1}, batch {batch_idx+1}\")\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        print(f\"!!! CRITICAL: NaN/Inf in gradient of {name} - Epoch {epoch+1}, Batch {batch_idx+1}\")\n",
    "                        # Add a break here for deeper inspection if this happens\n",
    "                        # import sys; sys.exit(\"Gradient instability detected.\")\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad(): # Disable gradient calculations for validation\n",
    "            for data_batch in val_loader:\n",
    "                data_batch = data_batch.to(device)\n",
    "                val_outputs = model(data_batch)\n",
    "                val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "                val_targets.extend(data_batch.y.cpu().numpy().flatten()) # Extract y from PyG Data object\n",
    "\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "\n",
    "        if device.type == 'cuda': # Ensure GPU operations are finished before timing an epoch\n",
    "            torch.cuda.synchronize()\n",
    "        end_epoch_time = time.time()\n",
    "\n",
    "        print(f\"Trial {trial.number}, Epoch {epoch+1}/{n_epochs}, Val RMSE: {val_rmse:.4f}, Time: {end_epoch_time - start_epoch_time:.2f}s\")\n",
    "\n",
    "        # Optuna Pruning: Report current validation RMSE to Optuna\n",
    "        trial.report(val_rmse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Manual Early Stopping Check\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            patience_counter = 0 # Reset patience if improvement is found\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} for trial {trial.number}\")\n",
    "                break # Exit training loop for current trial\n",
    "\n",
    "    # Final evaluation on validation set after training (or early stopping)\n",
    "    model.eval()\n",
    "    final_val_predictions = []\n",
    "    final_val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data_batch in val_loader:\n",
    "            data_batch = data_batch.to(device)\n",
    "            val_outputs = model(data_batch)\n",
    "            final_val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "            final_val_targets.extend(data_batch.y.cpu().numpy().flatten())\n",
    "\n",
    "    final_rmse = np.sqrt(mean_squared_error(final_val_targets, final_val_predictions))\n",
    "    final_r2 = r2_score(final_val_targets, final_val_predictions)\n",
    "\n",
    "    # Store R2 score as well in the study for later analysis\n",
    "    trial.set_user_attr(\"final_r2_score\", float(final_r2))\n",
    "\n",
    "    return final_rmse # Optuna minimizes this value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e4cec-523b-4256-9831-6625f53855f6",
   "metadata": {},
   "source": [
    "### Run Optuna Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a66f01e-b7b3-4e2f-8704-f92e31c2d069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study for GNN will be stored at: sqlite:///..\\studies\\gnn_study\\gnn_optuna_study.db\n",
      "Loaded existing study 'gnn_regression_pGI50' from sqlite:///..\\studies\\gnn_study\\gnn_optuna_study.db. Resuming optimization.\n",
      "\n",
      "Starting Optuna optimization for GNN...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ca60f564ed40e0bcf1dc4e221e1c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   0%|          | 00:00/4:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60, Epoch 1/190, Val RMSE: 0.8237, Time: 4.43s\n",
      "Trial 60, Epoch 2/190, Val RMSE: 0.7727, Time: 3.34s\n",
      "Trial 60, Epoch 3/190, Val RMSE: 0.7325, Time: 3.32s\n",
      "Trial 60, Epoch 4/190, Val RMSE: 0.7051, Time: 3.34s\n",
      "Trial 60, Epoch 5/190, Val RMSE: 0.7004, Time: 3.36s\n",
      "Trial 60, Epoch 6/190, Val RMSE: 0.7164, Time: 3.33s\n",
      "Trial 60, Epoch 7/190, Val RMSE: 0.6709, Time: 3.51s\n",
      "Trial 60, Epoch 8/190, Val RMSE: 0.6741, Time: 3.65s\n",
      "Trial 60, Epoch 9/190, Val RMSE: 0.6474, Time: 3.65s\n",
      "Trial 60, Epoch 10/190, Val RMSE: 0.6429, Time: 4.18s\n",
      "Trial 60, Epoch 11/190, Val RMSE: 0.6608, Time: 3.56s\n",
      "Trial 60, Epoch 12/190, Val RMSE: 0.6448, Time: 3.58s\n",
      "Trial 60, Epoch 13/190, Val RMSE: 0.6471, Time: 3.57s\n",
      "Trial 60, Epoch 14/190, Val RMSE: 0.6364, Time: 3.57s\n",
      "Trial 60, Epoch 15/190, Val RMSE: 0.6484, Time: 3.57s\n",
      "Trial 60, Epoch 16/190, Val RMSE: 0.6441, Time: 3.54s\n",
      "Trial 60, Epoch 17/190, Val RMSE: 0.6360, Time: 3.54s\n",
      "Trial 60, Epoch 18/190, Val RMSE: 0.6317, Time: 3.54s\n",
      "Trial 60, Epoch 19/190, Val RMSE: 0.6321, Time: 3.51s\n",
      "Trial 60, Epoch 20/190, Val RMSE: 0.6367, Time: 3.54s\n",
      "Trial 60, Epoch 21/190, Val RMSE: 0.6435, Time: 3.55s\n",
      "Trial 60, Epoch 22/190, Val RMSE: 0.6350, Time: 3.53s\n",
      "Trial 60, Epoch 23/190, Val RMSE: 0.6378, Time: 3.58s\n",
      "Trial 60, Epoch 24/190, Val RMSE: 0.6287, Time: 3.50s\n",
      "Trial 60, Epoch 25/190, Val RMSE: 0.6314, Time: 3.61s\n",
      "Trial 60, Epoch 26/190, Val RMSE: 0.6254, Time: 3.55s\n",
      "Trial 60, Epoch 27/190, Val RMSE: 0.6349, Time: 3.45s\n",
      "Trial 60, Epoch 28/190, Val RMSE: 0.6349, Time: 3.44s\n",
      "Trial 60, Epoch 29/190, Val RMSE: 0.6283, Time: 3.44s\n",
      "Trial 60, Epoch 30/190, Val RMSE: 0.6304, Time: 3.54s\n",
      "Trial 60, Epoch 31/190, Val RMSE: 0.6294, Time: 3.40s\n",
      "Trial 60, Epoch 32/190, Val RMSE: 0.6387, Time: 3.44s\n",
      "Trial 60, Epoch 33/190, Val RMSE: 0.6265, Time: 3.42s\n",
      "Trial 60, Epoch 34/190, Val RMSE: 0.6183, Time: 3.31s\n",
      "Trial 60, Epoch 35/190, Val RMSE: 0.6428, Time: 3.18s\n",
      "Trial 60, Epoch 36/190, Val RMSE: 0.6253, Time: 3.42s\n",
      "Trial 60, Epoch 37/190, Val RMSE: 0.6267, Time: 3.43s\n",
      "Trial 60, Epoch 38/190, Val RMSE: 0.6263, Time: 3.40s\n",
      "Trial 60, Epoch 39/190, Val RMSE: 0.6167, Time: 3.54s\n",
      "Trial 60, Epoch 40/190, Val RMSE: 0.6254, Time: 3.41s\n",
      "Trial 60, Epoch 41/190, Val RMSE: 0.6207, Time: 3.37s\n",
      "Trial 60, Epoch 42/190, Val RMSE: 0.6249, Time: 3.44s\n",
      "Trial 60, Epoch 43/190, Val RMSE: 0.6277, Time: 3.52s\n",
      "Trial 60, Epoch 44/190, Val RMSE: 0.6301, Time: 3.29s\n",
      "Trial 60, Epoch 45/190, Val RMSE: 0.6255, Time: 3.37s\n",
      "Trial 60, Epoch 46/190, Val RMSE: 0.6403, Time: 3.28s\n",
      "Trial 60, Epoch 47/190, Val RMSE: 0.6298, Time: 3.47s\n",
      "Trial 60, Epoch 48/190, Val RMSE: 0.6205, Time: 3.40s\n",
      "Trial 60, Epoch 49/190, Val RMSE: 0.6165, Time: 3.34s\n",
      "Trial 60, Epoch 50/190, Val RMSE: 0.6182, Time: 3.18s\n",
      "Trial 60, Epoch 51/190, Val RMSE: 0.6272, Time: 3.27s\n",
      "Trial 60, Epoch 52/190, Val RMSE: 0.6253, Time: 3.37s\n",
      "Trial 60, Epoch 53/190, Val RMSE: 0.6246, Time: 3.31s\n",
      "Trial 60, Epoch 54/190, Val RMSE: 0.6235, Time: 3.35s\n",
      "Trial 60, Epoch 55/190, Val RMSE: 0.6136, Time: 3.32s\n",
      "Trial 60, Epoch 56/190, Val RMSE: 0.6212, Time: 3.28s\n",
      "Trial 60, Epoch 57/190, Val RMSE: 0.6258, Time: 3.23s\n",
      "Trial 60, Epoch 58/190, Val RMSE: 0.6211, Time: 3.26s\n",
      "Trial 60, Epoch 59/190, Val RMSE: 0.6207, Time: 3.24s\n",
      "Trial 60, Epoch 60/190, Val RMSE: 0.6304, Time: 3.27s\n",
      "Trial 60, Epoch 61/190, Val RMSE: 0.6239, Time: 3.36s\n",
      "Trial 60, Epoch 62/190, Val RMSE: 0.6242, Time: 3.25s\n",
      "Trial 60, Epoch 63/190, Val RMSE: 0.6178, Time: 3.30s\n",
      "Trial 60, Epoch 64/190, Val RMSE: 0.6201, Time: 3.26s\n",
      "Trial 60, Epoch 65/190, Val RMSE: 0.6276, Time: 3.38s\n",
      "Trial 60, Epoch 66/190, Val RMSE: 0.6199, Time: 3.38s\n",
      "Trial 60, Epoch 67/190, Val RMSE: 0.6210, Time: 3.29s\n",
      "Trial 60, Epoch 68/190, Val RMSE: 0.6241, Time: 3.31s\n",
      "Trial 60, Epoch 69/190, Val RMSE: 0.6180, Time: 3.30s\n",
      "Trial 60, Epoch 70/190, Val RMSE: 0.6284, Time: 3.28s\n",
      "Trial 60, Epoch 71/190, Val RMSE: 0.6256, Time: 3.30s\n",
      "Trial 60, Epoch 72/190, Val RMSE: 0.6182, Time: 3.30s\n",
      "Trial 60, Epoch 73/190, Val RMSE: 0.6295, Time: 3.37s\n",
      "Trial 60, Epoch 74/190, Val RMSE: 0.6190, Time: 3.46s\n",
      "Trial 60, Epoch 75/190, Val RMSE: 0.6174, Time: 3.35s\n",
      "Trial 60, Epoch 76/190, Val RMSE: 0.6192, Time: 3.35s\n",
      "Trial 60, Epoch 77/190, Val RMSE: 0.6184, Time: 3.30s\n",
      "Trial 60, Epoch 78/190, Val RMSE: 0.6163, Time: 3.30s\n",
      "Trial 60, Epoch 79/190, Val RMSE: 0.6278, Time: 3.38s\n",
      "Trial 60, Epoch 80/190, Val RMSE: 0.6284, Time: 3.34s\n",
      "Trial 60, Epoch 81/190, Val RMSE: 0.6157, Time: 3.39s\n",
      "Trial 60, Epoch 82/190, Val RMSE: 0.6196, Time: 3.32s\n",
      "Trial 60, Epoch 83/190, Val RMSE: 0.6218, Time: 3.34s\n",
      "Trial 60, Epoch 84/190, Val RMSE: 0.6284, Time: 3.34s\n",
      "Trial 60, Epoch 85/190, Val RMSE: 0.6181, Time: 3.39s\n",
      "Trial 60, Epoch 86/190, Val RMSE: 0.6143, Time: 3.55s\n",
      "Trial 60, Epoch 87/190, Val RMSE: 0.6161, Time: 3.36s\n",
      "Trial 60, Epoch 88/190, Val RMSE: 0.6220, Time: 3.34s\n",
      "Trial 60, Epoch 89/190, Val RMSE: 0.6211, Time: 3.32s\n",
      "Trial 60, Epoch 90/190, Val RMSE: 0.6219, Time: 3.30s\n",
      "Trial 60, Epoch 91/190, Val RMSE: 0.6201, Time: 3.29s\n",
      "Trial 60, Epoch 92/190, Val RMSE: 0.6311, Time: 3.30s\n",
      "Trial 60, Epoch 93/190, Val RMSE: 0.6210, Time: 3.29s\n",
      "Trial 60, Epoch 94/190, Val RMSE: 0.6182, Time: 3.30s\n",
      "Trial 60, Epoch 95/190, Val RMSE: 0.6184, Time: 3.43s\n",
      "Trial 60, Epoch 96/190, Val RMSE: 0.6164, Time: 3.36s\n",
      "Trial 60, Epoch 97/190, Val RMSE: 0.6176, Time: 3.31s\n",
      "Trial 60, Epoch 98/190, Val RMSE: 0.6120, Time: 3.33s\n",
      "Trial 60, Epoch 99/190, Val RMSE: 0.6225, Time: 3.36s\n",
      "Trial 60, Epoch 100/190, Val RMSE: 0.6253, Time: 3.28s\n",
      "Trial 60, Epoch 101/190, Val RMSE: 0.6195, Time: 3.34s\n",
      "Trial 60, Epoch 102/190, Val RMSE: 0.6182, Time: 3.34s\n",
      "Trial 60, Epoch 103/190, Val RMSE: 0.6129, Time: 3.20s\n",
      "Trial 60, Epoch 104/190, Val RMSE: 0.6205, Time: 3.22s\n",
      "Trial 60, Epoch 105/190, Val RMSE: 0.6237, Time: 3.33s\n",
      "Trial 60, Epoch 106/190, Val RMSE: 0.6134, Time: 3.29s\n",
      "Trial 60, Epoch 107/190, Val RMSE: 0.6204, Time: 3.40s\n",
      "Trial 60, Epoch 108/190, Val RMSE: 0.6168, Time: 3.38s\n",
      "Trial 60, Epoch 109/190, Val RMSE: 0.6183, Time: 3.32s\n",
      "Trial 60, Epoch 110/190, Val RMSE: 0.6273, Time: 3.38s\n",
      "Trial 60, Epoch 111/190, Val RMSE: 0.6215, Time: 3.32s\n",
      "Trial 60, Epoch 112/190, Val RMSE: 0.6136, Time: 3.33s\n",
      "Trial 60, Epoch 113/190, Val RMSE: 0.6169, Time: 3.34s\n",
      "Trial 60, Epoch 114/190, Val RMSE: 0.6257, Time: 3.55s\n",
      "Trial 60, Epoch 115/190, Val RMSE: 0.6172, Time: 3.32s\n",
      "Trial 60, Epoch 116/190, Val RMSE: 0.6161, Time: 3.37s\n",
      "Trial 60, Epoch 117/190, Val RMSE: 0.6110, Time: 3.36s\n",
      "Trial 60, Epoch 118/190, Val RMSE: 0.6162, Time: 3.38s\n",
      "Trial 60, Epoch 119/190, Val RMSE: 0.6163, Time: 3.33s\n",
      "Trial 60, Epoch 120/190, Val RMSE: 0.6185, Time: 3.33s\n",
      "Trial 60, Epoch 121/190, Val RMSE: 0.6169, Time: 3.40s\n",
      "Trial 60, Epoch 122/190, Val RMSE: 0.6143, Time: 3.40s\n",
      "Trial 60, Epoch 123/190, Val RMSE: 0.6150, Time: 3.40s\n",
      "Trial 60, Epoch 124/190, Val RMSE: 0.6177, Time: 3.31s\n",
      "Trial 60, Epoch 125/190, Val RMSE: 0.6146, Time: 3.32s\n",
      "Trial 60, Epoch 126/190, Val RMSE: 0.6134, Time: 3.52s\n",
      "Trial 60, Epoch 127/190, Val RMSE: 0.6199, Time: 5.03s\n",
      "Trial 60, Epoch 128/190, Val RMSE: 0.6186, Time: 4.95s\n",
      "Trial 60, Epoch 129/190, Val RMSE: 0.6232, Time: 5.17s\n",
      "Trial 60, Epoch 130/190, Val RMSE: 0.6138, Time: 4.45s\n",
      "Trial 60, Epoch 131/190, Val RMSE: 0.6170, Time: 3.77s\n",
      "Trial 60, Epoch 132/190, Val RMSE: 0.6132, Time: 3.44s\n",
      "Trial 60, Epoch 133/190, Val RMSE: 0.6132, Time: 3.40s\n",
      "Trial 60, Epoch 134/190, Val RMSE: 0.6231, Time: 3.37s\n",
      "Trial 60, Epoch 135/190, Val RMSE: 0.6179, Time: 3.37s\n",
      "Trial 60, Epoch 136/190, Val RMSE: 0.6174, Time: 3.32s\n",
      "Trial 60, Epoch 137/190, Val RMSE: 0.6219, Time: 3.36s\n",
      "Trial 60, Epoch 138/190, Val RMSE: 0.6131, Time: 3.27s\n",
      "Trial 60, Epoch 139/190, Val RMSE: 0.6138, Time: 3.30s\n",
      "Trial 60, Epoch 140/190, Val RMSE: 0.6158, Time: 3.29s\n",
      "Trial 60, Epoch 141/190, Val RMSE: 0.6150, Time: 3.31s\n",
      "Trial 60, Epoch 142/190, Val RMSE: 0.6127, Time: 3.30s\n",
      "Trial 60, Epoch 143/190, Val RMSE: 0.6111, Time: 3.36s\n",
      "Trial 60, Epoch 144/190, Val RMSE: 0.6227, Time: 3.34s\n",
      "Trial 60, Epoch 145/190, Val RMSE: 0.6120, Time: 3.33s\n",
      "Trial 60, Epoch 146/190, Val RMSE: 0.6216, Time: 3.33s\n",
      "Trial 60, Epoch 147/190, Val RMSE: 0.6175, Time: 3.30s\n",
      "Trial 60, Epoch 148/190, Val RMSE: 0.6128, Time: 3.34s\n",
      "Trial 60, Epoch 149/190, Val RMSE: 0.6248, Time: 3.33s\n",
      "Trial 60, Epoch 150/190, Val RMSE: 0.6120, Time: 3.34s\n",
      "Trial 60, Epoch 151/190, Val RMSE: 0.6184, Time: 3.34s\n",
      "Trial 60, Epoch 152/190, Val RMSE: 0.6202, Time: 3.33s\n",
      "Trial 60, Epoch 153/190, Val RMSE: 0.6195, Time: 3.34s\n",
      "Trial 60, Epoch 154/190, Val RMSE: 0.6108, Time: 3.33s\n",
      "Trial 60, Epoch 155/190, Val RMSE: 0.6107, Time: 3.30s\n",
      "Trial 60, Epoch 156/190, Val RMSE: 0.6143, Time: 3.32s\n",
      "Trial 60, Epoch 157/190, Val RMSE: 0.6175, Time: 3.33s\n",
      "Trial 60, Epoch 158/190, Val RMSE: 0.6121, Time: 3.27s\n",
      "Trial 60, Epoch 159/190, Val RMSE: 0.6123, Time: 3.31s\n",
      "Trial 60, Epoch 160/190, Val RMSE: 0.6122, Time: 3.24s\n",
      "Trial 60, Epoch 161/190, Val RMSE: 0.6127, Time: 3.27s\n",
      "Trial 60, Epoch 162/190, Val RMSE: 0.6144, Time: 3.26s\n",
      "Trial 60, Epoch 163/190, Val RMSE: 0.6098, Time: 3.30s\n",
      "Trial 60, Epoch 164/190, Val RMSE: 0.6110, Time: 3.28s\n",
      "Trial 60, Epoch 165/190, Val RMSE: 0.6150, Time: 3.29s\n",
      "Trial 60, Epoch 166/190, Val RMSE: 0.6109, Time: 3.25s\n",
      "Trial 60, Epoch 167/190, Val RMSE: 0.6247, Time: 3.30s\n",
      "Trial 60, Epoch 168/190, Val RMSE: 0.6135, Time: 3.29s\n",
      "Trial 60, Epoch 169/190, Val RMSE: 0.6106, Time: 3.26s\n",
      "Trial 60, Epoch 170/190, Val RMSE: 0.6113, Time: 3.32s\n",
      "Trial 60, Epoch 171/190, Val RMSE: 0.6113, Time: 3.31s\n",
      "Trial 60, Epoch 172/190, Val RMSE: 0.6165, Time: 3.34s\n",
      "Trial 60, Epoch 173/190, Val RMSE: 0.6131, Time: 3.33s\n",
      "Trial 60, Epoch 174/190, Val RMSE: 0.6093, Time: 3.32s\n",
      "Trial 60, Epoch 175/190, Val RMSE: 0.6124, Time: 3.30s\n",
      "Trial 60, Epoch 176/190, Val RMSE: 0.6100, Time: 3.26s\n",
      "Trial 60, Epoch 177/190, Val RMSE: 0.6150, Time: 3.28s\n",
      "Trial 60, Epoch 178/190, Val RMSE: 0.6141, Time: 3.33s\n",
      "Trial 60, Epoch 179/190, Val RMSE: 0.6113, Time: 3.31s\n",
      "Trial 60, Epoch 180/190, Val RMSE: 0.6094, Time: 3.32s\n",
      "Trial 60, Epoch 181/190, Val RMSE: 0.6160, Time: 3.26s\n",
      "Trial 60, Epoch 182/190, Val RMSE: 0.6135, Time: 3.31s\n",
      "Trial 60, Epoch 183/190, Val RMSE: 0.6086, Time: 3.29s\n",
      "Trial 60, Epoch 184/190, Val RMSE: 0.6103, Time: 3.28s\n",
      "Trial 60, Epoch 185/190, Val RMSE: 0.6133, Time: 3.28s\n",
      "Trial 60, Epoch 186/190, Val RMSE: 0.6117, Time: 3.27s\n",
      "Trial 60, Epoch 187/190, Val RMSE: 0.6133, Time: 3.36s\n",
      "Trial 60, Epoch 188/190, Val RMSE: 0.6194, Time: 3.28s\n",
      "Trial 60, Epoch 189/190, Val RMSE: 0.6134, Time: 3.33s\n",
      "Trial 60, Epoch 190/190, Val RMSE: 0.6093, Time: 3.34s\n",
      "[I 2025-07-17 11:06:19,340] Trial 60 finished with value: 0.609277393700007 and parameters: {'hidden_channels': 999, 'lr': 0.0002654886343578734, 'batch_size': 128, 'n_epochs': 190, 'num_layers': 1, 'dropout_rate': 0.12998376396007172, 'weight_decay': 4.855663649252953e-08}. Best is trial 60 with value: 0.609277393700007.\n",
      "Trial 61, Epoch 1/184, Val RMSE: 0.8016, Time: 3.32s\n",
      "Trial 61, Epoch 2/184, Val RMSE: 0.7886, Time: 3.30s\n",
      "Trial 61, Epoch 3/184, Val RMSE: 0.7441, Time: 3.31s\n",
      "[I 2025-07-17 11:06:29,555] Trial 61 pruned. \n",
      "Trial 62, Epoch 1/245, Val RMSE: 0.7878, Time: 3.31s\n",
      "Trial 62, Epoch 2/245, Val RMSE: 0.7378, Time: 3.31s\n",
      "Trial 62, Epoch 3/245, Val RMSE: 0.7185, Time: 3.27s\n",
      "Trial 62, Epoch 4/245, Val RMSE: 0.7071, Time: 3.26s\n",
      "Trial 62, Epoch 5/245, Val RMSE: 0.6886, Time: 3.27s\n",
      "Trial 62, Epoch 6/245, Val RMSE: 0.6914, Time: 3.24s\n",
      "Trial 62, Epoch 7/245, Val RMSE: 0.6658, Time: 3.31s\n",
      "Trial 62, Epoch 8/245, Val RMSE: 0.6688, Time: 3.34s\n",
      "Trial 62, Epoch 9/245, Val RMSE: 0.6537, Time: 3.27s\n",
      "Trial 62, Epoch 10/245, Val RMSE: 0.6482, Time: 3.43s\n",
      "Trial 62, Epoch 11/245, Val RMSE: 0.6512, Time: 3.40s\n",
      "Trial 62, Epoch 12/245, Val RMSE: 0.6751, Time: 3.42s\n",
      "Trial 62, Epoch 13/245, Val RMSE: 0.6415, Time: 3.40s\n",
      "Trial 62, Epoch 14/245, Val RMSE: 0.6537, Time: 3.35s\n",
      "Trial 62, Epoch 15/245, Val RMSE: 0.6763, Time: 3.31s\n",
      "Trial 62, Epoch 16/245, Val RMSE: 0.6399, Time: 3.29s\n",
      "Trial 62, Epoch 17/245, Val RMSE: 0.6508, Time: 3.28s\n",
      "Trial 62, Epoch 18/245, Val RMSE: 0.6496, Time: 3.26s\n",
      "Trial 62, Epoch 19/245, Val RMSE: 0.6314, Time: 3.27s\n",
      "Trial 62, Epoch 20/245, Val RMSE: 0.6414, Time: 3.32s\n",
      "Trial 62, Epoch 21/245, Val RMSE: 0.6635, Time: 3.33s\n",
      "Trial 62, Epoch 22/245, Val RMSE: 0.6458, Time: 3.29s\n",
      "Trial 62, Epoch 23/245, Val RMSE: 0.6410, Time: 3.31s\n",
      "Trial 62, Epoch 24/245, Val RMSE: 0.6462, Time: 3.31s\n",
      "Trial 62, Epoch 25/245, Val RMSE: 0.6524, Time: 3.35s\n",
      "Trial 62, Epoch 26/245, Val RMSE: 0.6559, Time: 3.33s\n",
      "Trial 62, Epoch 27/245, Val RMSE: 0.6461, Time: 3.31s\n",
      "Trial 62, Epoch 28/245, Val RMSE: 0.6574, Time: 3.27s\n",
      "Trial 62, Epoch 29/245, Val RMSE: 0.6411, Time: 3.27s\n",
      "Trial 62, Epoch 30/245, Val RMSE: 0.6408, Time: 3.28s\n",
      "Trial 62, Epoch 31/245, Val RMSE: 0.6453, Time: 3.24s\n",
      "Trial 62, Epoch 32/245, Val RMSE: 0.6404, Time: 3.29s\n",
      "Trial 62, Epoch 33/245, Val RMSE: 0.6423, Time: 3.38s\n",
      "Trial 62, Epoch 34/245, Val RMSE: 0.6421, Time: 3.31s\n",
      "Trial 62, Epoch 35/245, Val RMSE: 0.6375, Time: 3.33s\n",
      "Trial 62, Epoch 36/245, Val RMSE: 0.6329, Time: 3.35s\n",
      "Trial 62, Epoch 37/245, Val RMSE: 0.6500, Time: 3.45s\n",
      "Trial 62, Epoch 38/245, Val RMSE: 0.6368, Time: 3.34s\n",
      "Trial 62, Epoch 39/245, Val RMSE: 0.6459, Time: 3.26s\n",
      "Trial 62, Epoch 40/245, Val RMSE: 0.6353, Time: 3.31s\n",
      "Trial 62, Epoch 41/245, Val RMSE: 0.6401, Time: 3.24s\n",
      "Trial 62, Epoch 42/245, Val RMSE: 0.6361, Time: 3.22s\n",
      "Trial 62, Epoch 43/245, Val RMSE: 0.6486, Time: 3.28s\n",
      "Trial 62, Epoch 44/245, Val RMSE: 0.6302, Time: 3.28s\n",
      "Trial 62, Epoch 45/245, Val RMSE: 0.6400, Time: 3.30s\n",
      "Trial 62, Epoch 46/245, Val RMSE: 0.6297, Time: 3.30s\n",
      "Trial 62, Epoch 47/245, Val RMSE: 0.6329, Time: 3.31s\n",
      "Trial 62, Epoch 48/245, Val RMSE: 0.6399, Time: 3.30s\n",
      "Trial 62, Epoch 49/245, Val RMSE: 0.6422, Time: 3.29s\n",
      "Trial 62, Epoch 50/245, Val RMSE: 0.6338, Time: 3.31s\n",
      "Trial 62, Epoch 51/245, Val RMSE: 0.6390, Time: 3.26s\n",
      "Trial 62, Epoch 52/245, Val RMSE: 0.6342, Time: 3.32s\n",
      "Trial 62, Epoch 53/245, Val RMSE: 0.6329, Time: 3.30s\n",
      "Trial 62, Epoch 54/245, Val RMSE: 0.6315, Time: 3.29s\n",
      "Trial 62, Epoch 55/245, Val RMSE: 0.6348, Time: 3.29s\n",
      "Trial 62, Epoch 56/245, Val RMSE: 0.6340, Time: 3.26s\n",
      "Trial 62, Epoch 57/245, Val RMSE: 0.6330, Time: 3.33s\n",
      "Trial 62, Epoch 58/245, Val RMSE: 0.6375, Time: 3.32s\n",
      "Trial 62, Epoch 59/245, Val RMSE: 0.6335, Time: 3.27s\n",
      "Trial 62, Epoch 60/245, Val RMSE: 0.6370, Time: 3.25s\n",
      "Trial 62, Epoch 61/245, Val RMSE: 0.6461, Time: 3.30s\n",
      "Trial 62, Epoch 62/245, Val RMSE: 0.6317, Time: 3.29s\n",
      "Trial 62, Epoch 63/245, Val RMSE: 0.6284, Time: 3.28s\n",
      "Trial 62, Epoch 64/245, Val RMSE: 0.6342, Time: 3.30s\n",
      "Trial 62, Epoch 65/245, Val RMSE: 0.6273, Time: 3.29s\n",
      "Trial 62, Epoch 66/245, Val RMSE: 0.6367, Time: 3.25s\n",
      "Trial 62, Epoch 67/245, Val RMSE: 0.6351, Time: 3.28s\n",
      "Trial 62, Epoch 68/245, Val RMSE: 0.6303, Time: 3.25s\n",
      "Trial 62, Epoch 69/245, Val RMSE: 0.6371, Time: 3.27s\n",
      "Trial 62, Epoch 70/245, Val RMSE: 0.6362, Time: 3.30s\n",
      "Trial 62, Epoch 71/245, Val RMSE: 0.6311, Time: 3.30s\n",
      "Trial 62, Epoch 72/245, Val RMSE: 0.6402, Time: 3.30s\n",
      "Trial 62, Epoch 73/245, Val RMSE: 0.6332, Time: 3.30s\n",
      "Trial 62, Epoch 74/245, Val RMSE: 0.6289, Time: 3.33s\n",
      "Trial 62, Epoch 75/245, Val RMSE: 0.6350, Time: 3.24s\n",
      "Trial 62, Epoch 76/245, Val RMSE: 0.6253, Time: 3.22s\n",
      "Trial 62, Epoch 77/245, Val RMSE: 0.6375, Time: 3.30s\n",
      "Trial 62, Epoch 78/245, Val RMSE: 0.6380, Time: 3.25s\n",
      "Trial 62, Epoch 79/245, Val RMSE: 0.6338, Time: 3.34s\n",
      "Trial 62, Epoch 80/245, Val RMSE: 0.6415, Time: 3.28s\n",
      "Trial 62, Epoch 81/245, Val RMSE: 0.6310, Time: 3.29s\n",
      "Trial 62, Epoch 82/245, Val RMSE: 0.6309, Time: 3.26s\n",
      "Trial 62, Epoch 83/245, Val RMSE: 0.6336, Time: 3.25s\n",
      "Trial 62, Epoch 84/245, Val RMSE: 0.6243, Time: 3.16s\n",
      "Trial 62, Epoch 85/245, Val RMSE: 0.6275, Time: 3.33s\n",
      "Trial 62, Epoch 86/245, Val RMSE: 0.6364, Time: 3.28s\n",
      "Trial 62, Epoch 87/245, Val RMSE: 0.6305, Time: 3.31s\n",
      "Trial 62, Epoch 88/245, Val RMSE: 0.6287, Time: 3.32s\n",
      "Trial 62, Epoch 89/245, Val RMSE: 0.6281, Time: 3.31s\n",
      "Trial 62, Epoch 90/245, Val RMSE: 0.6252, Time: 3.26s\n",
      "Trial 62, Epoch 91/245, Val RMSE: 0.6362, Time: 3.32s\n",
      "Trial 62, Epoch 92/245, Val RMSE: 0.6282, Time: 3.25s\n",
      "Trial 62, Epoch 93/245, Val RMSE: 0.6332, Time: 3.35s\n",
      "Trial 62, Epoch 94/245, Val RMSE: 0.6275, Time: 3.33s\n",
      "Trial 62, Epoch 95/245, Val RMSE: 0.6299, Time: 3.35s\n",
      "Trial 62, Epoch 96/245, Val RMSE: 0.6308, Time: 3.31s\n",
      "Trial 62, Epoch 97/245, Val RMSE: 0.6328, Time: 3.31s\n",
      "Trial 62, Epoch 98/245, Val RMSE: 0.6347, Time: 3.53s\n",
      "Trial 62, Epoch 99/245, Val RMSE: 0.6302, Time: 3.31s\n",
      "Trial 62, Epoch 100/245, Val RMSE: 0.6324, Time: 3.30s\n",
      "Trial 62, Epoch 101/245, Val RMSE: 0.6222, Time: 3.30s\n",
      "Trial 62, Epoch 102/245, Val RMSE: 0.6247, Time: 3.27s\n",
      "Trial 62, Epoch 103/245, Val RMSE: 0.6372, Time: 3.31s\n",
      "Trial 62, Epoch 104/245, Val RMSE: 0.6256, Time: 3.30s\n",
      "Trial 62, Epoch 105/245, Val RMSE: 0.6293, Time: 3.28s\n",
      "Trial 62, Epoch 106/245, Val RMSE: 0.6280, Time: 3.29s\n",
      "Trial 62, Epoch 107/245, Val RMSE: 0.6283, Time: 3.32s\n",
      "Trial 62, Epoch 108/245, Val RMSE: 0.6313, Time: 3.27s\n",
      "Trial 62, Epoch 109/245, Val RMSE: 0.6244, Time: 3.30s\n",
      "Trial 62, Epoch 110/245, Val RMSE: 0.6290, Time: 3.29s\n",
      "Trial 62, Epoch 111/245, Val RMSE: 0.6315, Time: 3.31s\n",
      "Trial 62, Epoch 112/245, Val RMSE: 0.6349, Time: 3.25s\n",
      "Trial 62, Epoch 113/245, Val RMSE: 0.6291, Time: 3.26s\n",
      "Trial 62, Epoch 114/245, Val RMSE: 0.6289, Time: 3.27s\n",
      "Trial 62, Epoch 115/245, Val RMSE: 0.6305, Time: 3.33s\n",
      "Trial 62, Epoch 116/245, Val RMSE: 0.6258, Time: 3.30s\n",
      "Trial 62, Epoch 117/245, Val RMSE: 0.6245, Time: 3.29s\n",
      "Trial 62, Epoch 118/245, Val RMSE: 0.6285, Time: 3.29s\n",
      "Trial 62, Epoch 119/245, Val RMSE: 0.6270, Time: 3.30s\n",
      "Trial 62, Epoch 120/245, Val RMSE: 0.6263, Time: 3.28s\n",
      "Trial 62, Epoch 121/245, Val RMSE: 0.6265, Time: 3.24s\n",
      "Trial 62, Epoch 122/245, Val RMSE: 0.6385, Time: 3.29s\n",
      "Trial 62, Epoch 123/245, Val RMSE: 0.6261, Time: 3.26s\n",
      "Trial 62, Epoch 124/245, Val RMSE: 0.6208, Time: 3.29s\n",
      "Trial 62, Epoch 125/245, Val RMSE: 0.6294, Time: 3.29s\n",
      "Trial 62, Epoch 126/245, Val RMSE: 0.6299, Time: 3.27s\n",
      "Trial 62, Epoch 127/245, Val RMSE: 0.6275, Time: 3.24s\n",
      "Trial 62, Epoch 128/245, Val RMSE: 0.6259, Time: 3.27s\n",
      "Trial 62, Epoch 129/245, Val RMSE: 0.6277, Time: 3.33s\n",
      "Trial 62, Epoch 130/245, Val RMSE: 0.6318, Time: 3.31s\n",
      "Trial 62, Epoch 131/245, Val RMSE: 0.6349, Time: 3.30s\n",
      "Trial 62, Epoch 132/245, Val RMSE: 0.6294, Time: 3.29s\n",
      "Trial 62, Epoch 133/245, Val RMSE: 0.6314, Time: 3.28s\n",
      "Trial 62, Epoch 134/245, Val RMSE: 0.6317, Time: 3.31s\n",
      "Trial 62, Epoch 135/245, Val RMSE: 0.6257, Time: 3.33s\n",
      "Trial 62, Epoch 136/245, Val RMSE: 0.6216, Time: 3.30s\n",
      "Trial 62, Epoch 137/245, Val RMSE: 0.6285, Time: 3.30s\n",
      "Trial 62, Epoch 138/245, Val RMSE: 0.6282, Time: 3.33s\n",
      "Trial 62, Epoch 139/245, Val RMSE: 0.6294, Time: 3.29s\n",
      "Trial 62, Epoch 140/245, Val RMSE: 0.6239, Time: 3.30s\n",
      "Trial 62, Epoch 141/245, Val RMSE: 0.6270, Time: 3.31s\n",
      "Trial 62, Epoch 142/245, Val RMSE: 0.6289, Time: 3.29s\n",
      "Trial 62, Epoch 143/245, Val RMSE: 0.6242, Time: 3.33s\n",
      "Trial 62, Epoch 144/245, Val RMSE: 0.6252, Time: 3.31s\n",
      "Trial 62, Epoch 145/245, Val RMSE: 0.6255, Time: 3.29s\n",
      "Trial 62, Epoch 146/245, Val RMSE: 0.6288, Time: 3.32s\n",
      "Trial 62, Epoch 147/245, Val RMSE: 0.6216, Time: 3.29s\n",
      "Trial 62, Epoch 148/245, Val RMSE: 0.6263, Time: 3.28s\n",
      "Trial 62, Epoch 149/245, Val RMSE: 0.6383, Time: 3.24s\n",
      "Trial 62, Epoch 150/245, Val RMSE: 0.6257, Time: 3.20s\n",
      "[I 2025-07-17 11:14:47,761] Trial 62 pruned. \n",
      "Trial 63, Epoch 1/150, Val RMSE: 0.8079, Time: 3.26s\n",
      "Trial 63, Epoch 2/150, Val RMSE: 0.7931, Time: 3.29s\n",
      "Trial 63, Epoch 3/150, Val RMSE: 0.7144, Time: 3.27s\n",
      "Trial 63, Epoch 4/150, Val RMSE: 0.7183, Time: 3.29s\n",
      "Trial 63, Epoch 5/150, Val RMSE: 0.6893, Time: 3.28s\n",
      "Trial 63, Epoch 6/150, Val RMSE: 0.6999, Time: 3.31s\n",
      "Trial 63, Epoch 7/150, Val RMSE: 0.6706, Time: 3.28s\n",
      "Trial 63, Epoch 8/150, Val RMSE: 0.6510, Time: 3.25s\n",
      "Trial 63, Epoch 9/150, Val RMSE: 0.6806, Time: 3.34s\n",
      "Trial 63, Epoch 10/150, Val RMSE: 0.6459, Time: 3.29s\n",
      "Trial 63, Epoch 11/150, Val RMSE: 0.6427, Time: 3.22s\n",
      "Trial 63, Epoch 12/150, Val RMSE: 0.6496, Time: 3.30s\n",
      "Trial 63, Epoch 13/150, Val RMSE: 0.6586, Time: 3.30s\n",
      "Trial 63, Epoch 14/150, Val RMSE: 0.6391, Time: 3.28s\n",
      "Trial 63, Epoch 15/150, Val RMSE: 0.6342, Time: 3.29s\n",
      "Trial 63, Epoch 16/150, Val RMSE: 0.6350, Time: 3.27s\n",
      "Trial 63, Epoch 17/150, Val RMSE: 0.6528, Time: 3.32s\n",
      "Trial 63, Epoch 18/150, Val RMSE: 0.6426, Time: 3.30s\n",
      "Trial 63, Epoch 19/150, Val RMSE: 0.6376, Time: 3.36s\n",
      "Trial 63, Epoch 20/150, Val RMSE: 0.6441, Time: 3.30s\n",
      "Trial 63, Epoch 21/150, Val RMSE: 0.6402, Time: 3.29s\n",
      "Trial 63, Epoch 22/150, Val RMSE: 0.6424, Time: 3.30s\n",
      "Trial 63, Epoch 23/150, Val RMSE: 0.6320, Time: 3.25s\n",
      "Trial 63, Epoch 24/150, Val RMSE: 0.6435, Time: 3.29s\n",
      "Trial 63, Epoch 25/150, Val RMSE: 0.6376, Time: 3.28s\n",
      "Trial 63, Epoch 26/150, Val RMSE: 0.6437, Time: 3.41s\n",
      "Trial 63, Epoch 27/150, Val RMSE: 0.6357, Time: 3.35s\n",
      "Trial 63, Epoch 28/150, Val RMSE: 0.6325, Time: 3.38s\n",
      "Trial 63, Epoch 29/150, Val RMSE: 0.6330, Time: 3.37s\n",
      "Trial 63, Epoch 30/150, Val RMSE: 0.6398, Time: 3.35s\n",
      "Trial 63, Epoch 31/150, Val RMSE: 0.6292, Time: 3.33s\n",
      "Trial 63, Epoch 32/150, Val RMSE: 0.6394, Time: 3.26s\n",
      "Trial 63, Epoch 33/150, Val RMSE: 0.6366, Time: 3.28s\n",
      "Trial 63, Epoch 34/150, Val RMSE: 0.6300, Time: 3.30s\n",
      "Trial 63, Epoch 35/150, Val RMSE: 0.6404, Time: 3.33s\n",
      "Trial 63, Epoch 36/150, Val RMSE: 0.6322, Time: 3.32s\n",
      "Trial 63, Epoch 37/150, Val RMSE: 0.6382, Time: 3.27s\n",
      "Trial 63, Epoch 38/150, Val RMSE: 0.6349, Time: 3.30s\n",
      "Trial 63, Epoch 39/150, Val RMSE: 0.6343, Time: 3.55s\n",
      "Trial 63, Epoch 40/150, Val RMSE: 0.6312, Time: 3.29s\n",
      "Trial 63, Epoch 41/150, Val RMSE: 0.6356, Time: 3.29s\n",
      "Trial 63, Epoch 42/150, Val RMSE: 0.6397, Time: 3.28s\n",
      "Trial 63, Epoch 43/150, Val RMSE: 0.6325, Time: 3.28s\n",
      "Trial 63, Epoch 44/150, Val RMSE: 0.6329, Time: 3.24s\n",
      "Trial 63, Epoch 45/150, Val RMSE: 0.6298, Time: 3.32s\n",
      "Trial 63, Epoch 46/150, Val RMSE: 0.6323, Time: 3.27s\n",
      "Trial 63, Epoch 47/150, Val RMSE: 0.6451, Time: 3.30s\n",
      "Trial 63, Epoch 48/150, Val RMSE: 0.6335, Time: 3.31s\n",
      "Trial 63, Epoch 49/150, Val RMSE: 0.6291, Time: 3.30s\n",
      "Trial 63, Epoch 50/150, Val RMSE: 0.6336, Time: 3.34s\n",
      "Trial 63, Epoch 51/150, Val RMSE: 0.6347, Time: 3.34s\n",
      "Trial 63, Epoch 52/150, Val RMSE: 0.6398, Time: 3.36s\n",
      "Trial 63, Epoch 53/150, Val RMSE: 0.6398, Time: 3.29s\n",
      "Trial 63, Epoch 54/150, Val RMSE: 0.6383, Time: 3.33s\n",
      "Trial 63, Epoch 55/150, Val RMSE: 0.6320, Time: 3.31s\n",
      "Trial 63, Epoch 56/150, Val RMSE: 0.6423, Time: 3.33s\n",
      "Trial 63, Epoch 57/150, Val RMSE: 0.6278, Time: 3.30s\n",
      "Trial 63, Epoch 58/150, Val RMSE: 0.6355, Time: 3.52s\n",
      "Trial 63, Epoch 59/150, Val RMSE: 0.6399, Time: 3.59s\n",
      "Trial 63, Epoch 60/150, Val RMSE: 0.6412, Time: 3.36s\n",
      "Trial 63, Epoch 61/150, Val RMSE: 0.6287, Time: 3.37s\n",
      "Trial 63, Epoch 62/150, Val RMSE: 0.6317, Time: 3.31s\n",
      "Trial 63, Epoch 63/150, Val RMSE: 0.6507, Time: 3.33s\n",
      "Trial 63, Epoch 64/150, Val RMSE: 0.6315, Time: 3.33s\n",
      "Trial 63, Epoch 65/150, Val RMSE: 0.6486, Time: 3.35s\n",
      "Trial 63, Epoch 66/150, Val RMSE: 0.6266, Time: 3.28s\n",
      "Trial 63, Epoch 67/150, Val RMSE: 0.6325, Time: 3.35s\n",
      "Trial 63, Epoch 68/150, Val RMSE: 0.6355, Time: 3.35s\n",
      "Trial 63, Epoch 69/150, Val RMSE: 0.6387, Time: 3.31s\n",
      "Trial 63, Epoch 70/150, Val RMSE: 0.6273, Time: 3.34s\n",
      "Trial 63, Epoch 71/150, Val RMSE: 0.6304, Time: 3.27s\n",
      "Trial 63, Epoch 72/150, Val RMSE: 0.6398, Time: 3.33s\n",
      "Trial 63, Epoch 73/150, Val RMSE: 0.6293, Time: 3.38s\n",
      "Trial 63, Epoch 74/150, Val RMSE: 0.6284, Time: 3.43s\n",
      "Trial 63, Epoch 75/150, Val RMSE: 0.6315, Time: 3.35s\n",
      "Trial 63, Epoch 76/150, Val RMSE: 0.6301, Time: 3.37s\n",
      "Trial 63, Epoch 77/150, Val RMSE: 0.6353, Time: 3.35s\n",
      "Trial 63, Epoch 78/150, Val RMSE: 0.6321, Time: 3.41s\n",
      "Trial 63, Epoch 79/150, Val RMSE: 0.6361, Time: 3.35s\n",
      "Trial 63, Epoch 80/150, Val RMSE: 0.6316, Time: 3.27s\n",
      "Trial 63, Epoch 81/150, Val RMSE: 0.6284, Time: 3.29s\n",
      "Trial 63, Epoch 82/150, Val RMSE: 0.6328, Time: 3.29s\n",
      "Trial 63, Epoch 83/150, Val RMSE: 0.6347, Time: 3.26s\n",
      "Trial 63, Epoch 84/150, Val RMSE: 0.6305, Time: 3.29s\n",
      "Trial 63, Epoch 85/150, Val RMSE: 0.6421, Time: 3.32s\n",
      "Trial 63, Epoch 86/150, Val RMSE: 0.6315, Time: 3.29s\n",
      "Trial 63, Epoch 87/150, Val RMSE: 0.6322, Time: 3.25s\n",
      "Trial 63, Epoch 88/150, Val RMSE: 0.6345, Time: 3.28s\n",
      "Trial 63, Epoch 89/150, Val RMSE: 0.6287, Time: 3.29s\n",
      "Trial 63, Epoch 90/150, Val RMSE: 0.6319, Time: 3.30s\n",
      "Trial 63, Epoch 91/150, Val RMSE: 0.6331, Time: 3.33s\n",
      "Trial 63, Epoch 92/150, Val RMSE: 0.6349, Time: 3.30s\n",
      "Trial 63, Epoch 93/150, Val RMSE: 0.6321, Time: 3.33s\n",
      "Trial 63, Epoch 94/150, Val RMSE: 0.6305, Time: 3.28s\n",
      "Trial 63, Epoch 95/150, Val RMSE: 0.6287, Time: 3.20s\n",
      "Trial 63, Epoch 96/150, Val RMSE: 0.6270, Time: 3.30s\n",
      "Trial 63, Epoch 97/150, Val RMSE: 0.6402, Time: 3.31s\n",
      "Trial 63, Epoch 98/150, Val RMSE: 0.6276, Time: 3.30s\n",
      "Trial 63, Epoch 99/150, Val RMSE: 0.6269, Time: 3.26s\n",
      "Trial 63, Epoch 100/150, Val RMSE: 0.6297, Time: 3.28s\n",
      "Trial 63, Epoch 101/150, Val RMSE: 0.6339, Time: 3.27s\n",
      "[I 2025-07-17 11:20:25,007] Trial 63 pruned. \n",
      "Trial 64, Epoch 1/171, Val RMSE: 0.8634, Time: 4.73s\n",
      "Trial 64, Epoch 2/171, Val RMSE: 0.7638, Time: 4.62s\n",
      "Trial 64, Epoch 3/171, Val RMSE: 0.7382, Time: 4.69s\n",
      "Trial 64, Epoch 4/171, Val RMSE: 0.7007, Time: 4.67s\n",
      "Trial 64, Epoch 5/171, Val RMSE: 0.6878, Time: 4.66s\n",
      "Trial 64, Epoch 6/171, Val RMSE: 0.6709, Time: 4.71s\n",
      "Trial 64, Epoch 7/171, Val RMSE: 0.6538, Time: 4.71s\n",
      "Trial 64, Epoch 8/171, Val RMSE: 0.6556, Time: 4.67s\n",
      "Trial 64, Epoch 9/171, Val RMSE: 0.6492, Time: 4.68s\n",
      "Trial 64, Epoch 10/171, Val RMSE: 0.6610, Time: 4.63s\n",
      "Trial 64, Epoch 11/171, Val RMSE: 0.6460, Time: 4.76s\n",
      "Trial 64, Epoch 12/171, Val RMSE: 0.6386, Time: 4.67s\n",
      "Trial 64, Epoch 13/171, Val RMSE: 0.6531, Time: 4.62s\n",
      "Trial 64, Epoch 14/171, Val RMSE: 0.6472, Time: 4.57s\n",
      "Trial 64, Epoch 15/171, Val RMSE: 0.6427, Time: 4.69s\n",
      "Trial 64, Epoch 16/171, Val RMSE: 0.6357, Time: 4.73s\n",
      "Trial 64, Epoch 17/171, Val RMSE: 0.6172, Time: 4.67s\n",
      "Trial 64, Epoch 18/171, Val RMSE: 0.6392, Time: 4.72s\n",
      "Trial 64, Epoch 19/171, Val RMSE: 0.6334, Time: 4.74s\n",
      "Trial 64, Epoch 20/171, Val RMSE: 0.6287, Time: 4.85s\n",
      "Trial 64, Epoch 21/171, Val RMSE: 0.6565, Time: 4.67s\n",
      "Trial 64, Epoch 22/171, Val RMSE: 0.6214, Time: 4.58s\n",
      "Trial 64, Epoch 23/171, Val RMSE: 0.6340, Time: 4.70s\n",
      "Trial 64, Epoch 24/171, Val RMSE: 0.6280, Time: 4.71s\n",
      "Trial 64, Epoch 25/171, Val RMSE: 0.6308, Time: 4.75s\n",
      "Trial 64, Epoch 26/171, Val RMSE: 0.6265, Time: 4.72s\n",
      "Trial 64, Epoch 27/171, Val RMSE: 0.6316, Time: 4.65s\n",
      "Trial 64, Epoch 28/171, Val RMSE: 0.6331, Time: 4.63s\n",
      "Trial 64, Epoch 29/171, Val RMSE: 0.6294, Time: 4.69s\n",
      "Trial 64, Epoch 30/171, Val RMSE: 0.6384, Time: 4.70s\n",
      "Trial 64, Epoch 31/171, Val RMSE: 0.6251, Time: 4.74s\n",
      "Trial 64, Epoch 32/171, Val RMSE: 0.6293, Time: 4.77s\n",
      "Trial 64, Epoch 33/171, Val RMSE: 0.6303, Time: 4.79s\n",
      "Trial 64, Epoch 34/171, Val RMSE: 0.6273, Time: 4.70s\n",
      "Trial 64, Epoch 35/171, Val RMSE: 0.6324, Time: 4.69s\n",
      "Trial 64, Epoch 36/171, Val RMSE: 0.6394, Time: 4.70s\n",
      "Trial 64, Epoch 37/171, Val RMSE: 0.6231, Time: 4.70s\n",
      "Trial 64, Epoch 38/171, Val RMSE: 0.6188, Time: 4.70s\n",
      "Trial 64, Epoch 39/171, Val RMSE: 0.6316, Time: 4.68s\n",
      "Trial 64, Epoch 40/171, Val RMSE: 0.6278, Time: 4.71s\n",
      "Trial 64, Epoch 41/171, Val RMSE: 0.6282, Time: 4.69s\n",
      "Trial 64, Epoch 42/171, Val RMSE: 0.6220, Time: 4.71s\n",
      "Trial 64, Epoch 43/171, Val RMSE: 0.6294, Time: 4.73s\n",
      "Trial 64, Epoch 44/171, Val RMSE: 0.6286, Time: 4.75s\n",
      "Trial 64, Epoch 45/171, Val RMSE: 0.6230, Time: 4.74s\n",
      "Trial 64, Epoch 46/171, Val RMSE: 0.6197, Time: 4.70s\n",
      "Trial 64, Epoch 47/171, Val RMSE: 0.6325, Time: 4.71s\n",
      "Trial 64, Epoch 48/171, Val RMSE: 0.6251, Time: 4.71s\n",
      "Trial 64, Epoch 49/171, Val RMSE: 0.6251, Time: 4.76s\n",
      "Trial 64, Epoch 50/171, Val RMSE: 0.6274, Time: 4.69s\n",
      "Trial 64, Epoch 51/171, Val RMSE: 0.6212, Time: 4.70s\n",
      "Trial 64, Epoch 52/171, Val RMSE: 0.6209, Time: 4.69s\n",
      "Trial 64, Epoch 53/171, Val RMSE: 0.6175, Time: 4.67s\n",
      "Trial 64, Epoch 54/171, Val RMSE: 0.6272, Time: 4.67s\n",
      "Trial 64, Epoch 55/171, Val RMSE: 0.6199, Time: 4.64s\n",
      "Trial 64, Epoch 56/171, Val RMSE: 0.6240, Time: 4.67s\n",
      "Trial 64, Epoch 57/171, Val RMSE: 0.6220, Time: 4.69s\n",
      "Trial 64, Epoch 58/171, Val RMSE: 0.6282, Time: 4.72s\n",
      "Trial 64, Epoch 59/171, Val RMSE: 0.6220, Time: 4.68s\n",
      "Trial 64, Epoch 60/171, Val RMSE: 0.6214, Time: 4.67s\n",
      "Trial 64, Epoch 61/171, Val RMSE: 0.6325, Time: 4.66s\n",
      "Trial 64, Epoch 62/171, Val RMSE: 0.6161, Time: 4.67s\n",
      "Trial 64, Epoch 63/171, Val RMSE: 0.6194, Time: 4.78s\n",
      "Trial 64, Epoch 64/171, Val RMSE: 0.6240, Time: 4.66s\n",
      "Trial 64, Epoch 65/171, Val RMSE: 0.6233, Time: 4.67s\n",
      "Trial 64, Epoch 66/171, Val RMSE: 0.6207, Time: 4.69s\n",
      "Trial 64, Epoch 67/171, Val RMSE: 0.6246, Time: 4.66s\n",
      "Trial 64, Epoch 68/171, Val RMSE: 0.6211, Time: 4.64s\n",
      "Trial 64, Epoch 69/171, Val RMSE: 0.6269, Time: 4.60s\n",
      "Trial 64, Epoch 70/171, Val RMSE: 0.6203, Time: 4.70s\n",
      "Trial 64, Epoch 71/171, Val RMSE: 0.6297, Time: 4.53s\n",
      "Trial 64, Epoch 72/171, Val RMSE: 0.6210, Time: 4.66s\n",
      "Trial 64, Epoch 73/171, Val RMSE: 0.6238, Time: 4.59s\n",
      "Trial 64, Epoch 74/171, Val RMSE: 0.6313, Time: 4.67s\n",
      "Trial 64, Epoch 75/171, Val RMSE: 0.6237, Time: 4.74s\n",
      "Trial 64, Epoch 76/171, Val RMSE: 0.6172, Time: 4.63s\n",
      "Trial 64, Epoch 77/171, Val RMSE: 0.6189, Time: 4.73s\n",
      "Trial 64, Epoch 78/171, Val RMSE: 0.6294, Time: 4.69s\n",
      "Trial 64, Epoch 79/171, Val RMSE: 0.6225, Time: 4.54s\n",
      "Trial 64, Epoch 80/171, Val RMSE: 0.6181, Time: 4.58s\n",
      "Trial 64, Epoch 81/171, Val RMSE: 0.6251, Time: 4.58s\n",
      "Trial 64, Epoch 82/171, Val RMSE: 0.6205, Time: 4.66s\n",
      "Trial 64, Epoch 83/171, Val RMSE: 0.6223, Time: 4.85s\n",
      "Trial 64, Epoch 84/171, Val RMSE: 0.6227, Time: 4.77s\n",
      "Trial 64, Epoch 85/171, Val RMSE: 0.6248, Time: 4.90s\n",
      "Trial 64, Epoch 86/171, Val RMSE: 0.6251, Time: 4.88s\n",
      "Trial 64, Epoch 87/171, Val RMSE: 0.6189, Time: 4.78s\n",
      "Trial 64, Epoch 88/171, Val RMSE: 0.6191, Time: 4.64s\n",
      "Trial 64, Epoch 89/171, Val RMSE: 0.6255, Time: 4.68s\n",
      "Trial 64, Epoch 90/171, Val RMSE: 0.6231, Time: 4.71s\n",
      "Trial 64, Epoch 91/171, Val RMSE: 0.6219, Time: 4.69s\n",
      "Trial 64, Epoch 92/171, Val RMSE: 0.6159, Time: 4.71s\n",
      "Trial 64, Epoch 93/171, Val RMSE: 0.6207, Time: 4.70s\n",
      "Trial 64, Epoch 94/171, Val RMSE: 0.6166, Time: 4.63s\n",
      "Trial 64, Epoch 95/171, Val RMSE: 0.6362, Time: 4.72s\n",
      "Trial 64, Epoch 96/171, Val RMSE: 0.6186, Time: 4.72s\n",
      "Trial 64, Epoch 97/171, Val RMSE: 0.6242, Time: 4.76s\n",
      "Trial 64, Epoch 98/171, Val RMSE: 0.6217, Time: 4.69s\n",
      "Trial 64, Epoch 99/171, Val RMSE: 0.6201, Time: 4.68s\n",
      "Trial 64, Epoch 100/171, Val RMSE: 0.6278, Time: 4.71s\n",
      "Trial 64, Epoch 101/171, Val RMSE: 0.6194, Time: 4.69s\n",
      "Trial 64, Epoch 102/171, Val RMSE: 0.6194, Time: 4.71s\n",
      "Trial 64, Epoch 103/171, Val RMSE: 0.6225, Time: 4.72s\n",
      "Trial 64, Epoch 104/171, Val RMSE: 0.6159, Time: 4.63s\n",
      "Trial 64, Epoch 105/171, Val RMSE: 0.6187, Time: 4.64s\n",
      "Trial 64, Epoch 106/171, Val RMSE: 0.6194, Time: 4.69s\n",
      "Trial 64, Epoch 107/171, Val RMSE: 0.6145, Time: 4.67s\n",
      "Trial 64, Epoch 108/171, Val RMSE: 0.6171, Time: 4.59s\n",
      "Trial 64, Epoch 109/171, Val RMSE: 0.6166, Time: 4.69s\n",
      "Trial 64, Epoch 110/171, Val RMSE: 0.6162, Time: 4.70s\n",
      "Trial 64, Epoch 111/171, Val RMSE: 0.6214, Time: 4.69s\n",
      "Trial 64, Epoch 112/171, Val RMSE: 0.6220, Time: 4.67s\n",
      "Trial 64, Epoch 113/171, Val RMSE: 0.6199, Time: 4.70s\n",
      "Trial 64, Epoch 114/171, Val RMSE: 0.6191, Time: 4.62s\n",
      "Trial 64, Epoch 115/171, Val RMSE: 0.6174, Time: 4.70s\n",
      "Trial 64, Epoch 116/171, Val RMSE: 0.6155, Time: 4.71s\n",
      "Trial 64, Epoch 117/171, Val RMSE: 0.6213, Time: 4.69s\n",
      "Trial 64, Epoch 118/171, Val RMSE: 0.6169, Time: 4.72s\n",
      "Trial 64, Epoch 119/171, Val RMSE: 0.6164, Time: 4.73s\n",
      "Trial 64, Epoch 120/171, Val RMSE: 0.6187, Time: 4.70s\n",
      "Trial 64, Epoch 121/171, Val RMSE: 0.6195, Time: 4.73s\n",
      "Trial 64, Epoch 122/171, Val RMSE: 0.6201, Time: 4.68s\n",
      "Trial 64, Epoch 123/171, Val RMSE: 0.6200, Time: 4.71s\n",
      "Trial 64, Epoch 124/171, Val RMSE: 0.6156, Time: 4.73s\n",
      "Trial 64, Epoch 125/171, Val RMSE: 0.6152, Time: 4.68s\n",
      "Trial 64, Epoch 126/171, Val RMSE: 0.6193, Time: 4.71s\n",
      "Trial 64, Epoch 127/171, Val RMSE: 0.6134, Time: 4.66s\n",
      "Trial 64, Epoch 128/171, Val RMSE: 0.6105, Time: 4.73s\n",
      "Trial 64, Epoch 129/171, Val RMSE: 0.6150, Time: 4.74s\n",
      "Trial 64, Epoch 130/171, Val RMSE: 0.6167, Time: 4.65s\n",
      "Trial 64, Epoch 131/171, Val RMSE: 0.6135, Time: 4.70s\n",
      "Trial 64, Epoch 132/171, Val RMSE: 0.6144, Time: 4.66s\n",
      "Trial 64, Epoch 133/171, Val RMSE: 0.6161, Time: 4.65s\n",
      "Trial 64, Epoch 134/171, Val RMSE: 0.6180, Time: 4.62s\n",
      "Trial 64, Epoch 135/171, Val RMSE: 0.6153, Time: 4.68s\n",
      "Trial 64, Epoch 136/171, Val RMSE: 0.6155, Time: 4.68s\n",
      "Trial 64, Epoch 137/171, Val RMSE: 0.6141, Time: 4.65s\n",
      "Trial 64, Epoch 138/171, Val RMSE: 0.6136, Time: 4.67s\n",
      "Trial 64, Epoch 139/171, Val RMSE: 0.6147, Time: 4.71s\n",
      "Trial 64, Epoch 140/171, Val RMSE: 0.6181, Time: 4.66s\n",
      "Trial 64, Epoch 141/171, Val RMSE: 0.6183, Time: 4.68s\n",
      "Trial 64, Epoch 142/171, Val RMSE: 0.6216, Time: 4.72s\n",
      "Trial 64, Epoch 143/171, Val RMSE: 0.6159, Time: 4.76s\n",
      "Trial 64, Epoch 144/171, Val RMSE: 0.6181, Time: 4.71s\n",
      "Trial 64, Epoch 145/171, Val RMSE: 0.6129, Time: 4.74s\n",
      "Trial 64, Epoch 146/171, Val RMSE: 0.6205, Time: 4.59s\n",
      "Trial 64, Epoch 147/171, Val RMSE: 0.6159, Time: 4.80s\n",
      "Trial 64, Epoch 148/171, Val RMSE: 0.6167, Time: 4.69s\n",
      "Trial 64, Epoch 149/171, Val RMSE: 0.6172, Time: 4.70s\n",
      "Trial 64, Epoch 150/171, Val RMSE: 0.6181, Time: 4.66s\n",
      "Trial 64, Epoch 151/171, Val RMSE: 0.6177, Time: 4.65s\n",
      "Trial 64, Epoch 152/171, Val RMSE: 0.6153, Time: 4.70s\n",
      "Trial 64, Epoch 153/171, Val RMSE: 0.6198, Time: 4.62s\n",
      "Trial 64, Epoch 154/171, Val RMSE: 0.6150, Time: 4.68s\n",
      "Trial 64, Epoch 155/171, Val RMSE: 0.6145, Time: 4.67s\n",
      "Trial 64, Epoch 156/171, Val RMSE: 0.6107, Time: 4.61s\n",
      "Trial 64, Epoch 157/171, Val RMSE: 0.6176, Time: 4.65s\n",
      "Trial 64, Epoch 158/171, Val RMSE: 0.6171, Time: 4.68s\n",
      "Trial 64, Epoch 159/171, Val RMSE: 0.6141, Time: 4.66s\n",
      "Trial 64, Epoch 160/171, Val RMSE: 0.6167, Time: 4.61s\n",
      "Trial 64, Epoch 161/171, Val RMSE: 0.6147, Time: 4.60s\n",
      "Trial 64, Epoch 162/171, Val RMSE: 0.6166, Time: 4.64s\n",
      "Trial 64, Epoch 163/171, Val RMSE: 0.6171, Time: 4.64s\n",
      "[I 2025-07-17 11:33:13,057] Trial 64 pruned. \n",
      "Trial 65, Epoch 1/201, Val RMSE: 0.8265, Time: 3.27s\n",
      "Trial 65, Epoch 2/201, Val RMSE: 0.8136, Time: 3.28s\n",
      "[I 2025-07-17 11:33:19,856] Trial 65 pruned. \n",
      "Trial 66, Epoch 1/267, Val RMSE: 0.8371, Time: 2.43s\n",
      "Trial 66, Epoch 2/267, Val RMSE: 0.7573, Time: 2.43s\n",
      "Trial 66, Epoch 3/267, Val RMSE: 0.7149, Time: 2.48s\n",
      "Trial 66, Epoch 4/267, Val RMSE: 0.7113, Time: 2.45s\n",
      "Trial 66, Epoch 5/267, Val RMSE: 0.6854, Time: 2.45s\n",
      "Trial 66, Epoch 6/267, Val RMSE: 0.6976, Time: 2.41s\n",
      "Trial 66, Epoch 7/267, Val RMSE: 0.6763, Time: 2.47s\n",
      "Trial 66, Epoch 8/267, Val RMSE: 0.6537, Time: 2.46s\n",
      "Trial 66, Epoch 9/267, Val RMSE: 0.6540, Time: 2.50s\n",
      "Trial 66, Epoch 10/267, Val RMSE: 0.6841, Time: 2.47s\n",
      "Trial 66, Epoch 11/267, Val RMSE: 0.6592, Time: 2.45s\n",
      "Trial 66, Epoch 12/267, Val RMSE: 0.6620, Time: 2.42s\n",
      "Trial 66, Epoch 13/267, Val RMSE: 0.6530, Time: 2.49s\n",
      "Trial 66, Epoch 14/267, Val RMSE: 0.6353, Time: 2.42s\n",
      "Trial 66, Epoch 15/267, Val RMSE: 0.6646, Time: 2.46s\n",
      "Trial 66, Epoch 16/267, Val RMSE: 0.6553, Time: 2.43s\n",
      "Trial 66, Epoch 17/267, Val RMSE: 0.6541, Time: 2.46s\n",
      "Trial 66, Epoch 18/267, Val RMSE: 0.6486, Time: 2.46s\n",
      "Trial 66, Epoch 19/267, Val RMSE: 0.6414, Time: 2.45s\n",
      "Trial 66, Epoch 20/267, Val RMSE: 0.6400, Time: 2.40s\n",
      "Trial 66, Epoch 21/267, Val RMSE: 0.6393, Time: 2.48s\n",
      "Trial 66, Epoch 22/267, Val RMSE: 0.6529, Time: 2.43s\n",
      "Trial 66, Epoch 23/267, Val RMSE: 0.6686, Time: 2.48s\n",
      "Trial 66, Epoch 24/267, Val RMSE: 0.6478, Time: 2.45s\n",
      "Trial 66, Epoch 25/267, Val RMSE: 0.6431, Time: 2.48s\n",
      "Trial 66, Epoch 26/267, Val RMSE: 0.6538, Time: 2.44s\n",
      "Trial 66, Epoch 27/267, Val RMSE: 0.6460, Time: 2.45s\n",
      "Trial 66, Epoch 28/267, Val RMSE: 0.6467, Time: 2.46s\n",
      "Trial 66, Epoch 29/267, Val RMSE: 0.6443, Time: 2.48s\n",
      "Trial 66, Epoch 30/267, Val RMSE: 0.6381, Time: 2.48s\n",
      "Trial 66, Epoch 31/267, Val RMSE: 0.6418, Time: 2.45s\n",
      "Trial 66, Epoch 32/267, Val RMSE: 0.6807, Time: 2.46s\n",
      "Trial 66, Epoch 33/267, Val RMSE: 0.6366, Time: 2.47s\n",
      "Trial 66, Epoch 34/267, Val RMSE: 0.6438, Time: 2.44s\n",
      "Trial 66, Epoch 35/267, Val RMSE: 0.6413, Time: 2.46s\n",
      "Trial 66, Epoch 36/267, Val RMSE: 0.6412, Time: 2.47s\n",
      "Trial 66, Epoch 37/267, Val RMSE: 0.6452, Time: 2.47s\n",
      "Trial 66, Epoch 38/267, Val RMSE: 0.6372, Time: 2.43s\n",
      "Trial 66, Epoch 39/267, Val RMSE: 0.6415, Time: 2.42s\n",
      "Trial 66, Epoch 40/267, Val RMSE: 0.6409, Time: 2.43s\n",
      "Trial 66, Epoch 41/267, Val RMSE: 0.6430, Time: 2.46s\n",
      "Trial 66, Epoch 42/267, Val RMSE: 0.6374, Time: 2.45s\n",
      "Trial 66, Epoch 43/267, Val RMSE: 0.6368, Time: 2.47s\n",
      "Trial 66, Epoch 44/267, Val RMSE: 0.6521, Time: 2.44s\n",
      "Trial 66, Epoch 45/267, Val RMSE: 0.6402, Time: 2.48s\n",
      "Trial 66, Epoch 46/267, Val RMSE: 0.6381, Time: 2.40s\n",
      "Trial 66, Epoch 47/267, Val RMSE: 0.6478, Time: 2.43s\n",
      "Trial 66, Epoch 48/267, Val RMSE: 0.6413, Time: 2.43s\n",
      "Trial 66, Epoch 49/267, Val RMSE: 0.6366, Time: 2.46s\n",
      "Trial 66, Epoch 50/267, Val RMSE: 0.6418, Time: 2.48s\n",
      "Trial 66, Epoch 51/267, Val RMSE: 0.6514, Time: 2.43s\n",
      "Trial 66, Epoch 52/267, Val RMSE: 0.6467, Time: 2.48s\n",
      "Trial 66, Epoch 53/267, Val RMSE: 0.6360, Time: 2.37s\n",
      "Trial 66, Epoch 54/267, Val RMSE: 0.6469, Time: 2.47s\n",
      "Trial 66, Epoch 55/267, Val RMSE: 0.6365, Time: 2.50s\n",
      "Trial 66, Epoch 56/267, Val RMSE: 0.6431, Time: 2.51s\n",
      "Trial 66, Epoch 57/267, Val RMSE: 0.6382, Time: 2.48s\n",
      "Trial 66, Epoch 58/267, Val RMSE: 0.6400, Time: 2.46s\n",
      "Trial 66, Epoch 59/267, Val RMSE: 0.6445, Time: 2.45s\n",
      "Trial 66, Epoch 60/267, Val RMSE: 0.6455, Time: 2.42s\n",
      "Trial 66, Epoch 61/267, Val RMSE: 0.6408, Time: 2.44s\n",
      "Trial 66, Epoch 62/267, Val RMSE: 0.6433, Time: 2.49s\n",
      "Trial 66, Epoch 63/267, Val RMSE: 0.6398, Time: 2.47s\n",
      "Trial 66, Epoch 64/267, Val RMSE: 0.6509, Time: 2.47s\n",
      "Early stopping at epoch 64 for trial 66\n",
      "[I 2025-07-17 11:35:58,793] Trial 66 finished with value: 0.6509163012698056 and parameters: {'hidden_channels': 680, 'lr': 0.00017472100651736344, 'batch_size': 256, 'n_epochs': 267, 'num_layers': 1, 'dropout_rate': 0.2503328157705171, 'weight_decay': 3.04231057965054e-08}. Best is trial 60 with value: 0.609277393700007.\n",
      "Trial 67, Epoch 1/195, Val RMSE: 0.8415, Time: 3.35s\n",
      "Trial 67, Epoch 2/195, Val RMSE: 0.7462, Time: 3.37s\n",
      "Trial 67, Epoch 3/195, Val RMSE: 0.7341, Time: 3.36s\n",
      "Trial 67, Epoch 4/195, Val RMSE: 0.6968, Time: 3.34s\n",
      "Trial 67, Epoch 5/195, Val RMSE: 0.6892, Time: 3.38s\n",
      "Trial 67, Epoch 6/195, Val RMSE: 0.6719, Time: 3.39s\n",
      "Trial 67, Epoch 7/195, Val RMSE: 0.6795, Time: 3.41s\n",
      "Trial 67, Epoch 8/195, Val RMSE: 0.6916, Time: 3.34s\n",
      "Trial 67, Epoch 9/195, Val RMSE: 0.6639, Time: 3.41s\n",
      "Trial 67, Epoch 10/195, Val RMSE: 0.6643, Time: 3.30s\n",
      "Trial 67, Epoch 11/195, Val RMSE: 0.6563, Time: 3.31s\n",
      "Trial 67, Epoch 12/195, Val RMSE: 0.6592, Time: 3.29s\n",
      "Trial 67, Epoch 13/195, Val RMSE: 0.6658, Time: 3.32s\n",
      "Trial 67, Epoch 14/195, Val RMSE: 0.6436, Time: 3.30s\n",
      "Trial 67, Epoch 15/195, Val RMSE: 0.6570, Time: 3.27s\n",
      "Trial 67, Epoch 16/195, Val RMSE: 0.6522, Time: 3.35s\n",
      "Trial 67, Epoch 17/195, Val RMSE: 0.6385, Time: 3.54s\n",
      "Trial 67, Epoch 18/195, Val RMSE: 0.6444, Time: 3.32s\n",
      "Trial 67, Epoch 19/195, Val RMSE: 0.6502, Time: 3.31s\n",
      "Trial 67, Epoch 20/195, Val RMSE: 0.6397, Time: 3.28s\n",
      "Trial 67, Epoch 21/195, Val RMSE: 0.6520, Time: 3.33s\n",
      "Trial 67, Epoch 22/195, Val RMSE: 0.6446, Time: 3.32s\n",
      "Trial 67, Epoch 23/195, Val RMSE: 0.6390, Time: 3.31s\n",
      "Trial 67, Epoch 24/195, Val RMSE: 0.6485, Time: 3.32s\n",
      "Trial 67, Epoch 25/195, Val RMSE: 0.6390, Time: 3.44s\n",
      "Trial 67, Epoch 26/195, Val RMSE: 0.6470, Time: 3.40s\n",
      "Trial 67, Epoch 27/195, Val RMSE: 0.6455, Time: 3.36s\n",
      "Trial 67, Epoch 28/195, Val RMSE: 0.6407, Time: 3.35s\n",
      "Trial 67, Epoch 29/195, Val RMSE: 0.6521, Time: 3.38s\n",
      "Trial 67, Epoch 30/195, Val RMSE: 0.6439, Time: 3.42s\n",
      "Trial 67, Epoch 31/195, Val RMSE: 0.6469, Time: 3.41s\n",
      "Trial 67, Epoch 32/195, Val RMSE: 0.6388, Time: 3.35s\n",
      "Trial 67, Epoch 33/195, Val RMSE: 0.6337, Time: 3.32s\n",
      "Trial 67, Epoch 34/195, Val RMSE: 0.6493, Time: 3.38s\n",
      "Trial 67, Epoch 35/195, Val RMSE: 0.6375, Time: 3.32s\n",
      "Trial 67, Epoch 36/195, Val RMSE: 0.6380, Time: 3.35s\n",
      "Trial 67, Epoch 37/195, Val RMSE: 0.6366, Time: 3.33s\n",
      "Trial 67, Epoch 38/195, Val RMSE: 0.6389, Time: 3.32s\n",
      "Trial 67, Epoch 39/195, Val RMSE: 0.6365, Time: 3.34s\n",
      "Trial 67, Epoch 40/195, Val RMSE: 0.6356, Time: 3.29s\n",
      "Trial 67, Epoch 41/195, Val RMSE: 0.6337, Time: 3.30s\n",
      "Trial 67, Epoch 42/195, Val RMSE: 0.6353, Time: 3.32s\n",
      "Trial 67, Epoch 43/195, Val RMSE: 0.6343, Time: 3.34s\n",
      "Trial 67, Epoch 44/195, Val RMSE: 0.6347, Time: 3.34s\n",
      "Trial 67, Epoch 45/195, Val RMSE: 0.6291, Time: 3.31s\n",
      "Trial 67, Epoch 46/195, Val RMSE: 0.6337, Time: 3.29s\n",
      "Trial 67, Epoch 47/195, Val RMSE: 0.6358, Time: 3.29s\n",
      "Trial 67, Epoch 48/195, Val RMSE: 0.6342, Time: 3.27s\n",
      "Trial 67, Epoch 49/195, Val RMSE: 0.6362, Time: 3.30s\n",
      "Trial 67, Epoch 50/195, Val RMSE: 0.6334, Time: 3.29s\n",
      "Trial 67, Epoch 51/195, Val RMSE: 0.6326, Time: 3.26s\n",
      "Trial 67, Epoch 52/195, Val RMSE: 0.6311, Time: 3.33s\n",
      "Trial 67, Epoch 53/195, Val RMSE: 0.6305, Time: 3.25s\n",
      "Trial 67, Epoch 54/195, Val RMSE: 0.6294, Time: 3.36s\n",
      "Trial 67, Epoch 55/195, Val RMSE: 0.6346, Time: 3.33s\n",
      "Trial 67, Epoch 56/195, Val RMSE: 0.6330, Time: 3.26s\n",
      "Trial 67, Epoch 57/195, Val RMSE: 0.6284, Time: 3.27s\n",
      "Trial 67, Epoch 58/195, Val RMSE: 0.6343, Time: 3.26s\n",
      "Trial 67, Epoch 59/195, Val RMSE: 0.6391, Time: 3.33s\n",
      "Trial 67, Epoch 60/195, Val RMSE: 0.6350, Time: 3.29s\n",
      "Trial 67, Epoch 61/195, Val RMSE: 0.6349, Time: 3.30s\n",
      "Trial 67, Epoch 62/195, Val RMSE: 0.6384, Time: 3.32s\n",
      "Trial 67, Epoch 63/195, Val RMSE: 0.6297, Time: 3.25s\n",
      "Trial 67, Epoch 64/195, Val RMSE: 0.6357, Time: 3.27s\n",
      "Trial 67, Epoch 65/195, Val RMSE: 0.6282, Time: 3.34s\n",
      "Trial 67, Epoch 66/195, Val RMSE: 0.6374, Time: 3.31s\n",
      "Trial 67, Epoch 67/195, Val RMSE: 0.6360, Time: 3.37s\n",
      "Trial 67, Epoch 68/195, Val RMSE: 0.6326, Time: 3.34s\n",
      "Trial 67, Epoch 69/195, Val RMSE: 0.6453, Time: 3.33s\n",
      "Trial 67, Epoch 70/195, Val RMSE: 0.6314, Time: 3.40s\n",
      "Trial 67, Epoch 71/195, Val RMSE: 0.6292, Time: 3.32s\n",
      "Trial 67, Epoch 72/195, Val RMSE: 0.6312, Time: 3.37s\n",
      "Trial 67, Epoch 73/195, Val RMSE: 0.6314, Time: 3.33s\n",
      "Trial 67, Epoch 74/195, Val RMSE: 0.6324, Time: 3.34s\n",
      "Trial 67, Epoch 75/195, Val RMSE: 0.6352, Time: 3.30s\n",
      "Trial 67, Epoch 76/195, Val RMSE: 0.6311, Time: 3.33s\n",
      "Trial 67, Epoch 77/195, Val RMSE: 0.6332, Time: 3.33s\n",
      "Trial 67, Epoch 78/195, Val RMSE: 0.6292, Time: 3.30s\n",
      "Trial 67, Epoch 79/195, Val RMSE: 0.6311, Time: 3.35s\n",
      "Trial 67, Epoch 80/195, Val RMSE: 0.6295, Time: 3.31s\n",
      "Trial 67, Epoch 81/195, Val RMSE: 0.6305, Time: 3.37s\n",
      "Trial 67, Epoch 82/195, Val RMSE: 0.6318, Time: 3.32s\n",
      "Trial 67, Epoch 83/195, Val RMSE: 0.6288, Time: 3.31s\n",
      "Trial 67, Epoch 84/195, Val RMSE: 0.6320, Time: 3.22s\n",
      "Trial 67, Epoch 85/195, Val RMSE: 0.6318, Time: 3.35s\n",
      "Trial 67, Epoch 86/195, Val RMSE: 0.6278, Time: 3.29s\n",
      "Trial 67, Epoch 87/195, Val RMSE: 0.6284, Time: 3.29s\n",
      "Trial 67, Epoch 88/195, Val RMSE: 0.6300, Time: 3.52s\n",
      "Trial 67, Epoch 89/195, Val RMSE: 0.6336, Time: 3.37s\n",
      "Trial 67, Epoch 90/195, Val RMSE: 0.6265, Time: 3.52s\n",
      "Trial 67, Epoch 91/195, Val RMSE: 0.6267, Time: 4.09s\n",
      "Trial 67, Epoch 92/195, Val RMSE: 0.6290, Time: 3.53s\n",
      "Trial 67, Epoch 93/195, Val RMSE: 0.6331, Time: 3.35s\n",
      "Trial 67, Epoch 94/195, Val RMSE: 0.6350, Time: 3.31s\n",
      "Trial 67, Epoch 95/195, Val RMSE: 0.6299, Time: 3.32s\n",
      "Trial 67, Epoch 96/195, Val RMSE: 0.6327, Time: 3.38s\n",
      "Trial 67, Epoch 97/195, Val RMSE: 0.6276, Time: 3.43s\n",
      "Trial 67, Epoch 98/195, Val RMSE: 0.6290, Time: 3.30s\n",
      "Trial 67, Epoch 99/195, Val RMSE: 0.6241, Time: 3.35s\n",
      "Trial 67, Epoch 100/195, Val RMSE: 0.6269, Time: 3.32s\n",
      "Trial 67, Epoch 101/195, Val RMSE: 0.6312, Time: 3.38s\n",
      "Trial 67, Epoch 102/195, Val RMSE: 0.6325, Time: 3.34s\n",
      "Trial 67, Epoch 103/195, Val RMSE: 0.6355, Time: 3.32s\n",
      "Trial 67, Epoch 104/195, Val RMSE: 0.6290, Time: 3.34s\n",
      "Trial 67, Epoch 105/195, Val RMSE: 0.6325, Time: 3.40s\n",
      "Trial 67, Epoch 106/195, Val RMSE: 0.6244, Time: 3.41s\n",
      "Trial 67, Epoch 107/195, Val RMSE: 0.6381, Time: 3.44s\n",
      "Trial 67, Epoch 108/195, Val RMSE: 0.6311, Time: 3.37s\n",
      "Trial 67, Epoch 109/195, Val RMSE: 0.6257, Time: 3.36s\n",
      "Trial 67, Epoch 110/195, Val RMSE: 0.6270, Time: 3.33s\n",
      "Trial 67, Epoch 111/195, Val RMSE: 0.6255, Time: 3.34s\n",
      "Trial 67, Epoch 112/195, Val RMSE: 0.6414, Time: 3.31s\n",
      "Trial 67, Epoch 113/195, Val RMSE: 0.6317, Time: 3.33s\n",
      "Trial 67, Epoch 114/195, Val RMSE: 0.6296, Time: 3.33s\n",
      "Trial 67, Epoch 115/195, Val RMSE: 0.6322, Time: 3.31s\n",
      "Trial 67, Epoch 116/195, Val RMSE: 0.6327, Time: 3.35s\n",
      "Trial 67, Epoch 117/195, Val RMSE: 0.6347, Time: 3.29s\n",
      "Trial 67, Epoch 118/195, Val RMSE: 0.6270, Time: 3.25s\n",
      "Trial 67, Epoch 119/195, Val RMSE: 0.6239, Time: 3.28s\n",
      "Trial 67, Epoch 120/195, Val RMSE: 0.6250, Time: 3.29s\n",
      "Trial 67, Epoch 121/195, Val RMSE: 0.6303, Time: 3.27s\n",
      "Trial 67, Epoch 122/195, Val RMSE: 0.6292, Time: 3.30s\n",
      "Trial 67, Epoch 123/195, Val RMSE: 0.6277, Time: 3.27s\n",
      "Trial 67, Epoch 124/195, Val RMSE: 0.6280, Time: 3.28s\n",
      "Trial 67, Epoch 125/195, Val RMSE: 0.6356, Time: 3.33s\n",
      "Trial 67, Epoch 126/195, Val RMSE: 0.6280, Time: 3.36s\n",
      "Trial 67, Epoch 127/195, Val RMSE: 0.6308, Time: 3.28s\n",
      "Trial 67, Epoch 128/195, Val RMSE: 0.6243, Time: 3.29s\n",
      "Trial 67, Epoch 129/195, Val RMSE: 0.6309, Time: 3.28s\n",
      "Trial 67, Epoch 130/195, Val RMSE: 0.6279, Time: 3.29s\n",
      "Trial 67, Epoch 131/195, Val RMSE: 0.6325, Time: 3.32s\n",
      "Trial 67, Epoch 132/195, Val RMSE: 0.6269, Time: 3.28s\n",
      "Trial 67, Epoch 133/195, Val RMSE: 0.6294, Time: 3.29s\n",
      "Trial 67, Epoch 134/195, Val RMSE: 0.6267, Time: 3.28s\n",
      "Trial 67, Epoch 135/195, Val RMSE: 0.6272, Time: 3.31s\n",
      "Trial 67, Epoch 136/195, Val RMSE: 0.6370, Time: 3.31s\n",
      "Trial 67, Epoch 137/195, Val RMSE: 0.6263, Time: 3.33s\n",
      "Trial 67, Epoch 138/195, Val RMSE: 0.6249, Time: 3.32s\n",
      "Trial 67, Epoch 139/195, Val RMSE: 0.6340, Time: 3.31s\n",
      "Trial 67, Epoch 140/195, Val RMSE: 0.6299, Time: 3.31s\n",
      "Trial 67, Epoch 141/195, Val RMSE: 0.6318, Time: 3.36s\n",
      "Trial 67, Epoch 142/195, Val RMSE: 0.6322, Time: 3.35s\n",
      "Trial 67, Epoch 143/195, Val RMSE: 0.6271, Time: 3.32s\n",
      "[I 2025-07-17 11:43:59,458] Trial 67 pruned. \n",
      "Trial 68, Epoch 1/221, Val RMSE: 0.7950, Time: 3.32s\n",
      "Trial 68, Epoch 2/221, Val RMSE: 0.7302, Time: 3.33s\n",
      "Trial 68, Epoch 3/221, Val RMSE: 0.7225, Time: 3.30s\n",
      "Trial 68, Epoch 4/221, Val RMSE: 0.6919, Time: 3.33s\n",
      "Trial 68, Epoch 5/221, Val RMSE: 0.6912, Time: 3.32s\n",
      "Trial 68, Epoch 6/221, Val RMSE: 0.6798, Time: 3.37s\n",
      "Trial 68, Epoch 7/221, Val RMSE: 0.6608, Time: 3.29s\n",
      "Trial 68, Epoch 8/221, Val RMSE: 0.6553, Time: 3.32s\n",
      "Trial 68, Epoch 9/221, Val RMSE: 0.6478, Time: 3.31s\n",
      "Trial 68, Epoch 10/221, Val RMSE: 0.6543, Time: 3.27s\n",
      "Trial 68, Epoch 11/221, Val RMSE: 0.6447, Time: 3.31s\n",
      "Trial 68, Epoch 12/221, Val RMSE: 0.6486, Time: 3.26s\n",
      "Trial 68, Epoch 13/221, Val RMSE: 0.6417, Time: 3.28s\n",
      "Trial 68, Epoch 14/221, Val RMSE: 0.6402, Time: 3.28s\n",
      "Trial 68, Epoch 15/221, Val RMSE: 0.6442, Time: 3.28s\n",
      "Trial 68, Epoch 16/221, Val RMSE: 0.6356, Time: 3.29s\n",
      "Trial 68, Epoch 17/221, Val RMSE: 0.6309, Time: 3.32s\n",
      "Trial 68, Epoch 18/221, Val RMSE: 0.6453, Time: 3.24s\n",
      "Trial 68, Epoch 19/221, Val RMSE: 0.6355, Time: 3.31s\n",
      "Trial 68, Epoch 20/221, Val RMSE: 0.6342, Time: 3.21s\n",
      "Trial 68, Epoch 21/221, Val RMSE: 0.6392, Time: 3.30s\n",
      "Trial 68, Epoch 22/221, Val RMSE: 0.6372, Time: 3.27s\n",
      "Trial 68, Epoch 23/221, Val RMSE: 0.6334, Time: 3.30s\n",
      "Trial 68, Epoch 24/221, Val RMSE: 0.6395, Time: 3.27s\n",
      "Trial 68, Epoch 25/221, Val RMSE: 0.6334, Time: 3.27s\n",
      "Trial 68, Epoch 26/221, Val RMSE: 0.6337, Time: 3.25s\n",
      "Trial 68, Epoch 27/221, Val RMSE: 0.6346, Time: 3.29s\n",
      "Trial 68, Epoch 28/221, Val RMSE: 0.6303, Time: 3.24s\n",
      "Trial 68, Epoch 29/221, Val RMSE: 0.6350, Time: 3.31s\n",
      "Trial 68, Epoch 30/221, Val RMSE: 0.6411, Time: 3.28s\n",
      "Trial 68, Epoch 31/221, Val RMSE: 0.6359, Time: 3.29s\n",
      "Trial 68, Epoch 32/221, Val RMSE: 0.6370, Time: 3.29s\n",
      "Trial 68, Epoch 33/221, Val RMSE: 0.6304, Time: 3.23s\n",
      "Trial 68, Epoch 34/221, Val RMSE: 0.6312, Time: 3.29s\n",
      "Trial 68, Epoch 35/221, Val RMSE: 0.6377, Time: 3.34s\n",
      "Trial 68, Epoch 36/221, Val RMSE: 0.6347, Time: 3.30s\n",
      "Trial 68, Epoch 37/221, Val RMSE: 0.6320, Time: 3.32s\n",
      "Trial 68, Epoch 38/221, Val RMSE: 0.6326, Time: 3.29s\n",
      "Trial 68, Epoch 39/221, Val RMSE: 0.6312, Time: 3.31s\n",
      "Trial 68, Epoch 40/221, Val RMSE: 0.6368, Time: 3.30s\n",
      "Trial 68, Epoch 41/221, Val RMSE: 0.6313, Time: 3.38s\n",
      "Trial 68, Epoch 42/221, Val RMSE: 0.6407, Time: 3.43s\n",
      "Trial 68, Epoch 43/221, Val RMSE: 0.6305, Time: 3.45s\n",
      "Trial 68, Epoch 44/221, Val RMSE: 0.6303, Time: 3.41s\n",
      "Trial 68, Epoch 45/221, Val RMSE: 0.6334, Time: 3.40s\n",
      "Trial 68, Epoch 46/221, Val RMSE: 0.6307, Time: 3.23s\n",
      "Trial 68, Epoch 47/221, Val RMSE: 0.6317, Time: 3.30s\n",
      "Trial 68, Epoch 48/221, Val RMSE: 0.6393, Time: 3.37s\n",
      "Trial 68, Epoch 49/221, Val RMSE: 0.6274, Time: 3.36s\n",
      "Trial 68, Epoch 50/221, Val RMSE: 0.6263, Time: 3.29s\n",
      "Trial 68, Epoch 51/221, Val RMSE: 0.6283, Time: 3.25s\n",
      "Trial 68, Epoch 52/221, Val RMSE: 0.6359, Time: 3.29s\n",
      "Trial 68, Epoch 53/221, Val RMSE: 0.6279, Time: 3.53s\n",
      "Trial 68, Epoch 54/221, Val RMSE: 0.6240, Time: 3.22s\n",
      "Trial 68, Epoch 55/221, Val RMSE: 0.6298, Time: 3.22s\n",
      "Trial 68, Epoch 56/221, Val RMSE: 0.6321, Time: 3.27s\n",
      "Trial 68, Epoch 57/221, Val RMSE: 0.6314, Time: 3.26s\n",
      "Trial 68, Epoch 58/221, Val RMSE: 0.6257, Time: 3.27s\n",
      "Trial 68, Epoch 59/221, Val RMSE: 0.6279, Time: 3.28s\n",
      "Trial 68, Epoch 60/221, Val RMSE: 0.6284, Time: 3.28s\n",
      "Trial 68, Epoch 61/221, Val RMSE: 0.6223, Time: 3.18s\n",
      "Trial 68, Epoch 62/221, Val RMSE: 0.6359, Time: 3.28s\n",
      "Trial 68, Epoch 63/221, Val RMSE: 0.6275, Time: 3.25s\n",
      "Trial 68, Epoch 64/221, Val RMSE: 0.6323, Time: 3.28s\n",
      "Trial 68, Epoch 65/221, Val RMSE: 0.6266, Time: 3.26s\n",
      "Trial 68, Epoch 66/221, Val RMSE: 0.6334, Time: 3.30s\n",
      "Trial 68, Epoch 67/221, Val RMSE: 0.6267, Time: 3.28s\n",
      "Trial 68, Epoch 68/221, Val RMSE: 0.6264, Time: 3.32s\n",
      "Trial 68, Epoch 69/221, Val RMSE: 0.6351, Time: 3.26s\n",
      "Trial 68, Epoch 70/221, Val RMSE: 0.6276, Time: 3.33s\n",
      "Trial 68, Epoch 71/221, Val RMSE: 0.6261, Time: 3.32s\n",
      "Trial 68, Epoch 72/221, Val RMSE: 0.6340, Time: 3.36s\n",
      "Trial 68, Epoch 73/221, Val RMSE: 0.6290, Time: 3.29s\n",
      "Trial 68, Epoch 74/221, Val RMSE: 0.6207, Time: 3.30s\n",
      "Trial 68, Epoch 75/221, Val RMSE: 0.6291, Time: 3.25s\n",
      "Trial 68, Epoch 76/221, Val RMSE: 0.6187, Time: 3.27s\n",
      "Trial 68, Epoch 77/221, Val RMSE: 0.6301, Time: 3.32s\n",
      "Trial 68, Epoch 78/221, Val RMSE: 0.6298, Time: 3.29s\n",
      "Trial 68, Epoch 79/221, Val RMSE: 0.6311, Time: 3.28s\n",
      "Trial 68, Epoch 80/221, Val RMSE: 0.6302, Time: 3.30s\n",
      "Trial 68, Epoch 81/221, Val RMSE: 0.6245, Time: 3.30s\n",
      "Trial 68, Epoch 82/221, Val RMSE: 0.6275, Time: 3.24s\n",
      "Trial 68, Epoch 83/221, Val RMSE: 0.6251, Time: 3.29s\n",
      "Trial 68, Epoch 84/221, Val RMSE: 0.6191, Time: 3.24s\n",
      "Trial 68, Epoch 85/221, Val RMSE: 0.6296, Time: 3.24s\n",
      "Trial 68, Epoch 86/221, Val RMSE: 0.6203, Time: 3.25s\n",
      "Trial 68, Epoch 87/221, Val RMSE: 0.6272, Time: 3.23s\n",
      "Trial 68, Epoch 88/221, Val RMSE: 0.6222, Time: 3.27s\n",
      "Trial 68, Epoch 89/221, Val RMSE: 0.6287, Time: 3.25s\n",
      "Trial 68, Epoch 90/221, Val RMSE: 0.6210, Time: 3.30s\n",
      "Trial 68, Epoch 91/221, Val RMSE: 0.6225, Time: 3.34s\n",
      "Trial 68, Epoch 92/221, Val RMSE: 0.6303, Time: 3.29s\n",
      "Trial 68, Epoch 93/221, Val RMSE: 0.6230, Time: 3.28s\n",
      "Trial 68, Epoch 94/221, Val RMSE: 0.6223, Time: 3.29s\n",
      "Trial 68, Epoch 95/221, Val RMSE: 0.6208, Time: 3.33s\n",
      "Trial 68, Epoch 96/221, Val RMSE: 0.6239, Time: 3.29s\n",
      "Trial 68, Epoch 97/221, Val RMSE: 0.6316, Time: 3.28s\n",
      "Trial 68, Epoch 98/221, Val RMSE: 0.6224, Time: 3.34s\n",
      "Trial 68, Epoch 99/221, Val RMSE: 0.6203, Time: 3.33s\n",
      "Trial 68, Epoch 100/221, Val RMSE: 0.6232, Time: 3.28s\n",
      "Trial 68, Epoch 101/221, Val RMSE: 0.6234, Time: 3.46s\n",
      "Trial 68, Epoch 102/221, Val RMSE: 0.6201, Time: 3.43s\n",
      "Trial 68, Epoch 103/221, Val RMSE: 0.6251, Time: 3.40s\n",
      "Trial 68, Epoch 104/221, Val RMSE: 0.6237, Time: 3.37s\n",
      "Trial 68, Epoch 105/221, Val RMSE: 0.6260, Time: 3.33s\n",
      "Trial 68, Epoch 106/221, Val RMSE: 0.6213, Time: 3.30s\n",
      "Trial 68, Epoch 107/221, Val RMSE: 0.6206, Time: 3.31s\n",
      "Trial 68, Epoch 108/221, Val RMSE: 0.6269, Time: 3.30s\n",
      "Trial 68, Epoch 109/221, Val RMSE: 0.6233, Time: 3.29s\n",
      "Trial 68, Epoch 110/221, Val RMSE: 0.6238, Time: 3.25s\n",
      "Trial 68, Epoch 111/221, Val RMSE: 0.6228, Time: 3.33s\n",
      "Trial 68, Epoch 112/221, Val RMSE: 0.6199, Time: 3.28s\n",
      "Trial 68, Epoch 113/221, Val RMSE: 0.6262, Time: 3.33s\n",
      "Trial 68, Epoch 114/221, Val RMSE: 0.6234, Time: 3.29s\n",
      "Trial 68, Epoch 115/221, Val RMSE: 0.6216, Time: 3.34s\n",
      "Trial 68, Epoch 116/221, Val RMSE: 0.6206, Time: 3.34s\n",
      "Trial 68, Epoch 117/221, Val RMSE: 0.6322, Time: 3.31s\n",
      "Trial 68, Epoch 118/221, Val RMSE: 0.6218, Time: 3.31s\n",
      "Trial 68, Epoch 119/221, Val RMSE: 0.6258, Time: 3.24s\n",
      "Trial 68, Epoch 120/221, Val RMSE: 0.6233, Time: 3.29s\n",
      "Trial 68, Epoch 121/221, Val RMSE: 0.6310, Time: 3.32s\n",
      "Trial 68, Epoch 122/221, Val RMSE: 0.6254, Time: 3.27s\n",
      "Trial 68, Epoch 123/221, Val RMSE: 0.6349, Time: 3.25s\n",
      "Trial 68, Epoch 124/221, Val RMSE: 0.6232, Time: 3.32s\n",
      "Trial 68, Epoch 125/221, Val RMSE: 0.6248, Time: 3.29s\n",
      "Trial 68, Epoch 126/221, Val RMSE: 0.6262, Time: 3.27s\n",
      "Early stopping at epoch 126 for trial 68\n",
      "[I 2025-07-17 11:50:58,584] Trial 68 finished with value: 0.6262438494799293 and parameters: {'hidden_channels': 736, 'lr': 0.00010430042670203735, 'batch_size': 128, 'n_epochs': 221, 'num_layers': 1, 'dropout_rate': 0.23577340014296744, 'weight_decay': 4.9689907455549516e-08}. Best is trial 60 with value: 0.609277393700007.\n",
      "Trial 69, Epoch 1/221, Val RMSE: 0.7952, Time: 3.31s\n",
      "Trial 69, Epoch 2/221, Val RMSE: 0.7360, Time: 3.32s\n",
      "Trial 69, Epoch 3/221, Val RMSE: 0.7310, Time: 3.28s\n",
      "Trial 69, Epoch 4/221, Val RMSE: 0.7046, Time: 3.24s\n",
      "Trial 69, Epoch 5/221, Val RMSE: 0.6892, Time: 3.26s\n",
      "Trial 69, Epoch 6/221, Val RMSE: 0.6738, Time: 3.27s\n",
      "Trial 69, Epoch 7/221, Val RMSE: 0.6692, Time: 3.28s\n",
      "Trial 69, Epoch 8/221, Val RMSE: 0.6552, Time: 3.33s\n",
      "Trial 69, Epoch 9/221, Val RMSE: 0.6455, Time: 3.35s\n",
      "Trial 69, Epoch 10/221, Val RMSE: 0.6502, Time: 3.28s\n",
      "Trial 69, Epoch 11/221, Val RMSE: 0.6594, Time: 3.28s\n",
      "Trial 69, Epoch 12/221, Val RMSE: 0.6357, Time: 3.27s\n",
      "Trial 69, Epoch 13/221, Val RMSE: 0.6422, Time: 3.27s\n",
      "Trial 69, Epoch 14/221, Val RMSE: 0.6436, Time: 3.31s\n",
      "Trial 69, Epoch 15/221, Val RMSE: 0.6540, Time: 3.32s\n",
      "Trial 69, Epoch 16/221, Val RMSE: 0.6412, Time: 3.36s\n",
      "Trial 69, Epoch 17/221, Val RMSE: 0.6397, Time: 3.31s\n",
      "Trial 69, Epoch 18/221, Val RMSE: 0.6582, Time: 3.30s\n",
      "Trial 69, Epoch 19/221, Val RMSE: 0.6496, Time: 3.57s\n",
      "Trial 69, Epoch 20/221, Val RMSE: 0.6616, Time: 3.32s\n",
      "Trial 69, Epoch 21/221, Val RMSE: 0.6423, Time: 3.33s\n",
      "Trial 69, Epoch 22/221, Val RMSE: 0.6412, Time: 3.36s\n",
      "Trial 69, Epoch 23/221, Val RMSE: 0.6371, Time: 3.33s\n",
      "Trial 69, Epoch 24/221, Val RMSE: 0.6420, Time: 3.29s\n",
      "Trial 69, Epoch 25/221, Val RMSE: 0.6395, Time: 3.36s\n",
      "Trial 69, Epoch 26/221, Val RMSE: 0.6522, Time: 3.36s\n",
      "Trial 69, Epoch 27/221, Val RMSE: 0.6376, Time: 3.30s\n",
      "Trial 69, Epoch 28/221, Val RMSE: 0.6338, Time: 3.29s\n",
      "Trial 69, Epoch 29/221, Val RMSE: 0.6441, Time: 3.25s\n",
      "Trial 69, Epoch 30/221, Val RMSE: 0.6364, Time: 3.28s\n",
      "Trial 69, Epoch 31/221, Val RMSE: 0.6485, Time: 3.29s\n",
      "Trial 69, Epoch 32/221, Val RMSE: 0.6415, Time: 3.29s\n",
      "Trial 69, Epoch 33/221, Val RMSE: 0.6343, Time: 3.25s\n",
      "Trial 69, Epoch 34/221, Val RMSE: 0.6559, Time: 3.32s\n",
      "Trial 69, Epoch 35/221, Val RMSE: 0.6402, Time: 3.27s\n",
      "Trial 69, Epoch 36/221, Val RMSE: 0.6485, Time: 3.28s\n",
      "Trial 69, Epoch 37/221, Val RMSE: 0.6379, Time: 3.28s\n",
      "Trial 69, Epoch 38/221, Val RMSE: 0.6359, Time: 3.28s\n",
      "Trial 69, Epoch 39/221, Val RMSE: 0.6470, Time: 3.24s\n",
      "Trial 69, Epoch 40/221, Val RMSE: 0.6344, Time: 3.20s\n",
      "Trial 69, Epoch 41/221, Val RMSE: 0.6401, Time: 3.24s\n",
      "Trial 69, Epoch 42/221, Val RMSE: 0.6417, Time: 3.29s\n",
      "Trial 69, Epoch 43/221, Val RMSE: 0.6352, Time: 3.33s\n",
      "Trial 69, Epoch 44/221, Val RMSE: 0.6568, Time: 3.28s\n",
      "Trial 69, Epoch 45/221, Val RMSE: 0.6401, Time: 3.28s\n",
      "Trial 69, Epoch 46/221, Val RMSE: 0.6349, Time: 3.29s\n",
      "Trial 69, Epoch 47/221, Val RMSE: 0.6338, Time: 3.30s\n",
      "Trial 69, Epoch 48/221, Val RMSE: 0.6345, Time: 3.26s\n",
      "Trial 69, Epoch 49/221, Val RMSE: 0.6319, Time: 3.18s\n",
      "Trial 69, Epoch 50/221, Val RMSE: 0.6331, Time: 3.31s\n",
      "Trial 69, Epoch 51/221, Val RMSE: 0.6367, Time: 3.30s\n",
      "Trial 69, Epoch 52/221, Val RMSE: 0.6339, Time: 3.35s\n",
      "Trial 69, Epoch 53/221, Val RMSE: 0.6432, Time: 3.31s\n",
      "Trial 69, Epoch 54/221, Val RMSE: 0.6363, Time: 3.28s\n",
      "Trial 69, Epoch 55/221, Val RMSE: 0.6396, Time: 3.24s\n",
      "Trial 69, Epoch 56/221, Val RMSE: 0.6485, Time: 3.29s\n",
      "Trial 69, Epoch 57/221, Val RMSE: 0.6315, Time: 3.34s\n",
      "Trial 69, Epoch 58/221, Val RMSE: 0.6295, Time: 3.33s\n",
      "Trial 69, Epoch 59/221, Val RMSE: 0.6362, Time: 3.28s\n",
      "Trial 69, Epoch 60/221, Val RMSE: 0.6385, Time: 3.27s\n",
      "Trial 69, Epoch 61/221, Val RMSE: 0.6350, Time: 3.29s\n",
      "Trial 69, Epoch 62/221, Val RMSE: 0.6332, Time: 3.30s\n",
      "Trial 69, Epoch 63/221, Val RMSE: 0.6330, Time: 3.27s\n",
      "Trial 69, Epoch 64/221, Val RMSE: 0.6300, Time: 3.21s\n",
      "Trial 69, Epoch 65/221, Val RMSE: 0.6365, Time: 3.24s\n",
      "Trial 69, Epoch 66/221, Val RMSE: 0.6396, Time: 3.25s\n",
      "Trial 69, Epoch 67/221, Val RMSE: 0.6383, Time: 3.26s\n",
      "Trial 69, Epoch 68/221, Val RMSE: 0.6367, Time: 3.24s\n",
      "Trial 69, Epoch 69/221, Val RMSE: 0.6310, Time: 3.22s\n",
      "Trial 69, Epoch 70/221, Val RMSE: 0.6340, Time: 3.23s\n",
      "Trial 69, Epoch 71/221, Val RMSE: 0.6330, Time: 3.30s\n",
      "Trial 69, Epoch 72/221, Val RMSE: 0.6371, Time: 3.24s\n",
      "Trial 69, Epoch 73/221, Val RMSE: 0.6354, Time: 3.36s\n",
      "Trial 69, Epoch 74/221, Val RMSE: 0.6327, Time: 3.24s\n",
      "Trial 69, Epoch 75/221, Val RMSE: 0.6351, Time: 3.25s\n",
      "Trial 69, Epoch 76/221, Val RMSE: 0.6333, Time: 3.29s\n",
      "Trial 69, Epoch 77/221, Val RMSE: 0.6370, Time: 3.24s\n",
      "Trial 69, Epoch 78/221, Val RMSE: 0.6376, Time: 3.28s\n",
      "Trial 69, Epoch 79/221, Val RMSE: 0.6364, Time: 3.27s\n",
      "Trial 69, Epoch 80/221, Val RMSE: 0.6414, Time: 3.28s\n",
      "Trial 69, Epoch 81/221, Val RMSE: 0.6321, Time: 3.29s\n",
      "Trial 69, Epoch 82/221, Val RMSE: 0.6346, Time: 3.28s\n",
      "Trial 69, Epoch 83/221, Val RMSE: 0.6411, Time: 3.31s\n",
      "Trial 69, Epoch 84/221, Val RMSE: 0.6314, Time: 3.28s\n",
      "Trial 69, Epoch 85/221, Val RMSE: 0.6337, Time: 3.28s\n",
      "Trial 69, Epoch 86/221, Val RMSE: 0.6352, Time: 3.28s\n",
      "Trial 69, Epoch 87/221, Val RMSE: 0.6357, Time: 3.29s\n",
      "[I 2025-07-17 11:55:46,765] Trial 69 pruned. \n",
      "Trial 70, Epoch 1/240, Val RMSE: 0.8023, Time: 3.27s\n",
      "Trial 70, Epoch 2/240, Val RMSE: 0.7707, Time: 3.37s\n",
      "Trial 70, Epoch 3/240, Val RMSE: 0.7420, Time: 3.31s\n",
      "[I 2025-07-17 11:55:56,977] Trial 70 pruned. \n",
      "Trial 71, Epoch 1/167, Val RMSE: 0.8429, Time: 3.47s\n",
      "Trial 71, Epoch 2/167, Val RMSE: 0.7562, Time: 3.55s\n",
      "Trial 71, Epoch 3/167, Val RMSE: 0.7148, Time: 3.36s\n",
      "Trial 71, Epoch 4/167, Val RMSE: 0.7381, Time: 3.32s\n",
      "Trial 71, Epoch 5/167, Val RMSE: 0.6901, Time: 3.50s\n",
      "Trial 71, Epoch 6/167, Val RMSE: 0.6883, Time: 3.46s\n",
      "Trial 71, Epoch 7/167, Val RMSE: 0.6623, Time: 3.56s\n",
      "Trial 71, Epoch 8/167, Val RMSE: 0.6776, Time: 3.44s\n",
      "Trial 71, Epoch 9/167, Val RMSE: 0.6478, Time: 3.45s\n",
      "Trial 71, Epoch 10/167, Val RMSE: 0.6420, Time: 3.38s\n",
      "Trial 71, Epoch 11/167, Val RMSE: 0.6439, Time: 3.33s\n",
      "Trial 71, Epoch 12/167, Val RMSE: 0.6421, Time: 3.27s\n",
      "Trial 71, Epoch 13/167, Val RMSE: 0.6391, Time: 3.30s\n",
      "Trial 71, Epoch 14/167, Val RMSE: 0.6215, Time: 3.32s\n",
      "Trial 71, Epoch 15/167, Val RMSE: 0.6399, Time: 3.30s\n",
      "Trial 71, Epoch 16/167, Val RMSE: 0.6388, Time: 3.34s\n",
      "Trial 71, Epoch 17/167, Val RMSE: 0.6233, Time: 3.36s\n",
      "Trial 71, Epoch 18/167, Val RMSE: 0.6417, Time: 3.38s\n",
      "Trial 71, Epoch 19/167, Val RMSE: 0.6248, Time: 3.33s\n",
      "Trial 71, Epoch 20/167, Val RMSE: 0.6299, Time: 3.30s\n",
      "Trial 71, Epoch 21/167, Val RMSE: 0.6219, Time: 3.29s\n",
      "Trial 71, Epoch 22/167, Val RMSE: 0.6212, Time: 3.33s\n",
      "Trial 71, Epoch 23/167, Val RMSE: 0.6216, Time: 3.28s\n",
      "Trial 71, Epoch 24/167, Val RMSE: 0.6346, Time: 3.29s\n",
      "Trial 71, Epoch 25/167, Val RMSE: 0.6249, Time: 3.25s\n",
      "Trial 71, Epoch 26/167, Val RMSE: 0.6342, Time: 3.31s\n",
      "Trial 71, Epoch 27/167, Val RMSE: 0.6233, Time: 3.32s\n",
      "Trial 71, Epoch 28/167, Val RMSE: 0.6295, Time: 3.30s\n",
      "Trial 71, Epoch 29/167, Val RMSE: 0.6172, Time: 3.34s\n",
      "Trial 71, Epoch 30/167, Val RMSE: 0.6336, Time: 3.37s\n",
      "Trial 71, Epoch 31/167, Val RMSE: 0.6276, Time: 3.35s\n",
      "Trial 71, Epoch 32/167, Val RMSE: 0.6222, Time: 3.32s\n",
      "Trial 71, Epoch 33/167, Val RMSE: 0.6298, Time: 3.36s\n",
      "Trial 71, Epoch 34/167, Val RMSE: 0.6222, Time: 3.36s\n",
      "Trial 71, Epoch 35/167, Val RMSE: 0.6198, Time: 3.35s\n",
      "Trial 71, Epoch 36/167, Val RMSE: 0.6193, Time: 3.37s\n",
      "Trial 71, Epoch 37/167, Val RMSE: 0.6402, Time: 3.36s\n",
      "Trial 71, Epoch 38/167, Val RMSE: 0.6171, Time: 3.32s\n",
      "Trial 71, Epoch 39/167, Val RMSE: 0.6371, Time: 3.30s\n",
      "Trial 71, Epoch 40/167, Val RMSE: 0.6138, Time: 3.31s\n",
      "Trial 71, Epoch 41/167, Val RMSE: 0.6297, Time: 3.33s\n",
      "Trial 71, Epoch 42/167, Val RMSE: 0.6207, Time: 3.33s\n",
      "Trial 71, Epoch 43/167, Val RMSE: 0.6221, Time: 3.27s\n",
      "Trial 71, Epoch 44/167, Val RMSE: 0.6169, Time: 3.32s\n",
      "Trial 71, Epoch 45/167, Val RMSE: 0.6098, Time: 3.29s\n",
      "Trial 71, Epoch 46/167, Val RMSE: 0.6191, Time: 3.32s\n",
      "Trial 71, Epoch 47/167, Val RMSE: 0.6141, Time: 3.28s\n",
      "Trial 71, Epoch 48/167, Val RMSE: 0.6243, Time: 3.28s\n",
      "Trial 71, Epoch 49/167, Val RMSE: 0.6163, Time: 3.26s\n",
      "Trial 71, Epoch 50/167, Val RMSE: 0.6205, Time: 3.27s\n",
      "Trial 71, Epoch 51/167, Val RMSE: 0.6271, Time: 3.24s\n",
      "Trial 71, Epoch 52/167, Val RMSE: 0.6154, Time: 3.33s\n",
      "Trial 71, Epoch 53/167, Val RMSE: 0.6184, Time: 3.32s\n",
      "Trial 71, Epoch 54/167, Val RMSE: 0.6240, Time: 3.34s\n",
      "Trial 71, Epoch 55/167, Val RMSE: 0.6146, Time: 3.32s\n",
      "Trial 71, Epoch 56/167, Val RMSE: 0.6189, Time: 3.31s\n",
      "Trial 71, Epoch 57/167, Val RMSE: 0.6185, Time: 3.34s\n",
      "Trial 71, Epoch 58/167, Val RMSE: 0.6232, Time: 3.31s\n",
      "Trial 71, Epoch 59/167, Val RMSE: 0.6272, Time: 3.31s\n",
      "Trial 71, Epoch 60/167, Val RMSE: 0.6197, Time: 3.31s\n",
      "Trial 71, Epoch 61/167, Val RMSE: 0.6118, Time: 3.33s\n",
      "Trial 71, Epoch 62/167, Val RMSE: 0.6186, Time: 3.29s\n",
      "Trial 71, Epoch 63/167, Val RMSE: 0.6102, Time: 3.32s\n",
      "Trial 71, Epoch 64/167, Val RMSE: 0.6202, Time: 3.31s\n",
      "Trial 71, Epoch 65/167, Val RMSE: 0.6131, Time: 3.29s\n",
      "Trial 71, Epoch 66/167, Val RMSE: 0.6162, Time: 3.32s\n",
      "Trial 71, Epoch 67/167, Val RMSE: 0.6192, Time: 3.31s\n",
      "Trial 71, Epoch 68/167, Val RMSE: 0.6129, Time: 3.35s\n",
      "Trial 71, Epoch 69/167, Val RMSE: 0.6208, Time: 3.28s\n",
      "Trial 71, Epoch 70/167, Val RMSE: 0.6174, Time: 3.38s\n",
      "Trial 71, Epoch 71/167, Val RMSE: 0.6166, Time: 3.34s\n",
      "Trial 71, Epoch 72/167, Val RMSE: 0.6135, Time: 3.31s\n",
      "Trial 71, Epoch 73/167, Val RMSE: 0.6190, Time: 3.37s\n",
      "Trial 71, Epoch 74/167, Val RMSE: 0.6286, Time: 3.28s\n",
      "Trial 71, Epoch 75/167, Val RMSE: 0.6121, Time: 3.33s\n",
      "Trial 71, Epoch 76/167, Val RMSE: 0.6111, Time: 3.31s\n",
      "Trial 71, Epoch 77/167, Val RMSE: 0.6152, Time: 3.30s\n",
      "Trial 71, Epoch 78/167, Val RMSE: 0.6156, Time: 3.28s\n",
      "Trial 71, Epoch 79/167, Val RMSE: 0.6143, Time: 3.29s\n",
      "Trial 71, Epoch 80/167, Val RMSE: 0.6103, Time: 3.32s\n",
      "Trial 71, Epoch 81/167, Val RMSE: 0.6144, Time: 3.33s\n",
      "Trial 71, Epoch 82/167, Val RMSE: 0.6135, Time: 3.30s\n",
      "Trial 71, Epoch 83/167, Val RMSE: 0.6157, Time: 3.28s\n",
      "Trial 71, Epoch 84/167, Val RMSE: 0.6165, Time: 3.30s\n",
      "Trial 71, Epoch 85/167, Val RMSE: 0.6165, Time: 3.30s\n",
      "Trial 71, Epoch 86/167, Val RMSE: 0.6087, Time: 3.28s\n",
      "Trial 71, Epoch 87/167, Val RMSE: 0.6171, Time: 3.29s\n",
      "Trial 71, Epoch 88/167, Val RMSE: 0.6164, Time: 3.32s\n",
      "Trial 71, Epoch 89/167, Val RMSE: 0.6159, Time: 3.29s\n",
      "Trial 71, Epoch 90/167, Val RMSE: 0.6169, Time: 3.31s\n",
      "Trial 71, Epoch 91/167, Val RMSE: 0.6175, Time: 3.30s\n",
      "Trial 71, Epoch 92/167, Val RMSE: 0.6193, Time: 3.29s\n",
      "Trial 71, Epoch 93/167, Val RMSE: 0.6147, Time: 3.32s\n",
      "Trial 71, Epoch 94/167, Val RMSE: 0.6126, Time: 3.30s\n",
      "Trial 71, Epoch 95/167, Val RMSE: 0.6106, Time: 3.29s\n",
      "Trial 71, Epoch 96/167, Val RMSE: 0.6205, Time: 3.26s\n",
      "Trial 71, Epoch 97/167, Val RMSE: 0.6081, Time: 3.35s\n",
      "Trial 71, Epoch 98/167, Val RMSE: 0.6151, Time: 3.28s\n",
      "Trial 71, Epoch 99/167, Val RMSE: 0.6221, Time: 3.31s\n",
      "Trial 71, Epoch 100/167, Val RMSE: 0.6238, Time: 3.46s\n",
      "Trial 71, Epoch 101/167, Val RMSE: 0.6182, Time: 3.34s\n",
      "Trial 71, Epoch 102/167, Val RMSE: 0.6152, Time: 3.32s\n",
      "Trial 71, Epoch 103/167, Val RMSE: 0.6160, Time: 3.31s\n",
      "Trial 71, Epoch 104/167, Val RMSE: 0.6169, Time: 3.32s\n",
      "Trial 71, Epoch 105/167, Val RMSE: 0.6129, Time: 3.31s\n",
      "Trial 71, Epoch 106/167, Val RMSE: 0.6164, Time: 3.32s\n",
      "Trial 71, Epoch 107/167, Val RMSE: 0.6119, Time: 3.48s\n",
      "Trial 71, Epoch 108/167, Val RMSE: 0.6195, Time: 3.31s\n",
      "Trial 71, Epoch 109/167, Val RMSE: 0.6136, Time: 3.35s\n",
      "Trial 71, Epoch 110/167, Val RMSE: 0.6141, Time: 3.32s\n",
      "Trial 71, Epoch 111/167, Val RMSE: 0.6140, Time: 3.29s\n",
      "Trial 71, Epoch 112/167, Val RMSE: 0.6169, Time: 3.31s\n",
      "Trial 71, Epoch 113/167, Val RMSE: 0.6135, Time: 3.34s\n",
      "Trial 71, Epoch 114/167, Val RMSE: 0.6197, Time: 3.33s\n",
      "Trial 71, Epoch 115/167, Val RMSE: 0.6127, Time: 3.38s\n",
      "Trial 71, Epoch 116/167, Val RMSE: 0.6147, Time: 3.31s\n",
      "Trial 71, Epoch 117/167, Val RMSE: 0.6167, Time: 3.34s\n",
      "Trial 71, Epoch 118/167, Val RMSE: 0.6110, Time: 3.31s\n",
      "Trial 71, Epoch 119/167, Val RMSE: 0.6109, Time: 3.31s\n",
      "Trial 71, Epoch 120/167, Val RMSE: 0.6080, Time: 3.28s\n",
      "Trial 71, Epoch 121/167, Val RMSE: 0.6135, Time: 3.29s\n",
      "Trial 71, Epoch 122/167, Val RMSE: 0.6190, Time: 3.30s\n",
      "Trial 71, Epoch 123/167, Val RMSE: 0.6161, Time: 3.31s\n",
      "Trial 71, Epoch 124/167, Val RMSE: 0.6136, Time: 3.25s\n",
      "Trial 71, Epoch 125/167, Val RMSE: 0.6187, Time: 3.33s\n",
      "Trial 71, Epoch 126/167, Val RMSE: 0.6141, Time: 3.28s\n",
      "Trial 71, Epoch 127/167, Val RMSE: 0.6149, Time: 3.32s\n",
      "Trial 71, Epoch 128/167, Val RMSE: 0.6132, Time: 3.32s\n",
      "Trial 71, Epoch 129/167, Val RMSE: 0.6154, Time: 3.32s\n",
      "Trial 71, Epoch 130/167, Val RMSE: 0.6141, Time: 3.22s\n",
      "Trial 71, Epoch 131/167, Val RMSE: 0.6117, Time: 3.30s\n",
      "Trial 71, Epoch 132/167, Val RMSE: 0.6098, Time: 3.31s\n",
      "Trial 71, Epoch 133/167, Val RMSE: 0.6123, Time: 3.34s\n",
      "Trial 71, Epoch 134/167, Val RMSE: 0.6132, Time: 3.30s\n",
      "Trial 71, Epoch 135/167, Val RMSE: 0.6179, Time: 3.30s\n",
      "Trial 71, Epoch 136/167, Val RMSE: 0.6126, Time: 3.34s\n",
      "Trial 71, Epoch 137/167, Val RMSE: 0.6191, Time: 3.35s\n",
      "Trial 71, Epoch 138/167, Val RMSE: 0.6162, Time: 3.32s\n",
      "Trial 71, Epoch 139/167, Val RMSE: 0.6089, Time: 3.30s\n",
      "Trial 71, Epoch 140/167, Val RMSE: 0.6120, Time: 3.29s\n",
      "Trial 71, Epoch 141/167, Val RMSE: 0.6134, Time: 3.29s\n",
      "Trial 71, Epoch 142/167, Val RMSE: 0.6159, Time: 3.33s\n",
      "Trial 71, Epoch 143/167, Val RMSE: 0.6113, Time: 3.31s\n",
      "Trial 71, Epoch 144/167, Val RMSE: 0.6113, Time: 3.32s\n",
      "Trial 71, Epoch 145/167, Val RMSE: 0.6078, Time: 3.29s\n",
      "Trial 71, Epoch 146/167, Val RMSE: 0.6140, Time: 3.34s\n",
      "Trial 71, Epoch 147/167, Val RMSE: 0.6151, Time: 3.34s\n",
      "Trial 71, Epoch 148/167, Val RMSE: 0.6136, Time: 3.32s\n",
      "Trial 71, Epoch 149/167, Val RMSE: 0.6155, Time: 3.27s\n",
      "Trial 71, Epoch 150/167, Val RMSE: 0.6207, Time: 3.32s\n",
      "Trial 71, Epoch 151/167, Val RMSE: 0.6222, Time: 3.35s\n",
      "Trial 71, Epoch 152/167, Val RMSE: 0.6126, Time: 3.38s\n",
      "Trial 71, Epoch 153/167, Val RMSE: 0.6153, Time: 3.31s\n",
      "Trial 71, Epoch 154/167, Val RMSE: 0.6195, Time: 3.30s\n",
      "Trial 71, Epoch 155/167, Val RMSE: 0.6109, Time: 3.29s\n",
      "Trial 71, Epoch 156/167, Val RMSE: 0.6141, Time: 3.32s\n",
      "Trial 71, Epoch 157/167, Val RMSE: 0.6141, Time: 3.28s\n",
      "Trial 71, Epoch 158/167, Val RMSE: 0.6161, Time: 3.26s\n",
      "Trial 71, Epoch 159/167, Val RMSE: 0.6099, Time: 3.27s\n",
      "Trial 71, Epoch 160/167, Val RMSE: 0.6119, Time: 3.33s\n",
      "Trial 71, Epoch 161/167, Val RMSE: 0.6074, Time: 3.29s\n",
      "Trial 71, Epoch 162/167, Val RMSE: 0.6177, Time: 3.29s\n",
      "Trial 71, Epoch 163/167, Val RMSE: 0.6139, Time: 3.29s\n",
      "Trial 71, Epoch 164/167, Val RMSE: 0.6151, Time: 3.25s\n",
      "Trial 71, Epoch 165/167, Val RMSE: 0.6138, Time: 3.29s\n",
      "Trial 71, Epoch 166/167, Val RMSE: 0.6123, Time: 3.37s\n",
      "Trial 71, Epoch 167/167, Val RMSE: 0.6160, Time: 3.27s\n",
      "[I 2025-07-17 12:05:16,566] Trial 71 finished with value: 0.6160146491651134 and parameters: {'hidden_channels': 1023, 'lr': 0.0002372101834409312, 'batch_size': 128, 'n_epochs': 167, 'num_layers': 1, 'dropout_rate': 0.2064030143001203, 'weight_decay': 2.0985378200479682e-07}. Best is trial 60 with value: 0.609277393700007.\n",
      "Trial 72, Epoch 1/161, Val RMSE: 0.8958, Time: 4.79s\n",
      "Trial 72, Epoch 2/161, Val RMSE: 0.8387, Time: 4.77s\n",
      "[I 2025-07-17 12:05:26,398] Trial 72 pruned. \n",
      "Trial 73, Epoch 1/173, Val RMSE: 0.8936, Time: 7.98s\n",
      "Trial 73, Epoch 2/173, Val RMSE: 0.7737, Time: 7.86s\n",
      "Trial 73, Epoch 3/173, Val RMSE: 0.7373, Time: 7.87s\n",
      "[I 2025-07-17 12:05:50,486] Trial 73 pruned. \n",
      "Trial 74, Epoch 1/279, Val RMSE: 0.7941, Time: 5.28s\n",
      "Trial 74, Epoch 2/279, Val RMSE: 0.7355, Time: 5.25s\n",
      "Trial 74, Epoch 3/279, Val RMSE: 0.7333, Time: 5.27s\n",
      "Trial 74, Epoch 4/279, Val RMSE: 0.7374, Time: 5.23s\n",
      "[I 2025-07-17 12:06:11,857] Trial 74 pruned. \n",
      "Trial 75, Epoch 1/238, Val RMSE: 0.9018, Time: 4.44s\n",
      "Trial 75, Epoch 2/238, Val RMSE: 0.7683, Time: 4.43s\n",
      "Trial 75, Epoch 3/238, Val RMSE: 0.7479, Time: 4.42s\n",
      "[I 2025-07-17 12:06:25,437] Trial 75 pruned. \n",
      "Trial 76, Epoch 1/184, Val RMSE: 0.7933, Time: 8.52s\n",
      "Trial 76, Epoch 2/184, Val RMSE: 0.7590, Time: 8.23s\n",
      "Trial 76, Epoch 3/184, Val RMSE: 0.7177, Time: 8.17s\n",
      "Trial 76, Epoch 4/184, Val RMSE: 0.7224, Time: 8.62s\n",
      "Trial 76, Epoch 5/184, Val RMSE: 0.7012, Time: 8.34s\n",
      "Trial 76, Epoch 6/184, Val RMSE: 0.7401, Time: 8.46s\n",
      "[I 2025-07-17 12:07:16,112] Trial 76 pruned. \n",
      "Trial 77, Epoch 1/207, Val RMSE: 0.7865, Time: 3.23s\n",
      "Trial 77, Epoch 2/207, Val RMSE: 0.7470, Time: 3.31s\n",
      "Trial 77, Epoch 3/207, Val RMSE: 0.7261, Time: 3.31s\n",
      "Trial 77, Epoch 4/207, Val RMSE: 0.7001, Time: 3.33s\n",
      "Trial 77, Epoch 5/207, Val RMSE: 0.6958, Time: 3.30s\n",
      "Trial 77, Epoch 6/207, Val RMSE: 0.6987, Time: 3.34s\n",
      "Trial 77, Epoch 7/207, Val RMSE: 0.6839, Time: 3.31s\n",
      "[I 2025-07-17 12:07:39,600] Trial 77 pruned. \n",
      "Trial 78, Epoch 1/255, Val RMSE: 0.8896, Time: 2.50s\n",
      "Trial 78, Epoch 2/255, Val RMSE: 0.7454, Time: 2.51s\n",
      "Trial 78, Epoch 3/255, Val RMSE: 0.7190, Time: 2.53s\n",
      "Trial 78, Epoch 4/255, Val RMSE: 0.7014, Time: 2.50s\n",
      "Trial 78, Epoch 5/255, Val RMSE: 0.6879, Time: 2.50s\n",
      "Trial 78, Epoch 6/255, Val RMSE: 0.6738, Time: 2.51s\n",
      "Trial 78, Epoch 7/255, Val RMSE: 0.6605, Time: 2.50s\n",
      "Trial 78, Epoch 8/255, Val RMSE: 0.6653, Time: 2.55s\n",
      "Trial 78, Epoch 9/255, Val RMSE: 0.6541, Time: 2.55s\n",
      "Trial 78, Epoch 10/255, Val RMSE: 0.6525, Time: 2.51s\n",
      "Trial 78, Epoch 11/255, Val RMSE: 0.6496, Time: 2.52s\n",
      "Trial 78, Epoch 12/255, Val RMSE: 0.6618, Time: 2.51s\n",
      "Trial 78, Epoch 13/255, Val RMSE: 0.6589, Time: 2.55s\n",
      "Trial 78, Epoch 14/255, Val RMSE: 0.6525, Time: 2.52s\n",
      "Trial 78, Epoch 15/255, Val RMSE: 0.6361, Time: 2.43s\n",
      "Trial 78, Epoch 16/255, Val RMSE: 0.6399, Time: 2.47s\n",
      "Trial 78, Epoch 17/255, Val RMSE: 0.6453, Time: 2.52s\n",
      "Trial 78, Epoch 18/255, Val RMSE: 0.6458, Time: 2.56s\n",
      "Trial 78, Epoch 19/255, Val RMSE: 0.6374, Time: 2.50s\n",
      "Trial 78, Epoch 20/255, Val RMSE: 0.6384, Time: 2.51s\n",
      "Trial 78, Epoch 21/255, Val RMSE: 0.6469, Time: 2.51s\n",
      "Trial 78, Epoch 22/255, Val RMSE: 0.6364, Time: 2.49s\n",
      "Trial 78, Epoch 23/255, Val RMSE: 0.6341, Time: 2.49s\n",
      "Trial 78, Epoch 24/255, Val RMSE: 0.6321, Time: 2.48s\n",
      "Trial 78, Epoch 25/255, Val RMSE: 0.6423, Time: 2.49s\n",
      "Trial 78, Epoch 26/255, Val RMSE: 0.6364, Time: 2.50s\n",
      "Trial 78, Epoch 27/255, Val RMSE: 0.6430, Time: 2.52s\n",
      "Trial 78, Epoch 28/255, Val RMSE: 0.6443, Time: 2.49s\n",
      "Trial 78, Epoch 29/255, Val RMSE: 0.6479, Time: 2.57s\n",
      "Trial 78, Epoch 30/255, Val RMSE: 0.6462, Time: 2.47s\n",
      "Trial 78, Epoch 31/255, Val RMSE: 0.6421, Time: 2.51s\n",
      "Trial 78, Epoch 32/255, Val RMSE: 0.6396, Time: 2.49s\n",
      "Trial 78, Epoch 33/255, Val RMSE: 0.6429, Time: 2.51s\n",
      "Trial 78, Epoch 34/255, Val RMSE: 0.6409, Time: 2.55s\n",
      "Trial 78, Epoch 35/255, Val RMSE: 0.6333, Time: 2.49s\n",
      "Trial 78, Epoch 36/255, Val RMSE: 0.6457, Time: 2.54s\n",
      "Trial 78, Epoch 37/255, Val RMSE: 0.6504, Time: 2.51s\n",
      "Trial 78, Epoch 38/255, Val RMSE: 0.6395, Time: 2.52s\n",
      "Trial 78, Epoch 39/255, Val RMSE: 0.6478, Time: 2.55s\n",
      "Trial 78, Epoch 40/255, Val RMSE: 0.6346, Time: 2.50s\n",
      "Trial 78, Epoch 41/255, Val RMSE: 0.6389, Time: 2.50s\n",
      "Trial 78, Epoch 42/255, Val RMSE: 0.6636, Time: 2.49s\n",
      "Trial 78, Epoch 43/255, Val RMSE: 0.6391, Time: 2.54s\n",
      "Trial 78, Epoch 44/255, Val RMSE: 0.6434, Time: 2.58s\n",
      "Trial 78, Epoch 45/255, Val RMSE: 0.6496, Time: 2.51s\n",
      "Trial 78, Epoch 46/255, Val RMSE: 0.6429, Time: 2.55s\n",
      "Trial 78, Epoch 47/255, Val RMSE: 0.6466, Time: 2.51s\n",
      "Trial 78, Epoch 48/255, Val RMSE: 0.6375, Time: 2.52s\n",
      "Trial 78, Epoch 49/255, Val RMSE: 0.6474, Time: 2.52s\n",
      "Trial 78, Epoch 50/255, Val RMSE: 0.6477, Time: 2.49s\n",
      "Trial 78, Epoch 51/255, Val RMSE: 0.6406, Time: 2.54s\n",
      "Trial 78, Epoch 52/255, Val RMSE: 0.6418, Time: 2.52s\n",
      "Trial 78, Epoch 53/255, Val RMSE: 0.6426, Time: 2.53s\n",
      "Trial 78, Epoch 54/255, Val RMSE: 0.6451, Time: 2.49s\n",
      "Trial 78, Epoch 55/255, Val RMSE: 0.6425, Time: 2.52s\n",
      "Trial 78, Epoch 56/255, Val RMSE: 0.6425, Time: 2.56s\n",
      "Trial 78, Epoch 57/255, Val RMSE: 0.6398, Time: 2.56s\n",
      "Trial 78, Epoch 58/255, Val RMSE: 0.6381, Time: 2.52s\n",
      "Trial 78, Epoch 59/255, Val RMSE: 0.6429, Time: 2.50s\n",
      "Trial 78, Epoch 60/255, Val RMSE: 0.6382, Time: 2.53s\n",
      "Trial 78, Epoch 61/255, Val RMSE: 0.6386, Time: 2.55s\n",
      "Trial 78, Epoch 62/255, Val RMSE: 0.6468, Time: 2.55s\n",
      "Trial 78, Epoch 63/255, Val RMSE: 0.6402, Time: 2.48s\n",
      "Trial 78, Epoch 64/255, Val RMSE: 0.6429, Time: 2.50s\n",
      "Trial 78, Epoch 65/255, Val RMSE: 0.6461, Time: 2.49s\n",
      "Trial 78, Epoch 66/255, Val RMSE: 0.6398, Time: 2.52s\n",
      "Trial 78, Epoch 67/255, Val RMSE: 0.6377, Time: 2.58s\n",
      "Trial 78, Epoch 68/255, Val RMSE: 0.6475, Time: 2.52s\n",
      "Trial 78, Epoch 69/255, Val RMSE: 0.6402, Time: 2.44s\n",
      "Trial 78, Epoch 70/255, Val RMSE: 0.6317, Time: 2.49s\n",
      "Trial 78, Epoch 71/255, Val RMSE: 0.6409, Time: 2.52s\n",
      "Trial 78, Epoch 72/255, Val RMSE: 0.6349, Time: 2.64s\n",
      "Trial 78, Epoch 73/255, Val RMSE: 0.6413, Time: 2.55s\n",
      "Trial 78, Epoch 74/255, Val RMSE: 0.6401, Time: 2.54s\n",
      "Trial 78, Epoch 75/255, Val RMSE: 0.6435, Time: 2.53s\n",
      "Trial 78, Epoch 76/255, Val RMSE: 0.6417, Time: 2.54s\n",
      "Trial 78, Epoch 77/255, Val RMSE: 0.6450, Time: 2.55s\n",
      "Trial 78, Epoch 78/255, Val RMSE: 0.6440, Time: 2.55s\n",
      "Trial 78, Epoch 79/255, Val RMSE: 0.6466, Time: 2.58s\n",
      "Trial 78, Epoch 80/255, Val RMSE: 0.6377, Time: 2.62s\n",
      "Trial 78, Epoch 81/255, Val RMSE: 0.6412, Time: 2.61s\n",
      "Trial 78, Epoch 82/255, Val RMSE: 0.6369, Time: 2.58s\n",
      "Trial 78, Epoch 83/255, Val RMSE: 0.6414, Time: 2.48s\n",
      "[I 2025-07-17 12:11:10,818] Trial 78 pruned. \n",
      "Trial 79, Epoch 1/320, Val RMSE: 0.8117, Time: 3.36s\n",
      "Trial 79, Epoch 2/320, Val RMSE: 0.7650, Time: 3.29s\n",
      "Trial 79, Epoch 3/320, Val RMSE: 0.7505, Time: 3.27s\n",
      "[I 2025-07-17 12:11:21,011] Trial 79 pruned. \n",
      "Trial 80, Epoch 1/231, Val RMSE: 0.8036, Time: 3.32s\n",
      "Trial 80, Epoch 2/231, Val RMSE: 0.7530, Time: 3.34s\n",
      "Trial 80, Epoch 3/231, Val RMSE: 0.7514, Time: 3.30s\n",
      "[I 2025-07-17 12:11:31,222] Trial 80 pruned. \n",
      "Trial 81, Epoch 1/174, Val RMSE: 0.8608, Time: 3.43s\n",
      "Trial 81, Epoch 2/174, Val RMSE: 0.7871, Time: 3.34s\n",
      "[I 2025-07-17 12:11:38,244] Trial 81 pruned. \n",
      "Trial 82, Epoch 1/209, Val RMSE: 0.8178, Time: 8.53s\n",
      "Trial 82, Epoch 2/209, Val RMSE: 0.7454, Time: 8.46s\n",
      "Trial 82, Epoch 3/209, Val RMSE: 0.7232, Time: 8.71s\n",
      "Trial 82, Epoch 4/209, Val RMSE: 0.6752, Time: 8.47s\n",
      "Trial 82, Epoch 5/209, Val RMSE: 0.6704, Time: 8.48s\n",
      "Trial 82, Epoch 6/209, Val RMSE: 0.6644, Time: 8.44s\n",
      "Trial 82, Epoch 7/209, Val RMSE: 0.6505, Time: 8.45s\n",
      "Trial 82, Epoch 8/209, Val RMSE: 0.6403, Time: 8.40s\n",
      "Trial 82, Epoch 9/209, Val RMSE: 0.6437, Time: 8.54s\n",
      "Trial 82, Epoch 10/209, Val RMSE: 0.6426, Time: 8.49s\n",
      "Trial 82, Epoch 11/209, Val RMSE: 0.6489, Time: 8.39s\n",
      "Trial 82, Epoch 12/209, Val RMSE: 0.6363, Time: 8.43s\n",
      "Trial 82, Epoch 13/209, Val RMSE: 0.6362, Time: 8.44s\n",
      "Trial 82, Epoch 14/209, Val RMSE: 0.6396, Time: 8.47s\n",
      "Trial 82, Epoch 15/209, Val RMSE: 0.6298, Time: 8.48s\n",
      "Trial 82, Epoch 16/209, Val RMSE: 0.6513, Time: 8.39s\n",
      "Trial 82, Epoch 17/209, Val RMSE: 0.6300, Time: 8.40s\n",
      "Trial 82, Epoch 18/209, Val RMSE: 0.6301, Time: 8.35s\n",
      "Trial 82, Epoch 19/209, Val RMSE: 0.6313, Time: 8.48s\n",
      "Trial 82, Epoch 20/209, Val RMSE: 0.6323, Time: 8.45s\n",
      "Trial 82, Epoch 21/209, Val RMSE: 0.6361, Time: 8.31s\n",
      "Trial 82, Epoch 22/209, Val RMSE: 0.6366, Time: 8.34s\n",
      "Trial 82, Epoch 23/209, Val RMSE: 0.6387, Time: 8.34s\n",
      "Trial 82, Epoch 24/209, Val RMSE: 0.6433, Time: 8.38s\n",
      "Trial 82, Epoch 25/209, Val RMSE: 0.6279, Time: 8.35s\n",
      "Trial 82, Epoch 26/209, Val RMSE: 0.6373, Time: 8.43s\n",
      "Trial 82, Epoch 27/209, Val RMSE: 0.6295, Time: 8.67s\n",
      "Trial 82, Epoch 28/209, Val RMSE: 0.6287, Time: 8.52s\n",
      "Trial 82, Epoch 29/209, Val RMSE: 0.6299, Time: 8.45s\n",
      "Trial 82, Epoch 30/209, Val RMSE: 0.6266, Time: 8.48s\n",
      "Trial 82, Epoch 31/209, Val RMSE: 0.6282, Time: 8.45s\n",
      "Trial 82, Epoch 32/209, Val RMSE: 0.6393, Time: 8.41s\n",
      "Trial 82, Epoch 33/209, Val RMSE: 0.6340, Time: 8.44s\n",
      "Trial 82, Epoch 34/209, Val RMSE: 0.6325, Time: 8.48s\n",
      "Trial 82, Epoch 35/209, Val RMSE: 0.6384, Time: 8.40s\n",
      "Trial 82, Epoch 36/209, Val RMSE: 0.6346, Time: 8.50s\n",
      "Trial 82, Epoch 37/209, Val RMSE: 0.6286, Time: 8.46s\n",
      "Trial 82, Epoch 38/209, Val RMSE: 0.6361, Time: 8.71s\n",
      "Trial 82, Epoch 39/209, Val RMSE: 0.6354, Time: 8.54s\n",
      "Trial 82, Epoch 40/209, Val RMSE: 0.6360, Time: 8.40s\n",
      "Trial 82, Epoch 41/209, Val RMSE: 0.6337, Time: 8.51s\n",
      "Trial 82, Epoch 42/209, Val RMSE: 0.6281, Time: 8.54s\n",
      "Trial 82, Epoch 43/209, Val RMSE: 0.6261, Time: 8.94s\n",
      "Trial 82, Epoch 44/209, Val RMSE: 0.6366, Time: 9.00s\n",
      "Trial 82, Epoch 45/209, Val RMSE: 0.6384, Time: 8.57s\n",
      "Trial 82, Epoch 46/209, Val RMSE: 0.6345, Time: 8.48s\n",
      "Trial 82, Epoch 47/209, Val RMSE: 0.6442, Time: 8.39s\n",
      "Trial 82, Epoch 48/209, Val RMSE: 0.6350, Time: 8.48s\n",
      "Trial 82, Epoch 49/209, Val RMSE: 0.6378, Time: 8.40s\n",
      "Trial 82, Epoch 50/209, Val RMSE: 0.6327, Time: 8.35s\n",
      "Trial 82, Epoch 51/209, Val RMSE: 0.6386, Time: 8.33s\n",
      "Trial 82, Epoch 52/209, Val RMSE: 0.6323, Time: 8.36s\n",
      "Trial 82, Epoch 53/209, Val RMSE: 0.6397, Time: 8.43s\n",
      "Trial 82, Epoch 54/209, Val RMSE: 0.6328, Time: 8.33s\n",
      "Trial 82, Epoch 55/209, Val RMSE: 0.6392, Time: 8.34s\n",
      "Trial 82, Epoch 56/209, Val RMSE: 0.6343, Time: 8.49s\n",
      "Trial 82, Epoch 57/209, Val RMSE: 0.6362, Time: 8.35s\n",
      "Trial 82, Epoch 58/209, Val RMSE: 0.6412, Time: 8.69s\n",
      "Trial 82, Epoch 59/209, Val RMSE: 0.6359, Time: 8.96s\n",
      "Trial 82, Epoch 60/209, Val RMSE: 0.6372, Time: 8.51s\n",
      "Trial 82, Epoch 61/209, Val RMSE: 0.6381, Time: 8.57s\n",
      "Trial 82, Epoch 62/209, Val RMSE: 0.6354, Time: 8.34s\n",
      "Trial 82, Epoch 63/209, Val RMSE: 0.6368, Time: 8.30s\n",
      "Trial 82, Epoch 64/209, Val RMSE: 0.6276, Time: 8.36s\n",
      "Trial 82, Epoch 65/209, Val RMSE: 0.6332, Time: 8.35s\n",
      "Trial 82, Epoch 66/209, Val RMSE: 0.6353, Time: 8.41s\n",
      "Trial 82, Epoch 67/209, Val RMSE: 0.6347, Time: 8.77s\n",
      "Trial 82, Epoch 68/209, Val RMSE: 0.6322, Time: 8.33s\n",
      "Trial 82, Epoch 69/209, Val RMSE: 0.6367, Time: 8.38s\n",
      "Trial 82, Epoch 70/209, Val RMSE: 0.6380, Time: 8.41s\n",
      "Trial 82, Epoch 71/209, Val RMSE: 0.6327, Time: 8.70s\n",
      "Trial 82, Epoch 72/209, Val RMSE: 0.6262, Time: 8.51s\n",
      "Trial 82, Epoch 73/209, Val RMSE: 0.6348, Time: 8.75s\n",
      "Trial 82, Epoch 74/209, Val RMSE: 0.6364, Time: 8.39s\n",
      "Trial 82, Epoch 75/209, Val RMSE: 0.6312, Time: 8.48s\n",
      "Trial 82, Epoch 76/209, Val RMSE: 0.6395, Time: 8.38s\n",
      "Trial 82, Epoch 77/209, Val RMSE: 0.6365, Time: 8.41s\n",
      "Trial 82, Epoch 78/209, Val RMSE: 0.6340, Time: 8.24s\n",
      "Trial 82, Epoch 79/209, Val RMSE: 0.6379, Time: 8.22s\n",
      "Trial 82, Epoch 80/209, Val RMSE: 0.6254, Time: 8.33s\n",
      "Trial 82, Epoch 81/209, Val RMSE: 0.6288, Time: 8.28s\n",
      "Trial 82, Epoch 82/209, Val RMSE: 0.6274, Time: 8.44s\n",
      "Trial 82, Epoch 83/209, Val RMSE: 0.6377, Time: 8.35s\n",
      "Trial 82, Epoch 84/209, Val RMSE: 0.6397, Time: 8.38s\n",
      "Trial 82, Epoch 85/209, Val RMSE: 0.6319, Time: 8.42s\n",
      "Trial 82, Epoch 86/209, Val RMSE: 0.6267, Time: 8.43s\n",
      "Trial 82, Epoch 87/209, Val RMSE: 0.6446, Time: 8.48s\n",
      "Trial 82, Epoch 88/209, Val RMSE: 0.6329, Time: 8.37s\n",
      "Trial 82, Epoch 89/209, Val RMSE: 0.6312, Time: 8.33s\n",
      "Trial 82, Epoch 90/209, Val RMSE: 0.6363, Time: 8.46s\n",
      "Trial 82, Epoch 91/209, Val RMSE: 0.6271, Time: 8.45s\n",
      "[I 2025-07-17 12:24:30,081] Trial 82 pruned. \n",
      "Trial 83, Epoch 1/152, Val RMSE: 0.8104, Time: 3.27s\n",
      "Trial 83, Epoch 2/152, Val RMSE: 0.7334, Time: 3.29s\n",
      "Trial 83, Epoch 3/152, Val RMSE: 0.7130, Time: 3.28s\n",
      "Trial 83, Epoch 4/152, Val RMSE: 0.6957, Time: 3.21s\n",
      "Trial 83, Epoch 5/152, Val RMSE: 0.6722, Time: 3.29s\n",
      "Trial 83, Epoch 6/152, Val RMSE: 0.6724, Time: 3.28s\n",
      "Trial 83, Epoch 7/152, Val RMSE: 0.6759, Time: 3.26s\n",
      "Trial 83, Epoch 8/152, Val RMSE: 0.6753, Time: 3.30s\n",
      "Trial 83, Epoch 9/152, Val RMSE: 0.6475, Time: 3.25s\n",
      "Trial 83, Epoch 10/152, Val RMSE: 0.6574, Time: 3.29s\n",
      "Trial 83, Epoch 11/152, Val RMSE: 0.6503, Time: 3.29s\n",
      "Trial 83, Epoch 12/152, Val RMSE: 0.6527, Time: 3.26s\n",
      "Trial 83, Epoch 13/152, Val RMSE: 0.6624, Time: 3.29s\n",
      "Trial 83, Epoch 14/152, Val RMSE: 0.6473, Time: 3.23s\n",
      "Trial 83, Epoch 15/152, Val RMSE: 0.6443, Time: 3.28s\n",
      "Trial 83, Epoch 16/152, Val RMSE: 0.6472, Time: 3.23s\n",
      "Trial 83, Epoch 17/152, Val RMSE: 0.6486, Time: 3.31s\n",
      "Trial 83, Epoch 18/152, Val RMSE: 0.6436, Time: 3.27s\n",
      "Trial 83, Epoch 19/152, Val RMSE: 0.6434, Time: 3.32s\n",
      "Trial 83, Epoch 20/152, Val RMSE: 0.6471, Time: 3.46s\n",
      "Trial 83, Epoch 21/152, Val RMSE: 0.6473, Time: 3.39s\n",
      "Trial 83, Epoch 22/152, Val RMSE: 0.6412, Time: 3.41s\n",
      "Trial 83, Epoch 23/152, Val RMSE: 0.6364, Time: 3.34s\n",
      "Trial 83, Epoch 24/152, Val RMSE: 0.6412, Time: 3.38s\n",
      "Trial 83, Epoch 25/152, Val RMSE: 0.6394, Time: 3.31s\n",
      "Trial 83, Epoch 26/152, Val RMSE: 0.6382, Time: 3.29s\n",
      "Trial 83, Epoch 27/152, Val RMSE: 0.6371, Time: 3.31s\n",
      "Trial 83, Epoch 28/152, Val RMSE: 0.6391, Time: 3.32s\n",
      "Trial 83, Epoch 29/152, Val RMSE: 0.6387, Time: 3.32s\n",
      "Trial 83, Epoch 30/152, Val RMSE: 0.6427, Time: 3.33s\n",
      "Trial 83, Epoch 31/152, Val RMSE: 0.6483, Time: 3.29s\n",
      "Trial 83, Epoch 32/152, Val RMSE: 0.6349, Time: 3.27s\n",
      "Trial 83, Epoch 33/152, Val RMSE: 0.6364, Time: 3.34s\n",
      "Trial 83, Epoch 34/152, Val RMSE: 0.6402, Time: 3.33s\n",
      "Trial 83, Epoch 35/152, Val RMSE: 0.6379, Time: 3.29s\n",
      "Trial 83, Epoch 36/152, Val RMSE: 0.6436, Time: 3.33s\n",
      "Trial 83, Epoch 37/152, Val RMSE: 0.6432, Time: 3.27s\n",
      "Trial 83, Epoch 38/152, Val RMSE: 0.6370, Time: 3.21s\n",
      "Trial 83, Epoch 39/152, Val RMSE: 0.6406, Time: 3.29s\n",
      "Trial 83, Epoch 40/152, Val RMSE: 0.6403, Time: 3.35s\n",
      "Trial 83, Epoch 41/152, Val RMSE: 0.6400, Time: 3.28s\n",
      "Trial 83, Epoch 42/152, Val RMSE: 0.6408, Time: 3.28s\n",
      "Trial 83, Epoch 43/152, Val RMSE: 0.6368, Time: 3.29s\n",
      "Trial 83, Epoch 44/152, Val RMSE: 0.6353, Time: 3.59s\n",
      "Trial 83, Epoch 45/152, Val RMSE: 0.6350, Time: 3.49s\n",
      "Trial 83, Epoch 46/152, Val RMSE: 0.6389, Time: 3.38s\n",
      "Trial 83, Epoch 47/152, Val RMSE: 0.6474, Time: 3.32s\n",
      "Trial 83, Epoch 48/152, Val RMSE: 0.6405, Time: 3.21s\n",
      "Trial 83, Epoch 49/152, Val RMSE: 0.6405, Time: 3.30s\n",
      "Trial 83, Epoch 50/152, Val RMSE: 0.6416, Time: 3.26s\n",
      "Trial 83, Epoch 51/152, Val RMSE: 0.6347, Time: 3.27s\n",
      "Trial 83, Epoch 52/152, Val RMSE: 0.6315, Time: 3.26s\n",
      "Trial 83, Epoch 53/152, Val RMSE: 0.6342, Time: 3.27s\n",
      "Trial 83, Epoch 54/152, Val RMSE: 0.6350, Time: 3.20s\n",
      "Trial 83, Epoch 55/152, Val RMSE: 0.6383, Time: 3.30s\n",
      "Trial 83, Epoch 56/152, Val RMSE: 0.6407, Time: 3.29s\n",
      "Trial 83, Epoch 57/152, Val RMSE: 0.6373, Time: 3.22s\n",
      "Trial 83, Epoch 58/152, Val RMSE: 0.6405, Time: 3.33s\n",
      "Trial 83, Epoch 59/152, Val RMSE: 0.6438, Time: 3.27s\n",
      "Trial 83, Epoch 60/152, Val RMSE: 0.6529, Time: 3.25s\n",
      "Trial 83, Epoch 61/152, Val RMSE: 0.6414, Time: 3.31s\n",
      "Trial 83, Epoch 62/152, Val RMSE: 0.6439, Time: 3.32s\n",
      "Trial 83, Epoch 63/152, Val RMSE: 0.6421, Time: 3.28s\n",
      "Trial 83, Epoch 64/152, Val RMSE: 0.6420, Time: 3.32s\n",
      "Trial 83, Epoch 65/152, Val RMSE: 0.6403, Time: 3.27s\n",
      "Trial 83, Epoch 66/152, Val RMSE: 0.6330, Time: 3.25s\n",
      "Trial 83, Epoch 67/152, Val RMSE: 0.6394, Time: 3.29s\n",
      "Trial 83, Epoch 68/152, Val RMSE: 0.6435, Time: 3.30s\n",
      "Trial 83, Epoch 69/152, Val RMSE: 0.6433, Time: 3.33s\n",
      "Trial 83, Epoch 70/152, Val RMSE: 0.6399, Time: 3.28s\n",
      "Trial 83, Epoch 71/152, Val RMSE: 0.6349, Time: 3.31s\n",
      "Trial 83, Epoch 72/152, Val RMSE: 0.6330, Time: 3.25s\n",
      "Trial 83, Epoch 73/152, Val RMSE: 0.6362, Time: 3.25s\n",
      "Trial 83, Epoch 74/152, Val RMSE: 0.6415, Time: 3.27s\n",
      "Trial 83, Epoch 75/152, Val RMSE: 0.6423, Time: 3.25s\n",
      "Trial 83, Epoch 76/152, Val RMSE: 0.6336, Time: 3.25s\n",
      "Trial 83, Epoch 77/152, Val RMSE: 0.6348, Time: 3.27s\n",
      "Trial 83, Epoch 78/152, Val RMSE: 0.6378, Time: 3.27s\n",
      "Trial 83, Epoch 79/152, Val RMSE: 0.6350, Time: 3.33s\n",
      "Trial 83, Epoch 80/152, Val RMSE: 0.6393, Time: 3.25s\n",
      "Trial 83, Epoch 81/152, Val RMSE: 0.6325, Time: 3.25s\n",
      "Trial 83, Epoch 82/152, Val RMSE: 0.6358, Time: 3.29s\n",
      "Trial 83, Epoch 83/152, Val RMSE: 0.6352, Time: 3.26s\n",
      "[I 2025-07-17 12:29:05,589] Trial 83 pruned. \n",
      "Trial 84, Epoch 1/498, Val RMSE: 0.8409, Time: 4.38s\n",
      "Trial 84, Epoch 2/498, Val RMSE: 0.8720, Time: 4.42s\n",
      "[I 2025-07-17 12:29:14,657] Trial 84 pruned. \n",
      "Trial 85, Epoch 1/259, Val RMSE: 0.9391, Time: 2.53s\n",
      "Trial 85, Epoch 2/259, Val RMSE: 0.7514, Time: 2.46s\n",
      "Trial 85, Epoch 3/259, Val RMSE: 0.7372, Time: 2.53s\n",
      "[I 2025-07-17 12:29:22,426] Trial 85 pruned. \n",
      "Trial 86, Epoch 1/375, Val RMSE: 0.7713, Time: 3.27s\n",
      "Trial 86, Epoch 2/375, Val RMSE: 0.7333, Time: 3.30s\n",
      "Trial 86, Epoch 3/375, Val RMSE: 0.7308, Time: 3.27s\n",
      "Trial 86, Epoch 4/375, Val RMSE: 0.7095, Time: 3.30s\n",
      "Trial 86, Epoch 5/375, Val RMSE: 0.6831, Time: 3.31s\n",
      "Trial 86, Epoch 6/375, Val RMSE: 0.6690, Time: 3.33s\n",
      "Trial 86, Epoch 7/375, Val RMSE: 0.6606, Time: 3.33s\n",
      "Trial 86, Epoch 8/375, Val RMSE: 0.6561, Time: 3.27s\n",
      "Trial 86, Epoch 9/375, Val RMSE: 0.6511, Time: 3.30s\n",
      "Trial 86, Epoch 10/375, Val RMSE: 0.6737, Time: 3.31s\n",
      "Trial 86, Epoch 11/375, Val RMSE: 0.6559, Time: 3.32s\n",
      "Trial 86, Epoch 12/375, Val RMSE: 0.6748, Time: 3.32s\n",
      "Trial 86, Epoch 13/375, Val RMSE: 0.6522, Time: 3.30s\n",
      "Trial 86, Epoch 14/375, Val RMSE: 0.6569, Time: 3.33s\n",
      "Trial 86, Epoch 15/375, Val RMSE: 0.6486, Time: 3.28s\n",
      "Trial 86, Epoch 16/375, Val RMSE: 0.6521, Time: 3.32s\n",
      "Trial 86, Epoch 17/375, Val RMSE: 0.6631, Time: 3.33s\n",
      "Trial 86, Epoch 18/375, Val RMSE: 0.6448, Time: 3.29s\n",
      "Trial 86, Epoch 19/375, Val RMSE: 0.6466, Time: 3.36s\n",
      "Trial 86, Epoch 20/375, Val RMSE: 0.6392, Time: 3.42s\n",
      "Trial 86, Epoch 21/375, Val RMSE: 0.6466, Time: 3.31s\n",
      "Trial 86, Epoch 22/375, Val RMSE: 0.6530, Time: 3.32s\n",
      "Trial 86, Epoch 23/375, Val RMSE: 0.6377, Time: 3.28s\n",
      "Trial 86, Epoch 24/375, Val RMSE: 0.6343, Time: 3.19s\n",
      "Trial 86, Epoch 25/375, Val RMSE: 0.6476, Time: 3.29s\n",
      "Trial 86, Epoch 26/375, Val RMSE: 0.6385, Time: 3.28s\n",
      "Trial 86, Epoch 27/375, Val RMSE: 0.6330, Time: 3.32s\n",
      "Trial 86, Epoch 28/375, Val RMSE: 0.6349, Time: 3.29s\n",
      "Trial 86, Epoch 29/375, Val RMSE: 0.6399, Time: 3.31s\n",
      "Trial 86, Epoch 30/375, Val RMSE: 0.6341, Time: 3.28s\n",
      "Trial 86, Epoch 31/375, Val RMSE: 0.6388, Time: 3.29s\n",
      "Trial 86, Epoch 32/375, Val RMSE: 0.6357, Time: 3.30s\n",
      "Trial 86, Epoch 33/375, Val RMSE: 0.6453, Time: 3.29s\n",
      "Trial 86, Epoch 34/375, Val RMSE: 0.6368, Time: 3.30s\n",
      "Trial 86, Epoch 35/375, Val RMSE: 0.6368, Time: 3.33s\n",
      "Trial 86, Epoch 36/375, Val RMSE: 0.6492, Time: 3.27s\n",
      "Trial 86, Epoch 37/375, Val RMSE: 0.6415, Time: 3.30s\n",
      "Trial 86, Epoch 38/375, Val RMSE: 0.6408, Time: 3.27s\n",
      "Trial 86, Epoch 39/375, Val RMSE: 0.6273, Time: 3.32s\n",
      "Trial 86, Epoch 40/375, Val RMSE: 0.6384, Time: 3.34s\n",
      "Trial 86, Epoch 41/375, Val RMSE: 0.6319, Time: 3.30s\n",
      "Trial 86, Epoch 42/375, Val RMSE: 0.6530, Time: 3.33s\n",
      "Trial 86, Epoch 43/375, Val RMSE: 0.6292, Time: 3.37s\n",
      "Trial 86, Epoch 44/375, Val RMSE: 0.6338, Time: 3.26s\n",
      "Trial 86, Epoch 45/375, Val RMSE: 0.6350, Time: 3.34s\n",
      "Trial 86, Epoch 46/375, Val RMSE: 0.6396, Time: 3.23s\n",
      "Trial 86, Epoch 47/375, Val RMSE: 0.6366, Time: 3.54s\n",
      "Trial 86, Epoch 48/375, Val RMSE: 0.6329, Time: 3.31s\n",
      "Trial 86, Epoch 49/375, Val RMSE: 0.6524, Time: 3.30s\n",
      "Trial 86, Epoch 50/375, Val RMSE: 0.6390, Time: 3.34s\n",
      "Trial 86, Epoch 51/375, Val RMSE: 0.6320, Time: 3.31s\n",
      "Trial 86, Epoch 52/375, Val RMSE: 0.6358, Time: 3.32s\n",
      "Trial 86, Epoch 53/375, Val RMSE: 0.6352, Time: 3.33s\n",
      "Trial 86, Epoch 54/375, Val RMSE: 0.6338, Time: 3.24s\n",
      "Trial 86, Epoch 55/375, Val RMSE: 0.6316, Time: 3.34s\n",
      "Trial 86, Epoch 56/375, Val RMSE: 0.6397, Time: 3.21s\n",
      "Trial 86, Epoch 57/375, Val RMSE: 0.6428, Time: 3.31s\n",
      "Trial 86, Epoch 58/375, Val RMSE: 0.6373, Time: 3.31s\n",
      "Trial 86, Epoch 59/375, Val RMSE: 0.6325, Time: 3.29s\n",
      "Trial 86, Epoch 60/375, Val RMSE: 0.6388, Time: 3.32s\n",
      "Trial 86, Epoch 61/375, Val RMSE: 0.6350, Time: 3.35s\n",
      "Trial 86, Epoch 62/375, Val RMSE: 0.6360, Time: 3.28s\n",
      "Trial 86, Epoch 63/375, Val RMSE: 0.6470, Time: 3.32s\n",
      "Trial 86, Epoch 64/375, Val RMSE: 0.6309, Time: 3.30s\n",
      "Trial 86, Epoch 65/375, Val RMSE: 0.6449, Time: 3.34s\n",
      "Trial 86, Epoch 66/375, Val RMSE: 0.6377, Time: 3.32s\n",
      "Trial 86, Epoch 67/375, Val RMSE: 0.6311, Time: 3.30s\n",
      "Trial 86, Epoch 68/375, Val RMSE: 0.6377, Time: 3.32s\n",
      "Trial 86, Epoch 69/375, Val RMSE: 0.6387, Time: 3.30s\n",
      "Trial 86, Epoch 70/375, Val RMSE: 0.6312, Time: 3.34s\n",
      "Trial 86, Epoch 71/375, Val RMSE: 0.6330, Time: 3.34s\n",
      "Trial 86, Epoch 72/375, Val RMSE: 0.6286, Time: 3.30s\n",
      "Trial 86, Epoch 73/375, Val RMSE: 0.6316, Time: 3.30s\n",
      "Trial 86, Epoch 74/375, Val RMSE: 0.6357, Time: 3.33s\n",
      "Trial 86, Epoch 75/375, Val RMSE: 0.6333, Time: 3.34s\n",
      "Trial 86, Epoch 76/375, Val RMSE: 0.6390, Time: 3.34s\n",
      "Trial 86, Epoch 77/375, Val RMSE: 0.6377, Time: 3.35s\n",
      "Trial 86, Epoch 78/375, Val RMSE: 0.6287, Time: 3.29s\n",
      "Trial 86, Epoch 79/375, Val RMSE: 0.6275, Time: 3.36s\n",
      "Trial 86, Epoch 80/375, Val RMSE: 0.6350, Time: 3.36s\n",
      "Trial 86, Epoch 81/375, Val RMSE: 0.6309, Time: 3.35s\n",
      "Trial 86, Epoch 82/375, Val RMSE: 0.6331, Time: 3.37s\n",
      "Trial 86, Epoch 83/375, Val RMSE: 0.6310, Time: 3.31s\n",
      "Trial 86, Epoch 84/375, Val RMSE: 0.6403, Time: 3.32s\n",
      "Trial 86, Epoch 85/375, Val RMSE: 0.6380, Time: 3.28s\n",
      "Trial 86, Epoch 86/375, Val RMSE: 0.6307, Time: 3.35s\n",
      "Trial 86, Epoch 87/375, Val RMSE: 0.6340, Time: 3.39s\n",
      "Trial 86, Epoch 88/375, Val RMSE: 0.6341, Time: 3.35s\n",
      "[I 2025-07-17 12:34:16,134] Trial 86 pruned. \n",
      "Trial 87, Epoch 1/193, Val RMSE: 0.8262, Time: 3.40s\n",
      "Trial 87, Epoch 2/193, Val RMSE: 0.7407, Time: 3.37s\n",
      "Trial 87, Epoch 3/193, Val RMSE: 0.7289, Time: 3.33s\n",
      "Trial 87, Epoch 4/193, Val RMSE: 0.6962, Time: 3.39s\n",
      "Trial 87, Epoch 5/193, Val RMSE: 0.6853, Time: 3.41s\n",
      "Trial 87, Epoch 6/193, Val RMSE: 0.6851, Time: 3.34s\n",
      "Trial 87, Epoch 7/193, Val RMSE: 0.6756, Time: 3.41s\n",
      "Trial 87, Epoch 8/193, Val RMSE: 0.6678, Time: 3.41s\n",
      "Trial 87, Epoch 9/193, Val RMSE: 0.6636, Time: 3.41s\n",
      "Trial 87, Epoch 10/193, Val RMSE: 0.6533, Time: 3.35s\n",
      "Trial 87, Epoch 11/193, Val RMSE: 0.6502, Time: 3.36s\n",
      "Trial 87, Epoch 12/193, Val RMSE: 0.6531, Time: 3.31s\n",
      "Trial 87, Epoch 13/193, Val RMSE: 0.6524, Time: 3.36s\n",
      "Trial 87, Epoch 14/193, Val RMSE: 0.6480, Time: 3.33s\n",
      "Trial 87, Epoch 15/193, Val RMSE: 0.6431, Time: 3.27s\n",
      "Trial 87, Epoch 16/193, Val RMSE: 0.6539, Time: 3.35s\n",
      "Trial 87, Epoch 17/193, Val RMSE: 0.6488, Time: 3.33s\n",
      "Trial 87, Epoch 18/193, Val RMSE: 0.6485, Time: 3.31s\n",
      "Trial 87, Epoch 19/193, Val RMSE: 0.6478, Time: 3.33s\n",
      "Trial 87, Epoch 20/193, Val RMSE: 0.6508, Time: 3.32s\n",
      "Trial 87, Epoch 21/193, Val RMSE: 0.6452, Time: 3.34s\n",
      "Trial 87, Epoch 22/193, Val RMSE: 0.6457, Time: 3.34s\n",
      "Trial 87, Epoch 23/193, Val RMSE: 0.6412, Time: 3.28s\n",
      "Trial 87, Epoch 24/193, Val RMSE: 0.6484, Time: 3.32s\n",
      "Trial 87, Epoch 25/193, Val RMSE: 0.6479, Time: 3.32s\n",
      "Trial 87, Epoch 26/193, Val RMSE: 0.6465, Time: 3.39s\n",
      "Trial 87, Epoch 27/193, Val RMSE: 0.6421, Time: 3.41s\n",
      "Trial 87, Epoch 28/193, Val RMSE: 0.6377, Time: 3.39s\n",
      "Trial 87, Epoch 29/193, Val RMSE: 0.6475, Time: 3.38s\n",
      "Trial 87, Epoch 30/193, Val RMSE: 0.6486, Time: 3.34s\n",
      "Trial 87, Epoch 31/193, Val RMSE: 0.6471, Time: 3.42s\n",
      "Trial 87, Epoch 32/193, Val RMSE: 0.6498, Time: 3.29s\n",
      "Trial 87, Epoch 33/193, Val RMSE: 0.6462, Time: 3.33s\n",
      "Trial 87, Epoch 34/193, Val RMSE: 0.6481, Time: 3.39s\n",
      "Trial 87, Epoch 35/193, Val RMSE: 0.6481, Time: 3.36s\n",
      "Trial 87, Epoch 36/193, Val RMSE: 0.6529, Time: 3.36s\n",
      "Trial 87, Epoch 37/193, Val RMSE: 0.6468, Time: 3.33s\n",
      "Trial 87, Epoch 38/193, Val RMSE: 0.6443, Time: 3.36s\n",
      "Trial 87, Epoch 39/193, Val RMSE: 0.6464, Time: 3.36s\n",
      "Trial 87, Epoch 40/193, Val RMSE: 0.6441, Time: 3.31s\n",
      "Trial 87, Epoch 41/193, Val RMSE: 0.6490, Time: 3.31s\n",
      "Trial 87, Epoch 42/193, Val RMSE: 0.6424, Time: 3.23s\n",
      "Trial 87, Epoch 43/193, Val RMSE: 0.6459, Time: 3.33s\n",
      "Trial 87, Epoch 44/193, Val RMSE: 0.6423, Time: 3.33s\n",
      "Trial 87, Epoch 45/193, Val RMSE: 0.6455, Time: 3.38s\n",
      "Trial 87, Epoch 46/193, Val RMSE: 0.6411, Time: 3.24s\n",
      "Trial 87, Epoch 47/193, Val RMSE: 0.6489, Time: 3.30s\n",
      "Trial 87, Epoch 48/193, Val RMSE: 0.6528, Time: 3.58s\n",
      "Trial 87, Epoch 49/193, Val RMSE: 0.6487, Time: 3.28s\n",
      "Trial 87, Epoch 50/193, Val RMSE: 0.6442, Time: 3.32s\n",
      "Trial 87, Epoch 51/193, Val RMSE: 0.6457, Time: 3.33s\n",
      "Trial 87, Epoch 52/193, Val RMSE: 0.6456, Time: 3.32s\n",
      "Trial 87, Epoch 53/193, Val RMSE: 0.6449, Time: 3.28s\n",
      "Trial 87, Epoch 54/193, Val RMSE: 0.6429, Time: 3.34s\n",
      "Trial 87, Epoch 55/193, Val RMSE: 0.6433, Time: 3.28s\n",
      "Trial 87, Epoch 56/193, Val RMSE: 0.6459, Time: 3.27s\n",
      "Trial 87, Epoch 57/193, Val RMSE: 0.6412, Time: 3.30s\n",
      "Trial 87, Epoch 58/193, Val RMSE: 0.6439, Time: 3.34s\n",
      "Trial 87, Epoch 59/193, Val RMSE: 0.6421, Time: 3.35s\n",
      "Trial 87, Epoch 60/193, Val RMSE: 0.6541, Time: 3.37s\n",
      "Trial 87, Epoch 61/193, Val RMSE: 0.6401, Time: 3.32s\n",
      "Trial 87, Epoch 62/193, Val RMSE: 0.6396, Time: 3.30s\n",
      "Trial 87, Epoch 63/193, Val RMSE: 0.6469, Time: 3.34s\n",
      "Trial 87, Epoch 64/193, Val RMSE: 0.6410, Time: 3.34s\n",
      "Trial 87, Epoch 65/193, Val RMSE: 0.6421, Time: 3.35s\n",
      "Trial 87, Epoch 66/193, Val RMSE: 0.6430, Time: 3.30s\n",
      "Trial 87, Epoch 67/193, Val RMSE: 0.6430, Time: 3.32s\n",
      "Trial 87, Epoch 68/193, Val RMSE: 0.6453, Time: 3.32s\n",
      "Trial 87, Epoch 69/193, Val RMSE: 0.6404, Time: 3.39s\n",
      "Trial 87, Epoch 70/193, Val RMSE: 0.6347, Time: 3.35s\n",
      "Trial 87, Epoch 71/193, Val RMSE: 0.6408, Time: 3.33s\n",
      "Trial 87, Epoch 72/193, Val RMSE: 0.6431, Time: 3.32s\n",
      "Trial 87, Epoch 73/193, Val RMSE: 0.6439, Time: 3.36s\n",
      "Trial 87, Epoch 74/193, Val RMSE: 0.6445, Time: 3.34s\n",
      "Trial 87, Epoch 75/193, Val RMSE: 0.6399, Time: 3.35s\n",
      "Trial 87, Epoch 76/193, Val RMSE: 0.6385, Time: 3.30s\n",
      "Trial 87, Epoch 77/193, Val RMSE: 0.6410, Time: 3.30s\n",
      "Trial 87, Epoch 78/193, Val RMSE: 0.6425, Time: 3.29s\n",
      "[I 2025-07-17 12:38:38,576] Trial 87 pruned. \n",
      "Trial 88, Epoch 1/199, Val RMSE: 0.8417, Time: 3.34s\n",
      "Trial 88, Epoch 2/199, Val RMSE: 0.7341, Time: 3.36s\n",
      "Trial 88, Epoch 3/199, Val RMSE: 0.7167, Time: 3.29s\n",
      "Trial 88, Epoch 4/199, Val RMSE: 0.7017, Time: 3.36s\n",
      "Trial 88, Epoch 5/199, Val RMSE: 0.6808, Time: 3.36s\n",
      "Trial 88, Epoch 6/199, Val RMSE: 0.6800, Time: 3.32s\n",
      "Trial 88, Epoch 7/199, Val RMSE: 0.6665, Time: 3.28s\n",
      "Trial 88, Epoch 8/199, Val RMSE: 0.6738, Time: 3.29s\n",
      "Trial 88, Epoch 9/199, Val RMSE: 0.6590, Time: 3.32s\n",
      "Trial 88, Epoch 10/199, Val RMSE: 0.6600, Time: 3.32s\n",
      "Trial 88, Epoch 11/199, Val RMSE: 0.6464, Time: 3.34s\n",
      "Trial 88, Epoch 12/199, Val RMSE: 0.6582, Time: 3.36s\n",
      "Trial 88, Epoch 13/199, Val RMSE: 0.6572, Time: 3.31s\n",
      "Trial 88, Epoch 14/199, Val RMSE: 0.6798, Time: 3.37s\n",
      "Trial 88, Epoch 15/199, Val RMSE: 0.6563, Time: 3.33s\n",
      "Trial 88, Epoch 16/199, Val RMSE: 0.6425, Time: 3.31s\n",
      "Trial 88, Epoch 17/199, Val RMSE: 0.6488, Time: 3.27s\n",
      "Trial 88, Epoch 18/199, Val RMSE: 0.6515, Time: 3.27s\n",
      "Trial 88, Epoch 19/199, Val RMSE: 0.6558, Time: 3.32s\n",
      "Trial 88, Epoch 20/199, Val RMSE: 0.6464, Time: 3.30s\n",
      "Trial 88, Epoch 21/199, Val RMSE: 0.6537, Time: 3.35s\n",
      "Trial 88, Epoch 22/199, Val RMSE: 0.6592, Time: 3.34s\n",
      "Trial 88, Epoch 23/199, Val RMSE: 0.6525, Time: 3.31s\n",
      "Trial 88, Epoch 24/199, Val RMSE: 0.6560, Time: 3.34s\n",
      "Trial 88, Epoch 25/199, Val RMSE: 0.6431, Time: 3.31s\n",
      "Trial 88, Epoch 26/199, Val RMSE: 0.6627, Time: 3.28s\n",
      "Trial 88, Epoch 27/199, Val RMSE: 0.6542, Time: 3.31s\n",
      "Trial 88, Epoch 28/199, Val RMSE: 0.6471, Time: 3.30s\n",
      "Trial 88, Epoch 29/199, Val RMSE: 0.6522, Time: 3.31s\n",
      "Trial 88, Epoch 30/199, Val RMSE: 0.6515, Time: 3.34s\n",
      "Trial 88, Epoch 31/199, Val RMSE: 0.6497, Time: 3.29s\n",
      "Trial 88, Epoch 32/199, Val RMSE: 0.6432, Time: 3.28s\n",
      "Trial 88, Epoch 33/199, Val RMSE: 0.6600, Time: 3.80s\n",
      "Trial 88, Epoch 34/199, Val RMSE: 0.6493, Time: 3.41s\n",
      "Trial 88, Epoch 35/199, Val RMSE: 0.6431, Time: 3.48s\n",
      "Trial 88, Epoch 36/199, Val RMSE: 0.6505, Time: 3.42s\n",
      "Trial 88, Epoch 37/199, Val RMSE: 0.6474, Time: 3.52s\n",
      "Trial 88, Epoch 38/199, Val RMSE: 0.6504, Time: 3.47s\n",
      "Trial 88, Epoch 39/199, Val RMSE: 0.6486, Time: 3.37s\n",
      "Trial 88, Epoch 40/199, Val RMSE: 0.6520, Time: 3.32s\n",
      "Trial 88, Epoch 41/199, Val RMSE: 0.6564, Time: 3.30s\n",
      "Trial 88, Epoch 42/199, Val RMSE: 0.6454, Time: 3.39s\n",
      "Trial 88, Epoch 43/199, Val RMSE: 0.6542, Time: 3.37s\n",
      "Trial 88, Epoch 44/199, Val RMSE: 0.6506, Time: 3.31s\n",
      "Trial 88, Epoch 45/199, Val RMSE: 0.6455, Time: 3.33s\n",
      "Trial 88, Epoch 46/199, Val RMSE: 0.6445, Time: 3.40s\n",
      "Trial 88, Epoch 47/199, Val RMSE: 0.6500, Time: 3.32s\n",
      "Trial 88, Epoch 48/199, Val RMSE: 0.6486, Time: 3.30s\n",
      "Trial 88, Epoch 49/199, Val RMSE: 0.6435, Time: 3.25s\n",
      "Trial 88, Epoch 50/199, Val RMSE: 0.6428, Time: 3.39s\n",
      "Trial 88, Epoch 51/199, Val RMSE: 0.6448, Time: 3.31s\n",
      "Trial 88, Epoch 52/199, Val RMSE: 0.6407, Time: 3.32s\n",
      "Trial 88, Epoch 53/199, Val RMSE: 0.6473, Time: 3.31s\n",
      "Trial 88, Epoch 54/199, Val RMSE: 0.6442, Time: 3.33s\n",
      "Trial 88, Epoch 55/199, Val RMSE: 0.6512, Time: 3.30s\n",
      "Trial 88, Epoch 56/199, Val RMSE: 0.6461, Time: 3.35s\n",
      "Trial 88, Epoch 57/199, Val RMSE: 0.6509, Time: 3.35s\n",
      "Trial 88, Epoch 58/199, Val RMSE: 0.6441, Time: 3.32s\n",
      "Trial 88, Epoch 59/199, Val RMSE: 0.6448, Time: 3.55s\n",
      "Trial 88, Epoch 60/199, Val RMSE: 0.6423, Time: 3.89s\n",
      "Trial 88, Epoch 61/199, Val RMSE: 0.6402, Time: 3.74s\n",
      "Trial 88, Epoch 62/199, Val RMSE: 0.6456, Time: 3.70s\n",
      "Trial 88, Epoch 63/199, Val RMSE: 0.6448, Time: 3.59s\n",
      "Trial 88, Epoch 64/199, Val RMSE: 0.6465, Time: 3.56s\n",
      "Trial 88, Epoch 65/199, Val RMSE: 0.6399, Time: 3.32s\n",
      "Trial 88, Epoch 66/199, Val RMSE: 0.6409, Time: 3.33s\n",
      "Trial 88, Epoch 67/199, Val RMSE: 0.6460, Time: 3.34s\n",
      "Trial 88, Epoch 68/199, Val RMSE: 0.6408, Time: 3.35s\n",
      "Trial 88, Epoch 69/199, Val RMSE: 0.6356, Time: 3.26s\n",
      "Trial 88, Epoch 70/199, Val RMSE: 0.6434, Time: 3.39s\n",
      "Trial 88, Epoch 71/199, Val RMSE: 0.6424, Time: 3.28s\n",
      "Trial 88, Epoch 72/199, Val RMSE: 0.6504, Time: 3.32s\n",
      "Trial 88, Epoch 73/199, Val RMSE: 0.6375, Time: 3.32s\n",
      "Trial 88, Epoch 74/199, Val RMSE: 0.6436, Time: 3.31s\n",
      "Trial 88, Epoch 75/199, Val RMSE: 0.6497, Time: 3.28s\n",
      "Trial 88, Epoch 76/199, Val RMSE: 0.6453, Time: 3.30s\n",
      "Trial 88, Epoch 77/199, Val RMSE: 0.6475, Time: 3.38s\n",
      "Trial 88, Epoch 78/199, Val RMSE: 0.6467, Time: 3.37s\n",
      "[I 2025-07-17 12:43:02,909] Trial 88 pruned. \n",
      "Trial 89, Epoch 1/218, Val RMSE: 0.8152, Time: 3.30s\n",
      "Trial 89, Epoch 2/218, Val RMSE: 0.7543, Time: 3.30s\n",
      "Trial 89, Epoch 3/218, Val RMSE: 0.7194, Time: 3.34s\n",
      "Trial 89, Epoch 4/218, Val RMSE: 0.7115, Time: 3.31s\n",
      "Trial 89, Epoch 5/218, Val RMSE: 0.7077, Time: 3.37s\n",
      "[I 2025-07-17 12:43:19,843] Trial 89 pruned. \n",
      "Trial 90, Epoch 1/178, Val RMSE: 0.9005, Time: 3.33s\n",
      "Trial 90, Epoch 2/178, Val RMSE: 0.7563, Time: 3.30s\n",
      "Trial 90, Epoch 3/178, Val RMSE: 0.7283, Time: 3.31s\n",
      "Trial 90, Epoch 4/178, Val RMSE: 0.7087, Time: 3.29s\n",
      "Trial 90, Epoch 5/178, Val RMSE: 0.6947, Time: 3.36s\n",
      "Trial 90, Epoch 6/178, Val RMSE: 0.6852, Time: 3.30s\n",
      "Trial 90, Epoch 7/178, Val RMSE: 0.6783, Time: 3.34s\n",
      "Trial 90, Epoch 8/178, Val RMSE: 0.6674, Time: 3.30s\n",
      "Trial 90, Epoch 9/178, Val RMSE: 0.6614, Time: 3.32s\n",
      "Trial 90, Epoch 10/178, Val RMSE: 0.6682, Time: 3.39s\n",
      "Trial 90, Epoch 11/178, Val RMSE: 0.6538, Time: 3.40s\n",
      "Trial 90, Epoch 12/178, Val RMSE: 0.6441, Time: 3.41s\n",
      "Trial 90, Epoch 13/178, Val RMSE: 0.6595, Time: 3.33s\n",
      "Trial 90, Epoch 14/178, Val RMSE: 0.6475, Time: 3.27s\n",
      "Trial 90, Epoch 15/178, Val RMSE: 0.6502, Time: 3.27s\n",
      "Trial 90, Epoch 16/178, Val RMSE: 0.6474, Time: 3.29s\n",
      "Trial 90, Epoch 17/178, Val RMSE: 0.6418, Time: 3.30s\n",
      "Trial 90, Epoch 18/178, Val RMSE: 0.6488, Time: 3.32s\n",
      "Trial 90, Epoch 19/178, Val RMSE: 0.6410, Time: 3.26s\n",
      "Trial 90, Epoch 20/178, Val RMSE: 0.6422, Time: 3.24s\n",
      "Trial 90, Epoch 21/178, Val RMSE: 0.6431, Time: 3.33s\n",
      "Trial 90, Epoch 22/178, Val RMSE: 0.6373, Time: 3.28s\n",
      "Trial 90, Epoch 23/178, Val RMSE: 0.6429, Time: 3.25s\n",
      "Trial 90, Epoch 24/178, Val RMSE: 0.6403, Time: 3.28s\n",
      "Trial 90, Epoch 25/178, Val RMSE: 0.6432, Time: 3.29s\n",
      "Trial 90, Epoch 26/178, Val RMSE: 0.6505, Time: 3.26s\n",
      "Trial 90, Epoch 27/178, Val RMSE: 0.6434, Time: 3.26s\n",
      "Trial 90, Epoch 28/178, Val RMSE: 0.6486, Time: 3.32s\n",
      "Trial 90, Epoch 29/178, Val RMSE: 0.6563, Time: 3.26s\n",
      "Trial 90, Epoch 30/178, Val RMSE: 0.6365, Time: 3.30s\n",
      "Trial 90, Epoch 31/178, Val RMSE: 0.6473, Time: 3.31s\n",
      "Trial 90, Epoch 32/178, Val RMSE: 0.6370, Time: 3.23s\n",
      "Trial 90, Epoch 33/178, Val RMSE: 0.6491, Time: 3.28s\n",
      "Trial 90, Epoch 34/178, Val RMSE: 0.6413, Time: 3.27s\n",
      "Trial 90, Epoch 35/178, Val RMSE: 0.6418, Time: 3.30s\n",
      "Trial 90, Epoch 36/178, Val RMSE: 0.6530, Time: 3.33s\n",
      "Trial 90, Epoch 37/178, Val RMSE: 0.6483, Time: 3.29s\n",
      "Trial 90, Epoch 38/178, Val RMSE: 0.6509, Time: 3.23s\n",
      "Trial 90, Epoch 39/178, Val RMSE: 0.6528, Time: 3.30s\n",
      "Trial 90, Epoch 40/178, Val RMSE: 0.6459, Time: 3.29s\n",
      "Trial 90, Epoch 41/178, Val RMSE: 0.6517, Time: 3.27s\n",
      "Trial 90, Epoch 42/178, Val RMSE: 0.6457, Time: 3.29s\n",
      "Trial 90, Epoch 43/178, Val RMSE: 0.6452, Time: 3.28s\n",
      "Trial 90, Epoch 44/178, Val RMSE: 0.6438, Time: 3.26s\n",
      "Trial 90, Epoch 45/178, Val RMSE: 0.6432, Time: 3.33s\n",
      "Trial 90, Epoch 46/178, Val RMSE: 0.6489, Time: 3.33s\n",
      "Trial 90, Epoch 47/178, Val RMSE: 0.6445, Time: 3.31s\n",
      "Trial 90, Epoch 48/178, Val RMSE: 0.6466, Time: 3.35s\n",
      "Trial 90, Epoch 49/178, Val RMSE: 0.6438, Time: 3.51s\n",
      "Trial 90, Epoch 50/178, Val RMSE: 0.6505, Time: 3.33s\n",
      "Trial 90, Epoch 51/178, Val RMSE: 0.6488, Time: 3.30s\n",
      "Trial 90, Epoch 52/178, Val RMSE: 0.6459, Time: 3.30s\n",
      "Trial 90, Epoch 53/178, Val RMSE: 0.6467, Time: 3.26s\n",
      "Trial 90, Epoch 54/178, Val RMSE: 0.6457, Time: 3.28s\n",
      "Trial 90, Epoch 55/178, Val RMSE: 0.6543, Time: 3.35s\n",
      "Trial 90, Epoch 56/178, Val RMSE: 0.6468, Time: 3.31s\n",
      "Trial 90, Epoch 57/178, Val RMSE: 0.6415, Time: 3.33s\n",
      "Trial 90, Epoch 58/178, Val RMSE: 0.6459, Time: 3.36s\n",
      "Trial 90, Epoch 59/178, Val RMSE: 0.6421, Time: 3.32s\n",
      "Trial 90, Epoch 60/178, Val RMSE: 0.6473, Time: 3.27s\n",
      "Trial 90, Epoch 61/178, Val RMSE: 0.6406, Time: 3.33s\n",
      "Trial 90, Epoch 62/178, Val RMSE: 0.6459, Time: 3.31s\n",
      "Trial 90, Epoch 63/178, Val RMSE: 0.6470, Time: 3.39s\n",
      "Trial 90, Epoch 64/178, Val RMSE: 0.6475, Time: 3.28s\n",
      "Trial 90, Epoch 65/178, Val RMSE: 0.6553, Time: 3.25s\n",
      "Trial 90, Epoch 66/178, Val RMSE: 0.6429, Time: 3.27s\n",
      "Trial 90, Epoch 67/178, Val RMSE: 0.6513, Time: 3.42s\n",
      "Trial 90, Epoch 68/178, Val RMSE: 0.6481, Time: 3.28s\n",
      "Trial 90, Epoch 69/178, Val RMSE: 0.6476, Time: 3.28s\n",
      "Trial 90, Epoch 70/178, Val RMSE: 0.6569, Time: 3.28s\n",
      "Trial 90, Epoch 71/178, Val RMSE: 0.6493, Time: 3.25s\n",
      "Trial 90, Epoch 72/178, Val RMSE: 0.6495, Time: 3.32s\n",
      "Trial 90, Epoch 73/178, Val RMSE: 0.6469, Time: 3.26s\n",
      "Trial 90, Epoch 74/178, Val RMSE: 0.6488, Time: 3.34s\n",
      "Trial 90, Epoch 75/178, Val RMSE: 0.6415, Time: 3.29s\n",
      "Trial 90, Epoch 76/178, Val RMSE: 0.6534, Time: 3.19s\n",
      "Trial 90, Epoch 77/178, Val RMSE: 0.6512, Time: 3.23s\n",
      "Trial 90, Epoch 78/178, Val RMSE: 0.6469, Time: 3.24s\n",
      "[I 2025-07-17 12:47:39,181] Trial 90 pruned. \n",
      "Trial 91, Epoch 1/164, Val RMSE: 0.8038, Time: 3.33s\n",
      "Trial 91, Epoch 2/164, Val RMSE: 0.7379, Time: 3.32s\n",
      "Trial 91, Epoch 3/164, Val RMSE: 0.7192, Time: 3.32s\n",
      "Trial 91, Epoch 4/164, Val RMSE: 0.7083, Time: 3.36s\n",
      "Trial 91, Epoch 5/164, Val RMSE: 0.6832, Time: 3.29s\n",
      "Trial 91, Epoch 6/164, Val RMSE: 0.6719, Time: 3.31s\n",
      "Trial 91, Epoch 7/164, Val RMSE: 0.6780, Time: 3.32s\n",
      "Trial 91, Epoch 8/164, Val RMSE: 0.6746, Time: 3.33s\n",
      "Trial 91, Epoch 9/164, Val RMSE: 0.6728, Time: 3.25s\n",
      "[I 2025-07-17 12:48:09,427] Trial 91 pruned. \n",
      "Trial 92, Epoch 1/288, Val RMSE: 0.8382, Time: 13.23s\n",
      "Trial 92, Epoch 2/288, Val RMSE: 0.7522, Time: 13.07s\n",
      "Trial 92, Epoch 3/288, Val RMSE: 0.7078, Time: 13.17s\n",
      "Trial 92, Epoch 4/288, Val RMSE: 0.6988, Time: 13.18s\n",
      "Trial 92, Epoch 5/288, Val RMSE: 0.6570, Time: 13.29s\n",
      "Trial 92, Epoch 6/288, Val RMSE: 0.6721, Time: 13.07s\n",
      "Trial 92, Epoch 7/288, Val RMSE: 0.6631, Time: 13.18s\n",
      "Trial 92, Epoch 8/288, Val RMSE: 0.6673, Time: 13.26s\n",
      "Trial 92, Epoch 9/288, Val RMSE: 0.6409, Time: 13.15s\n",
      "Trial 92, Epoch 10/288, Val RMSE: 0.6536, Time: 13.16s\n",
      "Trial 92, Epoch 11/288, Val RMSE: 0.6354, Time: 13.22s\n",
      "Trial 92, Epoch 12/288, Val RMSE: 0.6363, Time: 13.05s\n",
      "Trial 92, Epoch 13/288, Val RMSE: 0.6244, Time: 13.28s\n",
      "Trial 92, Epoch 14/288, Val RMSE: 0.6355, Time: 13.27s\n",
      "Trial 92, Epoch 15/288, Val RMSE: 0.6281, Time: 13.26s\n",
      "Trial 92, Epoch 16/288, Val RMSE: 0.6334, Time: 13.29s\n",
      "Trial 92, Epoch 17/288, Val RMSE: 0.6231, Time: 13.36s\n",
      "Trial 92, Epoch 18/288, Val RMSE: 0.6430, Time: 13.54s\n",
      "Trial 92, Epoch 19/288, Val RMSE: 0.6314, Time: 13.85s\n",
      "Trial 92, Epoch 20/288, Val RMSE: 0.6261, Time: 13.50s\n",
      "Trial 92, Epoch 21/288, Val RMSE: 0.6283, Time: 13.15s\n",
      "Trial 92, Epoch 22/288, Val RMSE: 0.6302, Time: 13.30s\n",
      "Trial 92, Epoch 23/288, Val RMSE: 0.6327, Time: 13.16s\n",
      "Trial 92, Epoch 24/288, Val RMSE: 0.6336, Time: 13.22s\n",
      "Trial 92, Epoch 25/288, Val RMSE: 0.6222, Time: 13.29s\n",
      "Trial 92, Epoch 26/288, Val RMSE: 0.6226, Time: 13.25s\n",
      "Trial 92, Epoch 27/288, Val RMSE: 0.6251, Time: 13.28s\n",
      "Trial 92, Epoch 28/288, Val RMSE: 0.6329, Time: 13.23s\n",
      "Trial 92, Epoch 29/288, Val RMSE: 0.6247, Time: 13.12s\n",
      "Trial 92, Epoch 30/288, Val RMSE: 0.6293, Time: 12.99s\n",
      "Trial 92, Epoch 31/288, Val RMSE: 0.6287, Time: 13.07s\n",
      "Trial 92, Epoch 32/288, Val RMSE: 0.6217, Time: 13.10s\n",
      "Trial 92, Epoch 33/288, Val RMSE: 0.6344, Time: 13.73s\n",
      "Trial 92, Epoch 34/288, Val RMSE: 0.6329, Time: 13.57s\n",
      "Trial 92, Epoch 35/288, Val RMSE: 0.6234, Time: 13.24s\n",
      "Trial 92, Epoch 36/288, Val RMSE: 0.6213, Time: 13.80s\n",
      "Trial 92, Epoch 37/288, Val RMSE: 0.6242, Time: 13.19s\n",
      "Trial 92, Epoch 38/288, Val RMSE: 0.6271, Time: 13.15s\n",
      "Trial 92, Epoch 39/288, Val RMSE: 0.6272, Time: 13.32s\n",
      "Trial 92, Epoch 40/288, Val RMSE: 0.6175, Time: 13.72s\n",
      "Trial 92, Epoch 41/288, Val RMSE: 0.6248, Time: 13.27s\n",
      "Trial 92, Epoch 42/288, Val RMSE: 0.6308, Time: 13.22s\n",
      "Trial 92, Epoch 43/288, Val RMSE: 0.6359, Time: 13.24s\n",
      "Trial 92, Epoch 44/288, Val RMSE: 0.6257, Time: 13.35s\n",
      "Trial 92, Epoch 45/288, Val RMSE: 0.6271, Time: 13.46s\n",
      "Trial 92, Epoch 46/288, Val RMSE: 0.6458, Time: 13.66s\n",
      "Trial 92, Epoch 47/288, Val RMSE: 0.6284, Time: 13.23s\n",
      "Trial 92, Epoch 48/288, Val RMSE: 0.6294, Time: 13.19s\n",
      "Trial 92, Epoch 49/288, Val RMSE: 0.6217, Time: 13.38s\n",
      "Trial 92, Epoch 50/288, Val RMSE: 0.6188, Time: 13.21s\n",
      "Trial 92, Epoch 51/288, Val RMSE: 0.6299, Time: 13.28s\n",
      "Trial 92, Epoch 52/288, Val RMSE: 0.6228, Time: 13.18s\n",
      "Trial 92, Epoch 53/288, Val RMSE: 0.6174, Time: 13.72s\n",
      "Trial 92, Epoch 54/288, Val RMSE: 0.6237, Time: 13.65s\n",
      "Trial 92, Epoch 55/288, Val RMSE: 0.6350, Time: 13.15s\n",
      "Trial 92, Epoch 56/288, Val RMSE: 0.6245, Time: 13.20s\n",
      "Trial 92, Epoch 57/288, Val RMSE: 0.6310, Time: 13.11s\n",
      "Trial 92, Epoch 58/288, Val RMSE: 0.6207, Time: 13.19s\n",
      "Trial 92, Epoch 59/288, Val RMSE: 0.6327, Time: 13.25s\n",
      "Trial 92, Epoch 60/288, Val RMSE: 0.6262, Time: 13.36s\n",
      "Trial 92, Epoch 61/288, Val RMSE: 0.6299, Time: 13.24s\n",
      "Trial 92, Epoch 62/288, Val RMSE: 0.6260, Time: 13.82s\n",
      "Trial 92, Epoch 63/288, Val RMSE: 0.6307, Time: 13.46s\n",
      "Trial 92, Epoch 64/288, Val RMSE: 0.6304, Time: 13.47s\n",
      "Trial 92, Epoch 65/288, Val RMSE: 0.6171, Time: 13.28s\n",
      "Trial 92, Epoch 66/288, Val RMSE: 0.6297, Time: 13.27s\n",
      "Trial 92, Epoch 67/288, Val RMSE: 0.6292, Time: 13.25s\n",
      "Trial 92, Epoch 68/288, Val RMSE: 0.6287, Time: 13.20s\n",
      "Trial 92, Epoch 69/288, Val RMSE: 0.6271, Time: 15.55s\n",
      "Trial 92, Epoch 70/288, Val RMSE: 0.6229, Time: 13.19s\n",
      "Trial 92, Epoch 71/288, Val RMSE: 0.6360, Time: 13.35s\n",
      "Trial 92, Epoch 72/288, Val RMSE: 0.6246, Time: 13.43s\n",
      "Trial 92, Epoch 73/288, Val RMSE: 0.6252, Time: 13.29s\n",
      "Trial 92, Epoch 74/288, Val RMSE: 0.6266, Time: 13.31s\n",
      "Trial 92, Epoch 75/288, Val RMSE: 0.6258, Time: 13.23s\n",
      "Trial 92, Epoch 76/288, Val RMSE: 0.6216, Time: 13.26s\n",
      "Trial 92, Epoch 77/288, Val RMSE: 0.6242, Time: 13.27s\n",
      "Trial 92, Epoch 78/288, Val RMSE: 0.6227, Time: 13.13s\n",
      "Trial 92, Epoch 79/288, Val RMSE: 0.6330, Time: 13.31s\n",
      "Trial 92, Epoch 80/288, Val RMSE: 0.6211, Time: 13.37s\n",
      "Trial 92, Epoch 81/288, Val RMSE: 0.6320, Time: 13.64s\n",
      "Trial 92, Epoch 82/288, Val RMSE: 0.6257, Time: 13.67s\n",
      "Trial 92, Epoch 83/288, Val RMSE: 0.6245, Time: 13.74s\n",
      "Trial 92, Epoch 84/288, Val RMSE: 0.6350, Time: 13.29s\n",
      "Trial 92, Epoch 85/288, Val RMSE: 0.6252, Time: 15.18s\n",
      "Trial 92, Epoch 86/288, Val RMSE: 0.6180, Time: 19.27s\n",
      "Trial 92, Epoch 87/288, Val RMSE: 0.6315, Time: 17.09s\n",
      "Trial 92, Epoch 88/288, Val RMSE: 0.6240, Time: 17.32s\n",
      "Trial 92, Epoch 89/288, Val RMSE: 0.6226, Time: 14.88s\n",
      "Trial 92, Epoch 90/288, Val RMSE: 0.6272, Time: 15.60s\n",
      "Trial 92, Epoch 91/288, Val RMSE: 0.6223, Time: 15.84s\n",
      "Trial 92, Epoch 92/288, Val RMSE: 0.6212, Time: 15.67s\n",
      "Trial 92, Epoch 93/288, Val RMSE: 0.6247, Time: 15.68s\n",
      "Trial 92, Epoch 94/288, Val RMSE: 0.6207, Time: 15.14s\n",
      "Trial 92, Epoch 95/288, Val RMSE: 0.6243, Time: 15.45s\n",
      "Trial 92, Epoch 96/288, Val RMSE: 0.6254, Time: 14.43s\n",
      "Trial 92, Epoch 97/288, Val RMSE: 0.6231, Time: 14.65s\n",
      "Trial 92, Epoch 98/288, Val RMSE: 0.6233, Time: 13.93s\n",
      "Trial 92, Epoch 99/288, Val RMSE: 0.6239, Time: 13.83s\n",
      "Trial 92, Epoch 100/288, Val RMSE: 0.6261, Time: 14.14s\n",
      "Trial 92, Epoch 101/288, Val RMSE: 0.6257, Time: 14.70s\n",
      "Trial 92, Epoch 102/288, Val RMSE: 0.6215, Time: 13.70s\n",
      "Trial 92, Epoch 103/288, Val RMSE: 0.6266, Time: 14.24s\n",
      "Trial 92, Epoch 104/288, Val RMSE: 0.6185, Time: 14.76s\n",
      "Trial 92, Epoch 105/288, Val RMSE: 0.6237, Time: 14.72s\n",
      "Trial 92, Epoch 106/288, Val RMSE: 0.6236, Time: 13.08s\n",
      "Trial 92, Epoch 107/288, Val RMSE: 0.6213, Time: 12.82s\n",
      "Trial 92, Epoch 108/288, Val RMSE: 0.6256, Time: 12.80s\n",
      "Trial 92, Epoch 109/288, Val RMSE: 0.6211, Time: 12.97s\n",
      "Trial 92, Epoch 110/288, Val RMSE: 0.6246, Time: 13.04s\n",
      "Trial 92, Epoch 111/288, Val RMSE: 0.6301, Time: 13.24s\n",
      "Trial 92, Epoch 112/288, Val RMSE: 0.6218, Time: 13.31s\n",
      "Trial 92, Epoch 113/288, Val RMSE: 0.6274, Time: 13.26s\n",
      "Trial 92, Epoch 114/288, Val RMSE: 0.6208, Time: 13.30s\n",
      "Trial 92, Epoch 115/288, Val RMSE: 0.6247, Time: 13.27s\n",
      "Early stopping at epoch 115 for trial 92\n",
      "[I 2025-07-17 13:14:25,092] Trial 92 finished with value: 0.6247360479014821 and parameters: {'hidden_channels': 599, 'lr': 0.0002482628349651697, 'batch_size': 32, 'n_epochs': 288, 'num_layers': 3, 'dropout_rate': 0.1865055653225735, 'weight_decay': 2.0142708820606416e-08}. Best is trial 60 with value: 0.609277393700007.\n",
      "Trial 93, Epoch 1/294, Val RMSE: 0.8238, Time: 13.45s\n",
      "Trial 93, Epoch 2/294, Val RMSE: 0.7490, Time: 13.01s\n",
      "Trial 93, Epoch 3/294, Val RMSE: 0.7111, Time: 13.06s\n",
      "Trial 93, Epoch 4/294, Val RMSE: 0.7324, Time: 13.04s\n",
      "Trial 93, Epoch 5/294, Val RMSE: 0.6643, Time: 13.15s\n",
      "Trial 93, Epoch 6/294, Val RMSE: 0.6757, Time: 13.22s\n",
      "Trial 93, Epoch 7/294, Val RMSE: 0.6438, Time: 13.18s\n",
      "Trial 93, Epoch 8/294, Val RMSE: 0.6542, Time: 13.62s\n",
      "Trial 93, Epoch 9/294, Val RMSE: 0.6481, Time: 13.48s\n",
      "Trial 93, Epoch 10/294, Val RMSE: 0.6336, Time: 13.51s\n",
      "Trial 93, Epoch 11/294, Val RMSE: 0.6325, Time: 13.50s\n",
      "Trial 93, Epoch 12/294, Val RMSE: 0.6534, Time: 13.83s\n",
      "Trial 93, Epoch 13/294, Val RMSE: 0.6143, Time: 13.41s\n",
      "Trial 93, Epoch 14/294, Val RMSE: 0.6275, Time: 13.48s\n",
      "Trial 93, Epoch 15/294, Val RMSE: 0.6152, Time: 13.53s\n",
      "Trial 93, Epoch 16/294, Val RMSE: 0.6302, Time: 13.48s\n",
      "Trial 93, Epoch 17/294, Val RMSE: 0.6280, Time: 13.57s\n",
      "Trial 93, Epoch 18/294, Val RMSE: 0.6194, Time: 13.49s\n",
      "Trial 93, Epoch 19/294, Val RMSE: 0.6204, Time: 13.51s\n",
      "Trial 93, Epoch 20/294, Val RMSE: 0.6439, Time: 13.22s\n",
      "Trial 93, Epoch 21/294, Val RMSE: 0.6208, Time: 13.30s\n",
      "Trial 93, Epoch 22/294, Val RMSE: 0.6145, Time: 13.33s\n",
      "Trial 93, Epoch 23/294, Val RMSE: 0.6266, Time: 13.29s\n",
      "Trial 93, Epoch 24/294, Val RMSE: 0.6264, Time: 13.56s\n",
      "Trial 93, Epoch 25/294, Val RMSE: 0.6248, Time: 13.43s\n",
      "Trial 93, Epoch 26/294, Val RMSE: 0.6158, Time: 13.51s\n",
      "Trial 93, Epoch 27/294, Val RMSE: 0.6222, Time: 13.45s\n",
      "Trial 93, Epoch 28/294, Val RMSE: 0.6191, Time: 13.32s\n",
      "Trial 93, Epoch 29/294, Val RMSE: 0.6091, Time: 13.56s\n",
      "Trial 93, Epoch 30/294, Val RMSE: 0.6222, Time: 13.62s\n",
      "Trial 93, Epoch 31/294, Val RMSE: 0.6186, Time: 13.49s\n",
      "Trial 93, Epoch 32/294, Val RMSE: 0.6255, Time: 13.59s\n",
      "Trial 93, Epoch 33/294, Val RMSE: 0.6457, Time: 13.44s\n",
      "Trial 93, Epoch 34/294, Val RMSE: 0.6253, Time: 13.82s\n",
      "Trial 93, Epoch 35/294, Val RMSE: 0.6305, Time: 13.50s\n",
      "Trial 93, Epoch 36/294, Val RMSE: 0.6176, Time: 13.46s\n",
      "Trial 93, Epoch 37/294, Val RMSE: 0.6151, Time: 13.41s\n",
      "Trial 93, Epoch 38/294, Val RMSE: 0.6115, Time: 13.32s\n",
      "Trial 93, Epoch 39/294, Val RMSE: 0.6174, Time: 13.50s\n",
      "Trial 93, Epoch 40/294, Val RMSE: 0.6160, Time: 13.31s\n",
      "Trial 93, Epoch 41/294, Val RMSE: 0.6190, Time: 13.37s\n",
      "Trial 93, Epoch 42/294, Val RMSE: 0.6182, Time: 13.39s\n",
      "Trial 93, Epoch 43/294, Val RMSE: 0.6211, Time: 13.66s\n",
      "Trial 93, Epoch 44/294, Val RMSE: 0.6222, Time: 13.65s\n",
      "Trial 93, Epoch 45/294, Val RMSE: 0.6191, Time: 13.36s\n",
      "Trial 93, Epoch 46/294, Val RMSE: 0.6195, Time: 13.25s\n",
      "Trial 93, Epoch 47/294, Val RMSE: 0.6173, Time: 13.23s\n",
      "Trial 93, Epoch 48/294, Val RMSE: 0.6109, Time: 13.32s\n",
      "Trial 93, Epoch 49/294, Val RMSE: 0.6117, Time: 13.24s\n",
      "Trial 93, Epoch 50/294, Val RMSE: 0.6185, Time: 13.26s\n",
      "Trial 93, Epoch 51/294, Val RMSE: 0.6234, Time: 13.39s\n",
      "Trial 93, Epoch 52/294, Val RMSE: 0.6188, Time: 13.36s\n",
      "Trial 93, Epoch 53/294, Val RMSE: 0.6176, Time: 13.43s\n",
      "Trial 93, Epoch 54/294, Val RMSE: 0.6204, Time: 13.34s\n",
      "Trial 93, Epoch 55/294, Val RMSE: 0.6328, Time: 13.27s\n",
      "Trial 93, Epoch 56/294, Val RMSE: 0.6198, Time: 13.45s\n",
      "Trial 93, Epoch 57/294, Val RMSE: 0.6092, Time: 13.59s\n",
      "Trial 93, Epoch 58/294, Val RMSE: 0.6161, Time: 13.31s\n",
      "Trial 93, Epoch 59/294, Val RMSE: 0.6085, Time: 13.39s\n",
      "Trial 93, Epoch 60/294, Val RMSE: 0.6189, Time: 13.42s\n",
      "Trial 93, Epoch 61/294, Val RMSE: 0.6184, Time: 13.57s\n",
      "Trial 93, Epoch 62/294, Val RMSE: 0.6165, Time: 13.40s\n",
      "Trial 93, Epoch 63/294, Val RMSE: 0.6106, Time: 13.51s\n",
      "Trial 93, Epoch 64/294, Val RMSE: 0.6108, Time: 13.43s\n",
      "Trial 93, Epoch 65/294, Val RMSE: 0.6124, Time: 13.37s\n",
      "Trial 93, Epoch 66/294, Val RMSE: 0.6118, Time: 13.47s\n",
      "Trial 93, Epoch 67/294, Val RMSE: 0.6152, Time: 13.38s\n",
      "Trial 93, Epoch 68/294, Val RMSE: 0.6152, Time: 13.49s\n",
      "Trial 93, Epoch 69/294, Val RMSE: 0.6191, Time: 13.49s\n",
      "Trial 93, Epoch 70/294, Val RMSE: 0.6129, Time: 13.57s\n",
      "Trial 93, Epoch 71/294, Val RMSE: 0.6152, Time: 13.39s\n",
      "Trial 93, Epoch 72/294, Val RMSE: 0.6137, Time: 13.42s\n",
      "Trial 93, Epoch 73/294, Val RMSE: 0.6224, Time: 13.21s\n",
      "Trial 93, Epoch 74/294, Val RMSE: 0.6136, Time: 13.35s\n",
      "Trial 93, Epoch 75/294, Val RMSE: 0.6099, Time: 13.40s\n",
      "Trial 93, Epoch 76/294, Val RMSE: 0.6135, Time: 13.34s\n",
      "Trial 93, Epoch 77/294, Val RMSE: 0.6127, Time: 13.26s\n",
      "Trial 93, Epoch 78/294, Val RMSE: 0.6126, Time: 13.75s\n",
      "Trial 93, Epoch 79/294, Val RMSE: 0.6134, Time: 14.10s\n",
      "Trial 93, Epoch 80/294, Val RMSE: 0.6184, Time: 13.63s\n",
      "Trial 93, Epoch 81/294, Val RMSE: 0.6105, Time: 13.41s\n",
      "Trial 93, Epoch 82/294, Val RMSE: 0.6144, Time: 13.46s\n",
      "Trial 93, Epoch 83/294, Val RMSE: 0.6162, Time: 13.51s\n",
      "Trial 93, Epoch 84/294, Val RMSE: 0.6166, Time: 13.78s\n",
      "Trial 93, Epoch 85/294, Val RMSE: 0.6139, Time: 13.76s\n",
      "Trial 93, Epoch 86/294, Val RMSE: 0.6133, Time: 13.87s\n",
      "Trial 93, Epoch 87/294, Val RMSE: 0.6153, Time: 13.86s\n",
      "Trial 93, Epoch 88/294, Val RMSE: 0.6135, Time: 13.99s\n",
      "Trial 93, Epoch 89/294, Val RMSE: 0.6248, Time: 13.72s\n",
      "Trial 93, Epoch 90/294, Val RMSE: 0.6099, Time: 13.59s\n",
      "Trial 93, Epoch 91/294, Val RMSE: 0.6122, Time: 13.52s\n",
      "Trial 93, Epoch 92/294, Val RMSE: 0.6092, Time: 13.56s\n",
      "Trial 93, Epoch 93/294, Val RMSE: 0.6136, Time: 13.65s\n",
      "Trial 93, Epoch 94/294, Val RMSE: 0.6136, Time: 13.54s\n",
      "Trial 93, Epoch 95/294, Val RMSE: 0.6158, Time: 13.52s\n",
      "Trial 93, Epoch 96/294, Val RMSE: 0.6109, Time: 13.95s\n",
      "Trial 93, Epoch 97/294, Val RMSE: 0.6155, Time: 14.00s\n",
      "Trial 93, Epoch 98/294, Val RMSE: 0.6085, Time: 13.66s\n",
      "Trial 93, Epoch 99/294, Val RMSE: 0.6145, Time: 13.59s\n",
      "Trial 93, Epoch 100/294, Val RMSE: 0.6111, Time: 13.63s\n",
      "Trial 93, Epoch 101/294, Val RMSE: 0.6106, Time: 13.79s\n",
      "Trial 93, Epoch 102/294, Val RMSE: 0.6109, Time: 13.65s\n",
      "Trial 93, Epoch 103/294, Val RMSE: 0.6144, Time: 13.95s\n",
      "Trial 93, Epoch 104/294, Val RMSE: 0.6122, Time: 13.69s\n",
      "Trial 93, Epoch 105/294, Val RMSE: 0.6103, Time: 13.67s\n",
      "Trial 93, Epoch 106/294, Val RMSE: 0.6144, Time: 14.18s\n",
      "Trial 93, Epoch 107/294, Val RMSE: 0.6261, Time: 13.64s\n",
      "Trial 93, Epoch 108/294, Val RMSE: 0.6126, Time: 13.39s\n",
      "Trial 93, Epoch 109/294, Val RMSE: 0.6081, Time: 13.53s\n",
      "Trial 93, Epoch 110/294, Val RMSE: 0.6137, Time: 13.48s\n",
      "Trial 93, Epoch 111/294, Val RMSE: 0.6098, Time: 13.50s\n",
      "Trial 93, Epoch 112/294, Val RMSE: 0.6173, Time: 13.54s\n",
      "Trial 93, Epoch 113/294, Val RMSE: 0.6154, Time: 13.60s\n",
      "Trial 93, Epoch 114/294, Val RMSE: 0.6185, Time: 13.66s\n",
      "Trial 93, Epoch 115/294, Val RMSE: 0.6101, Time: 14.05s\n",
      "Trial 93, Epoch 116/294, Val RMSE: 0.6118, Time: 13.73s\n",
      "Trial 93, Epoch 117/294, Val RMSE: 0.6050, Time: 13.58s\n",
      "Trial 93, Epoch 118/294, Val RMSE: 0.6099, Time: 13.67s\n",
      "Trial 93, Epoch 119/294, Val RMSE: 0.6096, Time: 13.71s\n",
      "Trial 93, Epoch 120/294, Val RMSE: 0.6170, Time: 13.71s\n",
      "Trial 93, Epoch 121/294, Val RMSE: 0.6139, Time: 13.79s\n",
      "Trial 93, Epoch 122/294, Val RMSE: 0.6114, Time: 13.90s\n",
      "Trial 93, Epoch 123/294, Val RMSE: 0.6164, Time: 13.82s\n",
      "Trial 93, Epoch 124/294, Val RMSE: 0.6096, Time: 13.75s\n",
      "Trial 93, Epoch 125/294, Val RMSE: 0.6133, Time: 13.53s\n",
      "Trial 93, Epoch 126/294, Val RMSE: 0.6101, Time: 13.50s\n",
      "Trial 93, Epoch 127/294, Val RMSE: 0.6152, Time: 13.59s\n",
      "Trial 93, Epoch 128/294, Val RMSE: 0.6184, Time: 13.38s\n",
      "Trial 93, Epoch 129/294, Val RMSE: 0.6151, Time: 13.51s\n",
      "Trial 93, Epoch 130/294, Val RMSE: 0.6198, Time: 13.58s\n",
      "Trial 93, Epoch 131/294, Val RMSE: 0.6100, Time: 13.63s\n",
      "Trial 93, Epoch 132/294, Val RMSE: 0.6101, Time: 13.54s\n",
      "Trial 93, Epoch 133/294, Val RMSE: 0.6062, Time: 13.70s\n",
      "Trial 93, Epoch 134/294, Val RMSE: 0.6138, Time: 13.97s\n",
      "Trial 93, Epoch 135/294, Val RMSE: 0.6149, Time: 13.73s\n",
      "Trial 93, Epoch 136/294, Val RMSE: 0.6124, Time: 13.55s\n",
      "Trial 93, Epoch 137/294, Val RMSE: 0.6106, Time: 13.48s\n",
      "Trial 93, Epoch 138/294, Val RMSE: 0.6117, Time: 13.45s\n",
      "Trial 93, Epoch 139/294, Val RMSE: 0.6142, Time: 13.40s\n",
      "Trial 93, Epoch 140/294, Val RMSE: 0.6130, Time: 13.61s\n",
      "Trial 93, Epoch 141/294, Val RMSE: 0.6133, Time: 13.77s\n",
      "Trial 93, Epoch 142/294, Val RMSE: 0.6092, Time: 13.65s\n",
      "Trial 93, Epoch 143/294, Val RMSE: 0.6136, Time: 13.46s\n",
      "Trial 93, Epoch 144/294, Val RMSE: 0.6101, Time: 13.53s\n",
      "Trial 93, Epoch 145/294, Val RMSE: 0.6078, Time: 13.90s\n",
      "Trial 93, Epoch 146/294, Val RMSE: 0.6103, Time: 13.95s\n",
      "Trial 93, Epoch 147/294, Val RMSE: 0.6138, Time: 13.72s\n",
      "Trial 93, Epoch 148/294, Val RMSE: 0.6130, Time: 13.52s\n",
      "Trial 93, Epoch 149/294, Val RMSE: 0.6130, Time: 13.66s\n",
      "Trial 93, Epoch 150/294, Val RMSE: 0.6145, Time: 13.68s\n",
      "Trial 93, Epoch 151/294, Val RMSE: 0.6127, Time: 13.61s\n",
      "Trial 93, Epoch 152/294, Val RMSE: 0.6114, Time: 13.62s\n",
      "Trial 93, Epoch 153/294, Val RMSE: 0.6135, Time: 13.64s\n",
      "Trial 93, Epoch 154/294, Val RMSE: 0.6116, Time: 13.72s\n",
      "Trial 93, Epoch 155/294, Val RMSE: 0.6149, Time: 13.59s\n",
      "Trial 93, Epoch 156/294, Val RMSE: 0.6074, Time: 13.72s\n",
      "Trial 93, Epoch 157/294, Val RMSE: 0.6085, Time: 13.65s\n",
      "Trial 93, Epoch 158/294, Val RMSE: 0.6165, Time: 13.65s\n",
      "Trial 93, Epoch 159/294, Val RMSE: 0.6150, Time: 13.60s\n",
      "Trial 93, Epoch 160/294, Val RMSE: 0.6123, Time: 13.43s\n",
      "Trial 93, Epoch 161/294, Val RMSE: 0.6122, Time: 13.52s\n",
      "Trial 93, Epoch 162/294, Val RMSE: 0.6105, Time: 13.49s\n",
      "Trial 93, Epoch 163/294, Val RMSE: 0.6088, Time: 13.93s\n",
      "Trial 93, Epoch 164/294, Val RMSE: 0.6120, Time: 13.52s\n",
      "Trial 93, Epoch 165/294, Val RMSE: 0.6135, Time: 13.55s\n",
      "Trial 93, Epoch 166/294, Val RMSE: 0.6141, Time: 13.81s\n",
      "Trial 93, Epoch 167/294, Val RMSE: 0.6104, Time: 13.60s\n",
      "Early stopping at epoch 167 for trial 93\n",
      "[I 2025-07-17 13:52:12,459] Trial 93 finished with value: 0.6104138010305505 and parameters: {'hidden_channels': 726, 'lr': 0.0002538047237606104, 'batch_size': 32, 'n_epochs': 294, 'num_layers': 3, 'dropout_rate': 0.18240873682082875, 'weight_decay': 1.8941492395765198e-08}. Best is trial 60 with value: 0.609277393700007.\n",
      "Trial 94, Epoch 1/329, Val RMSE: 0.8256, Time: 13.72s\n",
      "Trial 94, Epoch 2/329, Val RMSE: 0.7344, Time: 13.72s\n",
      "Trial 94, Epoch 3/329, Val RMSE: 0.6933, Time: 13.61s\n",
      "Trial 94, Epoch 4/329, Val RMSE: 0.6709, Time: 13.75s\n",
      "Trial 94, Epoch 5/329, Val RMSE: 0.6658, Time: 13.67s\n",
      "Trial 94, Epoch 6/329, Val RMSE: 0.6545, Time: 13.67s\n",
      "Trial 94, Epoch 7/329, Val RMSE: 0.6400, Time: 13.70s\n",
      "Trial 94, Epoch 8/329, Val RMSE: 0.6294, Time: 13.80s\n",
      "Trial 94, Epoch 9/329, Val RMSE: 0.6275, Time: 13.80s\n",
      "Trial 94, Epoch 10/329, Val RMSE: 0.6304, Time: 14.12s\n",
      "Trial 94, Epoch 11/329, Val RMSE: 0.6398, Time: 13.87s\n",
      "Trial 94, Epoch 12/329, Val RMSE: 0.6504, Time: 13.57s\n",
      "Trial 94, Epoch 13/329, Val RMSE: 0.6203, Time: 13.64s\n",
      "Trial 94, Epoch 14/329, Val RMSE: 0.6532, Time: 13.53s\n",
      "Trial 94, Epoch 15/329, Val RMSE: 0.6276, Time: 13.66s\n",
      "Trial 94, Epoch 16/329, Val RMSE: 0.6323, Time: 13.64s\n",
      "Trial 94, Epoch 17/329, Val RMSE: 0.6349, Time: 14.08s\n",
      "Trial 94, Epoch 18/329, Val RMSE: 0.6273, Time: 13.66s\n",
      "Trial 94, Epoch 19/329, Val RMSE: 0.6261, Time: 13.75s\n",
      "Trial 94, Epoch 20/329, Val RMSE: 0.6171, Time: 13.92s\n",
      "Trial 94, Epoch 21/329, Val RMSE: 0.6338, Time: 13.79s\n",
      "Trial 94, Epoch 22/329, Val RMSE: 0.6345, Time: 13.58s\n",
      "Trial 94, Epoch 23/329, Val RMSE: 0.6301, Time: 13.92s\n",
      "Trial 94, Epoch 24/329, Val RMSE: 0.6219, Time: 13.78s\n",
      "Trial 94, Epoch 25/329, Val RMSE: 0.6304, Time: 13.64s\n",
      "Trial 94, Epoch 26/329, Val RMSE: 0.6267, Time: 13.94s\n",
      "Trial 94, Epoch 27/329, Val RMSE: 0.6194, Time: 13.78s\n",
      "Trial 94, Epoch 28/329, Val RMSE: 0.6169, Time: 13.54s\n",
      "Trial 94, Epoch 29/329, Val RMSE: 0.6288, Time: 13.59s\n",
      "Trial 94, Epoch 30/329, Val RMSE: 0.6272, Time: 13.62s\n",
      "Trial 94, Epoch 31/329, Val RMSE: 0.6222, Time: 13.63s\n",
      "Trial 94, Epoch 32/329, Val RMSE: 0.6298, Time: 13.58s\n",
      "Trial 94, Epoch 33/329, Val RMSE: 0.6169, Time: 13.69s\n",
      "Trial 94, Epoch 34/329, Val RMSE: 0.6139, Time: 13.92s\n",
      "Trial 94, Epoch 35/329, Val RMSE: 0.6321, Time: 14.04s\n",
      "Trial 94, Epoch 36/329, Val RMSE: 0.6201, Time: 13.73s\n",
      "Trial 94, Epoch 37/329, Val RMSE: 0.6209, Time: 13.72s\n",
      "Trial 94, Epoch 38/329, Val RMSE: 0.6157, Time: 13.78s\n",
      "Trial 94, Epoch 39/329, Val RMSE: 0.6283, Time: 13.76s\n",
      "Trial 94, Epoch 40/329, Val RMSE: 0.6252, Time: 13.63s\n",
      "Trial 94, Epoch 41/329, Val RMSE: 0.6211, Time: 13.78s\n",
      "Trial 94, Epoch 42/329, Val RMSE: 0.6131, Time: 13.69s\n",
      "Trial 94, Epoch 43/329, Val RMSE: 0.6212, Time: 13.86s\n",
      "Trial 94, Epoch 44/329, Val RMSE: 0.6207, Time: 13.92s\n",
      "Trial 94, Epoch 45/329, Val RMSE: 0.6203, Time: 13.76s\n",
      "Trial 94, Epoch 46/329, Val RMSE: 0.6307, Time: 13.59s\n",
      "Trial 94, Epoch 47/329, Val RMSE: 0.6212, Time: 13.43s\n",
      "Trial 94, Epoch 48/329, Val RMSE: 0.6162, Time: 13.64s\n",
      "Trial 94, Epoch 49/329, Val RMSE: 0.6160, Time: 13.49s\n",
      "Trial 94, Epoch 50/329, Val RMSE: 0.6169, Time: 13.79s\n",
      "Trial 94, Epoch 51/329, Val RMSE: 0.6155, Time: 13.52s\n",
      "Trial 94, Epoch 52/329, Val RMSE: 0.6171, Time: 13.57s\n",
      "Trial 94, Epoch 53/329, Val RMSE: 0.6180, Time: 13.57s\n",
      "Trial 94, Epoch 54/329, Val RMSE: 0.6170, Time: 13.64s\n",
      "Trial 94, Epoch 55/329, Val RMSE: 0.6306, Time: 13.37s\n",
      "Trial 94, Epoch 56/329, Val RMSE: 0.6186, Time: 13.63s\n",
      "Trial 94, Epoch 57/329, Val RMSE: 0.6217, Time: 13.74s\n",
      "Trial 94, Epoch 58/329, Val RMSE: 0.6152, Time: 13.74s\n",
      "Trial 94, Epoch 59/329, Val RMSE: 0.6225, Time: 13.74s\n",
      "Trial 94, Epoch 60/329, Val RMSE: 0.6234, Time: 13.68s\n",
      "Trial 94, Epoch 61/329, Val RMSE: 0.6175, Time: 13.72s\n",
      "Trial 94, Epoch 62/329, Val RMSE: 0.6209, Time: 13.60s\n",
      "Trial 94, Epoch 63/329, Val RMSE: 0.6227, Time: 13.59s\n",
      "Trial 94, Epoch 64/329, Val RMSE: 0.6161, Time: 13.58s\n",
      "Trial 94, Epoch 65/329, Val RMSE: 0.6214, Time: 13.70s\n",
      "Trial 94, Epoch 66/329, Val RMSE: 0.6198, Time: 14.12s\n",
      "Trial 94, Epoch 67/329, Val RMSE: 0.6299, Time: 13.74s\n",
      "Trial 94, Epoch 68/329, Val RMSE: 0.6113, Time: 13.66s\n",
      "Trial 94, Epoch 69/329, Val RMSE: 0.6171, Time: 13.59s\n",
      "Trial 94, Epoch 70/329, Val RMSE: 0.6176, Time: 13.83s\n",
      "Trial 94, Epoch 71/329, Val RMSE: 0.6167, Time: 14.04s\n",
      "Trial 94, Epoch 72/329, Val RMSE: 0.6218, Time: 13.77s\n",
      "Trial 94, Epoch 73/329, Val RMSE: 0.6191, Time: 13.88s\n",
      "Trial 94, Epoch 74/329, Val RMSE: 0.6277, Time: 13.71s\n",
      "Trial 94, Epoch 75/329, Val RMSE: 0.6188, Time: 13.70s\n",
      "Trial 94, Epoch 76/329, Val RMSE: 0.6143, Time: 13.65s\n",
      "Trial 94, Epoch 77/329, Val RMSE: 0.6241, Time: 13.60s\n",
      "Trial 94, Epoch 78/329, Val RMSE: 0.6141, Time: 13.72s\n",
      "Trial 94, Epoch 79/329, Val RMSE: 0.6196, Time: 13.65s\n",
      "Trial 94, Epoch 80/329, Val RMSE: 0.6168, Time: 14.00s\n",
      "Trial 94, Epoch 81/329, Val RMSE: 0.6151, Time: 13.94s\n",
      "Trial 94, Epoch 82/329, Val RMSE: 0.6175, Time: 13.80s\n",
      "Trial 94, Epoch 83/329, Val RMSE: 0.6161, Time: 14.26s\n",
      "Trial 94, Epoch 84/329, Val RMSE: 0.6172, Time: 13.65s\n",
      "Trial 94, Epoch 85/329, Val RMSE: 0.6164, Time: 13.67s\n",
      "Trial 94, Epoch 86/329, Val RMSE: 0.6186, Time: 13.58s\n",
      "Trial 94, Epoch 87/329, Val RMSE: 0.6148, Time: 13.67s\n",
      "Trial 94, Epoch 88/329, Val RMSE: 0.6131, Time: 13.51s\n",
      "Trial 94, Epoch 89/329, Val RMSE: 0.6160, Time: 13.67s\n",
      "Trial 94, Epoch 90/329, Val RMSE: 0.6146, Time: 13.58s\n",
      "Trial 94, Epoch 91/329, Val RMSE: 0.6194, Time: 13.73s\n",
      "Trial 94, Epoch 92/329, Val RMSE: 0.6099, Time: 13.63s\n",
      "Trial 94, Epoch 93/329, Val RMSE: 0.6186, Time: 13.61s\n",
      "Trial 94, Epoch 94/329, Val RMSE: 0.6175, Time: 13.63s\n",
      "Trial 94, Epoch 95/329, Val RMSE: 0.6167, Time: 13.60s\n",
      "Trial 94, Epoch 96/329, Val RMSE: 0.6146, Time: 13.64s\n",
      "Trial 94, Epoch 97/329, Val RMSE: 0.6268, Time: 13.74s\n",
      "Trial 94, Epoch 98/329, Val RMSE: 0.6159, Time: 13.54s\n",
      "Trial 94, Epoch 99/329, Val RMSE: 0.6214, Time: 13.29s\n",
      "Trial 94, Epoch 100/329, Val RMSE: 0.6213, Time: 13.45s\n",
      "Trial 94, Epoch 101/329, Val RMSE: 0.6104, Time: 13.43s\n",
      "Trial 94, Epoch 102/329, Val RMSE: 0.6176, Time: 13.58s\n",
      "Trial 94, Epoch 103/329, Val RMSE: 0.6186, Time: 13.71s\n",
      "Trial 94, Epoch 104/329, Val RMSE: 0.6166, Time: 13.83s\n",
      "Trial 94, Epoch 105/329, Val RMSE: 0.6097, Time: 13.84s\n",
      "Trial 94, Epoch 106/329, Val RMSE: 0.6185, Time: 13.49s\n",
      "Trial 94, Epoch 107/329, Val RMSE: 0.6132, Time: 13.38s\n",
      "Trial 94, Epoch 108/329, Val RMSE: 0.6240, Time: 13.44s\n",
      "Trial 94, Epoch 109/329, Val RMSE: 0.6161, Time: 13.58s\n",
      "Trial 94, Epoch 110/329, Val RMSE: 0.6214, Time: 13.44s\n",
      "Trial 94, Epoch 111/329, Val RMSE: 0.6160, Time: 13.59s\n",
      "Trial 94, Epoch 112/329, Val RMSE: 0.6176, Time: 13.68s\n",
      "Trial 94, Epoch 113/329, Val RMSE: 0.6206, Time: 13.64s\n",
      "Trial 94, Epoch 114/329, Val RMSE: 0.6182, Time: 13.80s\n",
      "Trial 94, Epoch 115/329, Val RMSE: 0.6187, Time: 13.70s\n",
      "Trial 94, Epoch 116/329, Val RMSE: 0.6203, Time: 13.64s\n",
      "Trial 94, Epoch 117/329, Val RMSE: 0.6172, Time: 13.53s\n",
      "Trial 94, Epoch 118/329, Val RMSE: 0.6147, Time: 13.51s\n",
      "Trial 94, Epoch 119/329, Val RMSE: 0.6156, Time: 13.58s\n",
      "Trial 94, Epoch 120/329, Val RMSE: 0.6208, Time: 13.62s\n",
      "Trial 94, Epoch 121/329, Val RMSE: 0.6172, Time: 13.66s\n",
      "Trial 94, Epoch 122/329, Val RMSE: 0.6202, Time: 13.64s\n",
      "Trial 94, Epoch 123/329, Val RMSE: 0.6196, Time: 13.62s\n",
      "Trial 94, Epoch 124/329, Val RMSE: 0.6191, Time: 13.58s\n",
      "Trial 94, Epoch 125/329, Val RMSE: 0.6093, Time: 13.49s\n",
      "Trial 94, Epoch 126/329, Val RMSE: 0.6172, Time: 13.36s\n",
      "Trial 94, Epoch 127/329, Val RMSE: 0.6156, Time: 13.60s\n",
      "Trial 94, Epoch 128/329, Val RMSE: 0.6154, Time: 13.40s\n",
      "Trial 94, Epoch 129/329, Val RMSE: 0.6133, Time: 13.57s\n",
      "Trial 94, Epoch 130/329, Val RMSE: 0.6170, Time: 13.56s\n",
      "Trial 94, Epoch 131/329, Val RMSE: 0.6180, Time: 13.70s\n",
      "Trial 94, Epoch 132/329, Val RMSE: 0.6146, Time: 13.67s\n",
      "Trial 94, Epoch 133/329, Val RMSE: 0.6217, Time: 13.58s\n",
      "Trial 94, Epoch 134/329, Val RMSE: 0.6184, Time: 13.24s\n",
      "Trial 94, Epoch 135/329, Val RMSE: 0.6158, Time: 13.29s\n",
      "Trial 94, Epoch 136/329, Val RMSE: 0.6154, Time: 13.37s\n",
      "Trial 94, Epoch 137/329, Val RMSE: 0.6184, Time: 13.38s\n",
      "Trial 94, Epoch 138/329, Val RMSE: 0.6171, Time: 13.42s\n",
      "Trial 94, Epoch 139/329, Val RMSE: 0.6144, Time: 13.97s\n",
      "Trial 94, Epoch 140/329, Val RMSE: 0.6204, Time: 13.55s\n",
      "Trial 94, Epoch 141/329, Val RMSE: 0.6144, Time: 13.57s\n",
      "Trial 94, Epoch 142/329, Val RMSE: 0.6168, Time: 13.46s\n",
      "Trial 94, Epoch 143/329, Val RMSE: 0.6149, Time: 13.44s\n",
      "Trial 94, Epoch 144/329, Val RMSE: 0.6190, Time: 13.72s\n",
      "Trial 94, Epoch 145/329, Val RMSE: 0.6171, Time: 13.56s\n",
      "Trial 94, Epoch 146/329, Val RMSE: 0.6161, Time: 13.57s\n",
      "Trial 94, Epoch 147/329, Val RMSE: 0.6150, Time: 13.60s\n",
      "Trial 94, Epoch 148/329, Val RMSE: 0.6165, Time: 13.74s\n",
      "Trial 94, Epoch 149/329, Val RMSE: 0.6155, Time: 13.62s\n",
      "Trial 94, Epoch 150/329, Val RMSE: 0.6183, Time: 13.75s\n",
      "Trial 94, Epoch 151/329, Val RMSE: 0.6160, Time: 13.53s\n",
      "Trial 94, Epoch 152/329, Val RMSE: 0.6130, Time: 13.45s\n",
      "Trial 94, Epoch 153/329, Val RMSE: 0.6148, Time: 16.58s\n",
      "Trial 94, Epoch 154/329, Val RMSE: 0.6193, Time: 18.88s\n",
      "Trial 94, Epoch 155/329, Val RMSE: 0.6168, Time: 17.13s\n",
      "Trial 94, Epoch 156/329, Val RMSE: 0.6183, Time: 13.52s\n",
      "Trial 94, Epoch 157/329, Val RMSE: 0.6180, Time: 13.45s\n",
      "Trial 94, Epoch 158/329, Val RMSE: 0.6151, Time: 13.52s\n",
      "Trial 94, Epoch 159/329, Val RMSE: 0.6175, Time: 13.36s\n",
      "Trial 94, Epoch 160/329, Val RMSE: 0.6155, Time: 13.51s\n",
      "Trial 94, Epoch 161/329, Val RMSE: 0.6184, Time: 13.50s\n",
      "Trial 94, Epoch 162/329, Val RMSE: 0.6189, Time: 13.49s\n",
      "Trial 94, Epoch 163/329, Val RMSE: 0.6163, Time: 13.50s\n",
      "Trial 94, Epoch 164/329, Val RMSE: 0.6215, Time: 13.55s\n",
      "Trial 94, Epoch 165/329, Val RMSE: 0.6165, Time: 13.55s\n",
      "Trial 94, Epoch 166/329, Val RMSE: 0.6178, Time: 13.50s\n",
      "Trial 94, Epoch 167/329, Val RMSE: 0.6114, Time: 13.65s\n",
      "Trial 94, Epoch 168/329, Val RMSE: 0.6150, Time: 13.70s\n",
      "Trial 94, Epoch 169/329, Val RMSE: 0.6130, Time: 13.63s\n",
      "Trial 94, Epoch 170/329, Val RMSE: 0.6129, Time: 13.65s\n",
      "Trial 94, Epoch 171/329, Val RMSE: 0.6169, Time: 13.73s\n",
      "Trial 94, Epoch 172/329, Val RMSE: 0.6172, Time: 13.48s\n",
      "Trial 94, Epoch 173/329, Val RMSE: 0.6130, Time: 13.58s\n",
      "Trial 94, Epoch 174/329, Val RMSE: 0.6206, Time: 13.84s\n",
      "[I 2025-07-17 14:32:03,235] Trial 94 pruned. \n",
      "Trial 95, Epoch 1/285, Val RMSE: 0.7806, Time: 13.99s\n",
      "Trial 95, Epoch 2/285, Val RMSE: 0.7338, Time: 13.81s\n",
      "Trial 95, Epoch 3/285, Val RMSE: 0.7093, Time: 13.90s\n",
      "Trial 95, Epoch 4/285, Val RMSE: 0.6673, Time: 13.84s\n",
      "Trial 95, Epoch 5/285, Val RMSE: 0.6630, Time: 13.75s\n",
      "Trial 95, Epoch 6/285, Val RMSE: 0.6623, Time: 13.80s\n",
      "Trial 95, Epoch 7/285, Val RMSE: 0.6700, Time: 13.82s\n",
      "Trial 95, Epoch 8/285, Val RMSE: 0.6489, Time: 13.79s\n",
      "Trial 95, Epoch 9/285, Val RMSE: 0.6457, Time: 13.66s\n",
      "Trial 95, Epoch 10/285, Val RMSE: 0.6403, Time: 13.86s\n",
      "Trial 95, Epoch 11/285, Val RMSE: 0.6386, Time: 13.73s\n",
      "Trial 95, Epoch 12/285, Val RMSE: 0.6339, Time: 13.63s\n",
      "Trial 95, Epoch 13/285, Val RMSE: 0.6384, Time: 13.61s\n",
      "Trial 95, Epoch 14/285, Val RMSE: 0.6418, Time: 13.77s\n",
      "Trial 95, Epoch 15/285, Val RMSE: 0.6244, Time: 13.70s\n",
      "Trial 95, Epoch 16/285, Val RMSE: 0.6484, Time: 13.84s\n",
      "Trial 95, Epoch 17/285, Val RMSE: 0.6245, Time: 13.79s\n",
      "Trial 95, Epoch 18/285, Val RMSE: 0.6353, Time: 13.90s\n",
      "Trial 95, Epoch 19/285, Val RMSE: 0.6440, Time: 13.91s\n",
      "Trial 95, Epoch 20/285, Val RMSE: 0.6251, Time: 13.78s\n",
      "Trial 95, Epoch 21/285, Val RMSE: 0.6300, Time: 13.80s\n",
      "Trial 95, Epoch 22/285, Val RMSE: 0.6238, Time: 14.29s\n",
      "Trial 95, Epoch 23/285, Val RMSE: 0.6226, Time: 13.81s\n",
      "Trial 95, Epoch 24/285, Val RMSE: 0.6251, Time: 13.81s\n",
      "Trial 95, Epoch 25/285, Val RMSE: 0.6206, Time: 13.96s\n",
      "Trial 95, Epoch 26/285, Val RMSE: 0.6233, Time: 13.96s\n",
      "Trial 95, Epoch 27/285, Val RMSE: 0.6213, Time: 13.91s\n",
      "Trial 95, Epoch 28/285, Val RMSE: 0.6200, Time: 13.79s\n",
      "Trial 95, Epoch 29/285, Val RMSE: 0.6132, Time: 13.76s\n",
      "Trial 95, Epoch 30/285, Val RMSE: 0.6231, Time: 13.81s\n",
      "Trial 95, Epoch 31/285, Val RMSE: 0.6361, Time: 13.61s\n",
      "Trial 95, Epoch 32/285, Val RMSE: 0.6154, Time: 13.83s\n",
      "Trial 95, Epoch 33/285, Val RMSE: 0.6261, Time: 13.77s\n",
      "Trial 95, Epoch 34/285, Val RMSE: 0.6170, Time: 13.77s\n",
      "Trial 95, Epoch 35/285, Val RMSE: 0.6205, Time: 13.74s\n",
      "Trial 95, Epoch 36/285, Val RMSE: 0.6100, Time: 13.69s\n",
      "Trial 95, Epoch 37/285, Val RMSE: 0.6260, Time: 13.79s\n",
      "Trial 95, Epoch 38/285, Val RMSE: 0.6209, Time: 13.69s\n",
      "Trial 95, Epoch 39/285, Val RMSE: 0.6181, Time: 13.91s\n",
      "Trial 95, Epoch 40/285, Val RMSE: 0.6250, Time: 13.74s\n",
      "Trial 95, Epoch 41/285, Val RMSE: 0.6134, Time: 13.75s\n",
      "Trial 95, Epoch 42/285, Val RMSE: 0.6111, Time: 13.74s\n",
      "Trial 95, Epoch 43/285, Val RMSE: 0.6104, Time: 14.13s\n",
      "Trial 95, Epoch 44/285, Val RMSE: 0.6177, Time: 13.82s\n",
      "Trial 95, Epoch 45/285, Val RMSE: 0.6082, Time: 13.82s\n",
      "Trial 95, Epoch 46/285, Val RMSE: 0.6201, Time: 13.83s\n",
      "Trial 95, Epoch 47/285, Val RMSE: 0.6132, Time: 13.77s\n",
      "Trial 95, Epoch 48/285, Val RMSE: 0.6209, Time: 13.96s\n",
      "Trial 95, Epoch 49/285, Val RMSE: 0.6149, Time: 13.82s\n",
      "Trial 95, Epoch 50/285, Val RMSE: 0.6080, Time: 13.85s\n",
      "Trial 95, Epoch 51/285, Val RMSE: 0.6117, Time: 13.76s\n",
      "Trial 95, Epoch 52/285, Val RMSE: 0.6093, Time: 13.97s\n",
      "Trial 95, Epoch 53/285, Val RMSE: 0.6158, Time: 13.94s\n",
      "Trial 95, Epoch 54/285, Val RMSE: 0.6152, Time: 13.76s\n",
      "Trial 95, Epoch 55/285, Val RMSE: 0.6412, Time: 13.76s\n",
      "Trial 95, Epoch 56/285, Val RMSE: 0.6133, Time: 13.80s\n",
      "Trial 95, Epoch 57/285, Val RMSE: 0.6183, Time: 13.90s\n",
      "Trial 95, Epoch 58/285, Val RMSE: 0.6130, Time: 13.83s\n",
      "Trial 95, Epoch 59/285, Val RMSE: 0.6127, Time: 13.85s\n",
      "Trial 95, Epoch 60/285, Val RMSE: 0.6204, Time: 13.88s\n",
      "Trial 95, Epoch 61/285, Val RMSE: 0.6167, Time: 13.90s\n",
      "Trial 95, Epoch 62/285, Val RMSE: 0.6228, Time: 13.93s\n",
      "Trial 95, Epoch 63/285, Val RMSE: 0.6307, Time: 13.86s\n",
      "Trial 95, Epoch 64/285, Val RMSE: 0.6131, Time: 13.77s\n",
      "Trial 95, Epoch 65/285, Val RMSE: 0.6182, Time: 13.97s\n",
      "Trial 95, Epoch 66/285, Val RMSE: 0.6170, Time: 13.66s\n",
      "Trial 95, Epoch 67/285, Val RMSE: 0.6224, Time: 13.63s\n",
      "Trial 95, Epoch 68/285, Val RMSE: 0.6353, Time: 13.86s\n",
      "Trial 95, Epoch 69/285, Val RMSE: 0.6150, Time: 13.80s\n",
      "Trial 95, Epoch 70/285, Val RMSE: 0.6106, Time: 13.79s\n",
      "Trial 95, Epoch 71/285, Val RMSE: 0.6110, Time: 13.69s\n",
      "Trial 95, Epoch 72/285, Val RMSE: 0.6140, Time: 13.89s\n",
      "Trial 95, Epoch 73/285, Val RMSE: 0.6160, Time: 13.65s\n",
      "Trial 95, Epoch 74/285, Val RMSE: 0.6180, Time: 13.78s\n",
      "Trial 95, Epoch 75/285, Val RMSE: 0.6179, Time: 13.80s\n",
      "Trial 95, Epoch 76/285, Val RMSE: 0.6141, Time: 13.78s\n",
      "Trial 95, Epoch 77/285, Val RMSE: 0.6149, Time: 13.82s\n",
      "Trial 95, Epoch 78/285, Val RMSE: 0.6111, Time: 13.88s\n",
      "Trial 95, Epoch 79/285, Val RMSE: 0.6131, Time: 13.87s\n",
      "Trial 95, Epoch 80/285, Val RMSE: 0.6116, Time: 13.86s\n",
      "Trial 95, Epoch 81/285, Val RMSE: 0.6168, Time: 13.77s\n",
      "Trial 95, Epoch 82/285, Val RMSE: 0.6179, Time: 13.80s\n",
      "Trial 95, Epoch 83/285, Val RMSE: 0.6125, Time: 13.74s\n",
      "Trial 95, Epoch 84/285, Val RMSE: 0.6189, Time: 13.87s\n",
      "Trial 95, Epoch 85/285, Val RMSE: 0.6269, Time: 13.92s\n",
      "Trial 95, Epoch 86/285, Val RMSE: 0.6193, Time: 13.85s\n",
      "Trial 95, Epoch 87/285, Val RMSE: 0.6128, Time: 14.00s\n",
      "Trial 95, Epoch 88/285, Val RMSE: 0.6115, Time: 13.89s\n",
      "Trial 95, Epoch 89/285, Val RMSE: 0.6121, Time: 13.79s\n",
      "Trial 95, Epoch 90/285, Val RMSE: 0.6153, Time: 13.89s\n",
      "Trial 95, Epoch 91/285, Val RMSE: 0.6227, Time: 13.81s\n",
      "Trial 95, Epoch 92/285, Val RMSE: 0.6199, Time: 13.83s\n",
      "Trial 95, Epoch 93/285, Val RMSE: 0.6144, Time: 13.69s\n",
      "Trial 95, Epoch 94/285, Val RMSE: 0.6299, Time: 13.76s\n",
      "Trial 95, Epoch 95/285, Val RMSE: 0.6285, Time: 14.94s\n",
      "Trial 95, Epoch 96/285, Val RMSE: 0.6130, Time: 13.83s\n",
      "Trial 95, Epoch 97/285, Val RMSE: 0.6266, Time: 13.84s\n",
      "Trial 95, Epoch 98/285, Val RMSE: 0.6128, Time: 13.68s\n",
      "Trial 95, Epoch 99/285, Val RMSE: 0.6108, Time: 13.58s\n",
      "Trial 95, Epoch 100/285, Val RMSE: 0.6185, Time: 13.63s\n",
      "Early stopping at epoch 100 for trial 95\n",
      "[I 2025-07-17 14:55:09,253] Trial 95 finished with value: 0.6185222529794095 and parameters: {'hidden_channels': 832, 'lr': 0.0003166999843492428, 'batch_size': 32, 'n_epochs': 285, 'num_layers': 3, 'dropout_rate': 0.18206773164192533, 'weight_decay': 1.809735564286026e-08}. Best is trial 60 with value: 0.609277393700007.\n",
      "Trial 96, Epoch 1/291, Val RMSE: 0.8082, Time: 13.71s\n",
      "Trial 96, Epoch 2/291, Val RMSE: 0.7620, Time: 13.66s\n",
      "Trial 96, Epoch 3/291, Val RMSE: 0.6951, Time: 13.81s\n",
      "Trial 96, Epoch 4/291, Val RMSE: 0.7040, Time: 14.00s\n",
      "Trial 96, Epoch 5/291, Val RMSE: 0.6711, Time: 14.07s\n",
      "Trial 96, Epoch 6/291, Val RMSE: 0.6506, Time: 13.77s\n",
      "Trial 96, Epoch 7/291, Val RMSE: 0.6856, Time: 13.70s\n",
      "Trial 96, Epoch 8/291, Val RMSE: 0.6401, Time: 13.99s\n",
      "Trial 96, Epoch 9/291, Val RMSE: 0.6428, Time: 13.78s\n",
      "Trial 96, Epoch 10/291, Val RMSE: 0.6431, Time: 13.65s\n",
      "Trial 96, Epoch 11/291, Val RMSE: 0.6422, Time: 13.76s\n",
      "Trial 96, Epoch 12/291, Val RMSE: 0.6332, Time: 13.97s\n",
      "Trial 96, Epoch 13/291, Val RMSE: 0.6400, Time: 13.80s\n",
      "Trial 96, Epoch 14/291, Val RMSE: 0.6365, Time: 13.77s\n",
      "Trial 96, Epoch 15/291, Val RMSE: 0.6468, Time: 13.73s\n",
      "Trial 96, Epoch 16/291, Val RMSE: 0.6346, Time: 13.78s\n",
      "Trial 96, Epoch 17/291, Val RMSE: 0.6600, Time: 13.59s\n",
      "Trial 96, Epoch 18/291, Val RMSE: 0.6260, Time: 13.62s\n",
      "Trial 96, Epoch 19/291, Val RMSE: 0.6559, Time: 13.54s\n",
      "Trial 96, Epoch 20/291, Val RMSE: 0.6518, Time: 13.66s\n",
      "Trial 96, Epoch 21/291, Val RMSE: 0.6428, Time: 13.58s\n",
      "Trial 96, Epoch 22/291, Val RMSE: 0.6239, Time: 13.67s\n",
      "Trial 96, Epoch 23/291, Val RMSE: 0.6190, Time: 13.68s\n",
      "Trial 96, Epoch 24/291, Val RMSE: 0.6442, Time: 13.67s\n",
      "Trial 96, Epoch 25/291, Val RMSE: 0.6329, Time: 13.54s\n",
      "Trial 96, Epoch 26/291, Val RMSE: 0.6775, Time: 13.68s\n",
      "Trial 96, Epoch 27/291, Val RMSE: 0.6255, Time: 13.74s\n",
      "Trial 96, Epoch 28/291, Val RMSE: 0.6216, Time: 13.88s\n",
      "Trial 96, Epoch 29/291, Val RMSE: 0.6406, Time: 13.65s\n",
      "Trial 96, Epoch 30/291, Val RMSE: 0.6240, Time: 14.00s\n",
      "Trial 96, Epoch 31/291, Val RMSE: 0.6281, Time: 13.68s\n",
      "Trial 96, Epoch 32/291, Val RMSE: 0.6405, Time: 13.83s\n",
      "Trial 96, Epoch 33/291, Val RMSE: 0.6246, Time: 13.66s\n",
      "Trial 96, Epoch 34/291, Val RMSE: 0.6587, Time: 13.82s\n",
      "Trial 96, Epoch 35/291, Val RMSE: 0.6304, Time: 13.78s\n",
      "Trial 96, Epoch 36/291, Val RMSE: 0.6267, Time: 13.73s\n",
      "Trial 96, Epoch 37/291, Val RMSE: 0.6215, Time: 13.74s\n",
      "Trial 96, Epoch 38/291, Val RMSE: 0.6427, Time: 13.75s\n",
      "Trial 96, Epoch 39/291, Val RMSE: 0.6246, Time: 13.72s\n",
      "Trial 96, Epoch 40/291, Val RMSE: 0.6207, Time: 13.84s\n",
      "Trial 96, Epoch 41/291, Val RMSE: 0.6281, Time: 13.88s\n",
      "Trial 96, Epoch 42/291, Val RMSE: 0.6209, Time: 13.76s\n",
      "Trial 96, Epoch 43/291, Val RMSE: 0.6351, Time: 13.78s\n",
      "Trial 96, Epoch 44/291, Val RMSE: 0.6228, Time: 13.80s\n",
      "Trial 96, Epoch 45/291, Val RMSE: 0.6238, Time: 14.34s\n",
      "Trial 96, Epoch 46/291, Val RMSE: 0.6305, Time: 13.84s\n",
      "Trial 96, Epoch 47/291, Val RMSE: 0.6217, Time: 13.84s\n",
      "Trial 96, Epoch 48/291, Val RMSE: 0.6248, Time: 13.90s\n",
      "Trial 96, Epoch 49/291, Val RMSE: 0.6263, Time: 13.78s\n",
      "Trial 96, Epoch 50/291, Val RMSE: 0.6227, Time: 13.63s\n",
      "Trial 96, Epoch 51/291, Val RMSE: 0.6172, Time: 13.55s\n",
      "Trial 96, Epoch 52/291, Val RMSE: 0.6156, Time: 13.67s\n",
      "Trial 96, Epoch 53/291, Val RMSE: 0.6233, Time: 13.79s\n",
      "Trial 96, Epoch 54/291, Val RMSE: 0.6248, Time: 13.65s\n",
      "Trial 96, Epoch 55/291, Val RMSE: 0.6194, Time: 13.70s\n",
      "Trial 96, Epoch 56/291, Val RMSE: 0.6243, Time: 13.81s\n",
      "Trial 96, Epoch 57/291, Val RMSE: 0.6146, Time: 13.82s\n",
      "Trial 96, Epoch 58/291, Val RMSE: 0.6246, Time: 13.81s\n",
      "Trial 96, Epoch 59/291, Val RMSE: 0.6172, Time: 13.75s\n",
      "Trial 96, Epoch 60/291, Val RMSE: 0.6291, Time: 13.87s\n",
      "Trial 96, Epoch 61/291, Val RMSE: 0.6245, Time: 13.89s\n",
      "Trial 96, Epoch 62/291, Val RMSE: 0.6374, Time: 13.76s\n",
      "Trial 96, Epoch 63/291, Val RMSE: 0.6231, Time: 13.91s\n",
      "Trial 96, Epoch 64/291, Val RMSE: 0.6348, Time: 13.93s\n",
      "Trial 96, Epoch 65/291, Val RMSE: 0.6232, Time: 14.00s\n",
      "Trial 96, Epoch 66/291, Val RMSE: 0.6389, Time: 13.90s\n",
      "Trial 96, Epoch 67/291, Val RMSE: 0.6253, Time: 13.93s\n",
      "Trial 96, Epoch 68/291, Val RMSE: 0.6288, Time: 13.83s\n",
      "Trial 96, Epoch 69/291, Val RMSE: 0.6211, Time: 13.90s\n",
      "Trial 96, Epoch 70/291, Val RMSE: 0.6238, Time: 14.01s\n",
      "Trial 96, Epoch 71/291, Val RMSE: 0.6216, Time: 13.91s\n",
      "Trial 96, Epoch 72/291, Val RMSE: 0.6201, Time: 13.79s\n",
      "Trial 96, Epoch 73/291, Val RMSE: 0.6192, Time: 14.01s\n",
      "Trial 96, Epoch 74/291, Val RMSE: 0.6253, Time: 13.84s\n",
      "Trial 96, Epoch 75/291, Val RMSE: 0.6193, Time: 13.72s\n",
      "Trial 96, Epoch 76/291, Val RMSE: 0.6160, Time: 13.71s\n",
      "Trial 96, Epoch 77/291, Val RMSE: 0.6274, Time: 13.80s\n",
      "Trial 96, Epoch 78/291, Val RMSE: 0.6198, Time: 13.79s\n",
      "Trial 96, Epoch 79/291, Val RMSE: 0.6179, Time: 13.79s\n",
      "Trial 96, Epoch 80/291, Val RMSE: 0.6148, Time: 13.80s\n",
      "Trial 96, Epoch 81/291, Val RMSE: 0.6270, Time: 14.24s\n",
      "Trial 96, Epoch 82/291, Val RMSE: 0.6184, Time: 14.10s\n",
      "Trial 96, Epoch 83/291, Val RMSE: 0.6155, Time: 13.88s\n",
      "Trial 96, Epoch 84/291, Val RMSE: 0.6176, Time: 13.74s\n",
      "Trial 96, Epoch 85/291, Val RMSE: 0.6178, Time: 13.74s\n",
      "Trial 96, Epoch 86/291, Val RMSE: 0.6225, Time: 13.79s\n",
      "Trial 96, Epoch 87/291, Val RMSE: 0.6206, Time: 14.05s\n",
      "Trial 96, Epoch 88/291, Val RMSE: 0.6184, Time: 13.85s\n",
      "Trial 96, Epoch 89/291, Val RMSE: 0.6174, Time: 13.97s\n",
      "Trial 96, Epoch 90/291, Val RMSE: 0.6194, Time: 13.86s\n",
      "Trial 96, Epoch 91/291, Val RMSE: 0.6298, Time: 13.93s\n",
      "Trial 96, Epoch 92/291, Val RMSE: 0.6220, Time: 13.83s\n",
      "Trial 96, Epoch 93/291, Val RMSE: 0.6199, Time: 13.80s\n",
      "Trial 96, Epoch 94/291, Val RMSE: 0.6281, Time: 13.75s\n",
      "Trial 96, Epoch 95/291, Val RMSE: 0.6139, Time: 14.07s\n",
      "Trial 96, Epoch 96/291, Val RMSE: 0.6216, Time: 13.79s\n",
      "Trial 96, Epoch 97/291, Val RMSE: 0.6228, Time: 13.78s\n",
      "Trial 96, Epoch 98/291, Val RMSE: 0.6225, Time: 13.77s\n",
      "Trial 96, Epoch 99/291, Val RMSE: 0.6286, Time: 13.87s\n",
      "Trial 96, Epoch 100/291, Val RMSE: 0.6287, Time: 13.75s\n",
      "Trial 96, Epoch 101/291, Val RMSE: 0.6178, Time: 13.75s\n",
      "Trial 96, Epoch 102/291, Val RMSE: 0.6222, Time: 13.67s\n",
      "Trial 96, Epoch 103/291, Val RMSE: 0.6243, Time: 13.75s\n",
      "Trial 96, Epoch 104/291, Val RMSE: 0.6190, Time: 13.89s\n",
      "Trial 96, Epoch 105/291, Val RMSE: 0.6180, Time: 13.74s\n",
      "Trial 96, Epoch 106/291, Val RMSE: 0.6198, Time: 13.63s\n",
      "Trial 96, Epoch 107/291, Val RMSE: 0.6200, Time: 13.74s\n",
      "Trial 96, Epoch 108/291, Val RMSE: 0.6159, Time: 13.75s\n",
      "Trial 96, Epoch 109/291, Val RMSE: 0.6316, Time: 13.73s\n",
      "Trial 96, Epoch 110/291, Val RMSE: 0.6259, Time: 13.81s\n",
      "Trial 96, Epoch 111/291, Val RMSE: 0.6211, Time: 13.72s\n",
      "Trial 96, Epoch 112/291, Val RMSE: 0.6148, Time: 13.74s\n",
      "Trial 96, Epoch 113/291, Val RMSE: 0.6334, Time: 13.81s\n",
      "Trial 96, Epoch 114/291, Val RMSE: 0.6233, Time: 13.71s\n",
      "Trial 96, Epoch 115/291, Val RMSE: 0.6181, Time: 13.86s\n",
      "Trial 96, Epoch 116/291, Val RMSE: 0.6213, Time: 13.77s\n",
      "Trial 96, Epoch 117/291, Val RMSE: 0.6334, Time: 13.98s\n",
      "Trial 96, Epoch 118/291, Val RMSE: 0.6199, Time: 13.79s\n",
      "Trial 96, Epoch 119/291, Val RMSE: 0.6258, Time: 13.80s\n",
      "Trial 96, Epoch 120/291, Val RMSE: 0.6276, Time: 13.75s\n",
      "Trial 96, Epoch 121/291, Val RMSE: 0.6171, Time: 13.83s\n",
      "Trial 96, Epoch 122/291, Val RMSE: 0.6197, Time: 13.69s\n",
      "Trial 96, Epoch 123/291, Val RMSE: 0.6173, Time: 13.78s\n",
      "Trial 96, Epoch 124/291, Val RMSE: 0.6232, Time: 13.77s\n",
      "Trial 96, Epoch 125/291, Val RMSE: 0.6239, Time: 13.97s\n",
      "Trial 96, Epoch 126/291, Val RMSE: 0.6179, Time: 13.95s\n",
      "Trial 96, Epoch 127/291, Val RMSE: 0.6218, Time: 13.84s\n",
      "Trial 96, Epoch 128/291, Val RMSE: 0.6174, Time: 13.84s\n",
      "Trial 96, Epoch 129/291, Val RMSE: 0.6181, Time: 13.71s\n",
      "Trial 96, Epoch 130/291, Val RMSE: 0.6122, Time: 13.92s\n",
      "Trial 96, Epoch 131/291, Val RMSE: 0.6251, Time: 13.75s\n",
      "Trial 96, Epoch 132/291, Val RMSE: 0.6209, Time: 13.71s\n",
      "Trial 96, Epoch 133/291, Val RMSE: 0.6242, Time: 13.83s\n",
      "Trial 96, Epoch 134/291, Val RMSE: 0.6175, Time: 13.83s\n",
      "Trial 96, Epoch 135/291, Val RMSE: 0.6173, Time: 13.75s\n",
      "Trial 96, Epoch 136/291, Val RMSE: 0.6188, Time: 13.72s\n",
      "Trial 96, Epoch 137/291, Val RMSE: 0.6172, Time: 13.69s\n",
      "Trial 96, Epoch 138/291, Val RMSE: 0.6236, Time: 13.84s\n",
      "Trial 96, Epoch 139/291, Val RMSE: 0.6221, Time: 13.69s\n",
      "Trial 96, Epoch 140/291, Val RMSE: 0.6254, Time: 13.64s\n",
      "Trial 96, Epoch 141/291, Val RMSE: 0.6217, Time: 13.82s\n",
      "Trial 96, Epoch 142/291, Val RMSE: 0.6231, Time: 13.93s\n",
      "Trial 96, Epoch 143/291, Val RMSE: 0.6225, Time: 13.90s\n",
      "Trial 96, Epoch 144/291, Val RMSE: 0.6133, Time: 13.71s\n",
      "Trial 96, Epoch 145/291, Val RMSE: 0.6222, Time: 13.78s\n",
      "[I 2025-07-17 15:28:33,793] Trial 96 pruned. \n",
      "\n",
      "Optuna optimization finished for GNN.\n",
      "\n",
      "--- Best Trial Results for GNN ---\n",
      "Best trial number: 60\n",
      "Best RMSE (Validation): 0.6093\n",
      "Best hyperparameters:\n",
      "  hidden_channels: 999\n",
      "  lr: 0.0002654886343578734\n",
      "  batch_size: 128\n",
      "  n_epochs: 190\n",
      "  num_layers: 1\n",
      "  dropout_rate: 0.12998376396007172\n",
      "  weight_decay: 4.855663649252953e-08\n",
      "Best R2 Score (Validation): 0.6213\n"
     ]
    }
   ],
   "source": [
    "study_dir = Path(\"../studies/gnn_study\")\n",
    "study_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "study_db_path = f\"sqlite:///{study_dir / 'gnn_optuna_study.db'}\"\n",
    "study_name = \"gnn_regression_pGI50\"\n",
    "print(f\"Optuna study for GNN will be stored at: {study_db_path}\")\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(\n",
    "    n_startup_trials=10,  # Run at least these many trials completely before starting to prune\n",
    "    n_warmup_steps=20,    # Don't prune trials until they've completed these many epochs\n",
    "    interval_steps=10     # Check for pruning every these many epochs\n",
    ")\n",
    "# pruner = None\n",
    "\n",
    "# Check if a study with the same name already exists in the database\n",
    "# If it does, load it to resume the optimization.\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=study_db_path)\n",
    "    print(f\"Loaded existing study '{study_name}' from {study_db_path}. Resuming optimization.\")\n",
    "except KeyError:\n",
    "    # If the study does not exist, create a new one\n",
    "    print(f\"Creating new study '{study_name}' at {study_db_path}.\")\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"minimize\",\n",
    "        storage=study_db_path,\n",
    "        pruner=pruner\n",
    "    )\n",
    "\n",
    "print(\"\\nStarting Optuna optimization for GNN...\")\n",
    "# Run for 'n_trial' trials or 'timeout' seconds, whichever completes first\n",
    "study.optimize(objective, n_trials=None, timeout=14400, show_progress_bar=True)\n",
    "print(\"\\nOptuna optimization finished for GNN.\")\n",
    "\n",
    "# Print best trial results\n",
    "print(\"\\n--- Best Trial Results for GNN ---\")\n",
    "print(f\"Best trial number: {study.best_trial.number}\")\n",
    "print(f\"Best RMSE (Validation): {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "if \"final_r2_score\" in study.best_trial.user_attrs:\n",
    "    print(f\"Best R2 Score (Validation): {study.best_trial.user_attrs['final_r2_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773cb25-a8fb-4d90-96d7-8745c16b01be",
   "metadata": {},
   "source": [
    "## Train Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19444a-e710-425d-ac80-0f42e0fd3ef6",
   "metadata": {},
   "source": [
    "### Reinitialize Everything with Best Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31078f14-daa7-4943-8423-dba53f579e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters (GNN): {'hidden_channels': 999, 'lr': 0.0002654886343578734, 'batch_size': 128, 'n_epochs': 190, 'num_layers': 1, 'dropout_rate': 0.12998376396007172, 'weight_decay': 4.855663649252953e-08}\n",
      "Best hyperparameters from Optuna: {'hidden_channels': 999, 'lr': 0.0002654886343578734, 'batch_size': 128, 'n_epochs': 190, 'num_layers': 1, 'dropout_rate': 0.12998376396007172, 'weight_decay': 4.855663649252953e-08}\n",
      "Final GNN model, criterion, optimizer, and DataLoaders initialized with best parameters.\n",
      "Training on combined 15931 samples, testing on 2812 samples.\n"
     ]
    }
   ],
   "source": [
    "# Re-load the study to ensure the latest best parameters\n",
    "study_dir = Path(\"../studies/gnn_study\")\n",
    "study_db_path = f\"sqlite:///{study_dir / 'gnn_optuna_study.db'}\"\n",
    "study_name = \"gnn_regression_pGI50\"\n",
    "\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=study_db_path)\n",
    "    print(\"Best trial parameters (GNN):\", study.best_trial.params)\n",
    "    best_params = study.best_trial.params\n",
    "except KeyError:\n",
    "    print(f\"Study '{study_name}' does not exist at {study_db_path}. Please make sure the GNN Optuna study cell has been run.\")\n",
    "\n",
    "best_hidden_channels = best_params[\"hidden_channels\"]\n",
    "best_learning_rate = best_params[\"lr\"]\n",
    "best_batch_size = best_params[\"batch_size\"]\n",
    "best_n_epochs = best_params[\"n_epochs\"]\n",
    "best_num_layers = best_params[\"num_layers\"]\n",
    "best_dropout_rate = best_params[\"dropout_rate\"]\n",
    "best_weight_decay = best_params[\"weight_decay\"]\n",
    "\n",
    "print(f\"Best hyperparameters from Optuna: {best_params}\")\n",
    "\n",
    "# Re-initialize the model with best hyperparameters\n",
    "if not train_data_list:\n",
    "    raise ValueError(\"train_data_list is empty. Cannot determine feature dimensions for GNN.\")\n",
    "\n",
    "node_feature_dim = train_data_list[0].x.shape[1]\n",
    "global_feature_dim = train_data_list[0].global_features.shape[1]\n",
    "\n",
    "final_gnn_model = GNN(\n",
    "    node_feature_dim=node_feature_dim,\n",
    "    global_feature_dim=global_feature_dim,\n",
    "    hidden_channels=best_hidden_channels,\n",
    "    num_layers=best_num_layers,\n",
    "    dropout_rate=best_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Re-initialize criterion and optimizer\n",
    "final_criterion = nn.MSELoss()\n",
    "final_optimizer = optim.Adam(final_gnn_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "\n",
    "# Re-create DataLoaders with the best batch size (Training + Validation data COMBINED)\n",
    "final_train_val_data_list = train_data_list + val_data_list\n",
    "\n",
    "# Create DataLoaders with the best batch size\n",
    "num_workers = 0\n",
    "final_train_val_loader = PyGDataLoader(final_train_val_data_list, batch_size=best_batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Create the FINAL TEST DataLoader\n",
    "final_test_loader = PyGDataLoader(test_data_list, batch_size=best_batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"Final GNN model, criterion, optimizer, and DataLoaders initialized with best parameters.\")\n",
    "print(f\"Training on combined {len(final_train_val_data_list)} samples, testing on {len(test_data_list)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e421910-603a-469c-b7fe-133619e5eedc",
   "metadata": {},
   "source": [
    "### Get Current Commit ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863c9398-d629-407f-a118-575c9d400651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_commit_hash():\n",
    "    try:\n",
    "        # Get the short commit hash\n",
    "        commit_hash = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip().decode('ascii')\n",
    "        return commit_hash\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return \"unknown_commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa70146-1e6a-44ce-84a8-dca9bcb04dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Git Commit ID: 271d2f4\n"
     ]
    }
   ],
   "source": [
    "# Optionally, see the current commit ID\n",
    "current_commit = get_git_commit_hash()\n",
    "print(f\"Current Git Commit ID: {current_commit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ed59c-cb77-4821-8773-4936603ac335",
   "metadata": {},
   "source": [
    "### Train and Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc661f62-1bc3-47d7-9d83-41d64ff14e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining final GNN model for 190 epochs with best parameters...\n",
      "Associated Git Commit ID for saved model: 271d2f4\n",
      "Epoch 1/190, Train Loss: 2.0786, Eval RMSE on combined data: 0.7288, Time: 5.49s\n",
      "--- New best final GNN model saved at epoch 1 with RMSE: 0.7288 ---\n",
      "Epoch 2/190, Train Loss: 0.5506, Eval RMSE on combined data: 0.6565, Time: 5.28s\n",
      "--- New best final GNN model saved at epoch 2 with RMSE: 0.6565 ---\n",
      "Epoch 3/190, Train Loss: 0.4511, Eval RMSE on combined data: 0.5760, Time: 5.31s\n",
      "--- New best final GNN model saved at epoch 3 with RMSE: 0.5760 ---\n",
      "Epoch 4/190, Train Loss: 0.3592, Eval RMSE on combined data: 0.5163, Time: 5.25s\n",
      "--- New best final GNN model saved at epoch 4 with RMSE: 0.5163 ---\n",
      "Epoch 5/190, Train Loss: 0.2898, Eval RMSE on combined data: 0.4665, Time: 5.26s\n",
      "--- New best final GNN model saved at epoch 5 with RMSE: 0.4665 ---\n",
      "Epoch 6/190, Train Loss: 0.2443, Eval RMSE on combined data: 0.4380, Time: 5.23s\n",
      "--- New best final GNN model saved at epoch 6 with RMSE: 0.4380 ---\n",
      "Epoch 7/190, Train Loss: 0.2088, Eval RMSE on combined data: 0.4194, Time: 5.26s\n",
      "--- New best final GNN model saved at epoch 7 with RMSE: 0.4194 ---\n",
      "Epoch 8/190, Train Loss: 0.1873, Eval RMSE on combined data: 0.3901, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 8 with RMSE: 0.3901 ---\n",
      "Epoch 9/190, Train Loss: 0.1573, Eval RMSE on combined data: 0.3580, Time: 5.31s\n",
      "--- New best final GNN model saved at epoch 9 with RMSE: 0.3580 ---\n",
      "Epoch 10/190, Train Loss: 0.1475, Eval RMSE on combined data: 0.3388, Time: 5.25s\n",
      "--- New best final GNN model saved at epoch 10 with RMSE: 0.3388 ---\n",
      "Epoch 11/190, Train Loss: 0.1321, Eval RMSE on combined data: 0.3360, Time: 5.30s\n",
      "--- New best final GNN model saved at epoch 11 with RMSE: 0.3360 ---\n",
      "Epoch 12/190, Train Loss: 0.1251, Eval RMSE on combined data: 0.3491, Time: 5.23s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.3360\n",
      "Epoch 13/190, Train Loss: 0.1176, Eval RMSE on combined data: 0.3028, Time: 5.31s\n",
      "--- New best final GNN model saved at epoch 13 with RMSE: 0.3028 ---\n",
      "Epoch 14/190, Train Loss: 0.1047, Eval RMSE on combined data: 0.3119, Time: 5.22s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.3028\n",
      "Epoch 15/190, Train Loss: 0.0996, Eval RMSE on combined data: 0.3406, Time: 5.25s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.3028\n",
      "Epoch 16/190, Train Loss: 0.0940, Eval RMSE on combined data: 0.2665, Time: 5.25s\n",
      "--- New best final GNN model saved at epoch 16 with RMSE: 0.2665 ---\n",
      "Epoch 17/190, Train Loss: 0.0785, Eval RMSE on combined data: 0.2625, Time: 5.29s\n",
      "--- New best final GNN model saved at epoch 17 with RMSE: 0.2625 ---\n",
      "Epoch 18/190, Train Loss: 0.0802, Eval RMSE on combined data: 0.2483, Time: 5.24s\n",
      "--- New best final GNN model saved at epoch 18 with RMSE: 0.2483 ---\n",
      "Epoch 19/190, Train Loss: 0.0736, Eval RMSE on combined data: 0.2556, Time: 5.25s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2483\n",
      "Epoch 20/190, Train Loss: 0.0730, Eval RMSE on combined data: 0.2412, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 20 with RMSE: 0.2412 ---\n",
      "Epoch 21/190, Train Loss: 0.0761, Eval RMSE on combined data: 0.2623, Time: 5.21s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2412\n",
      "Epoch 22/190, Train Loss: 0.0752, Eval RMSE on combined data: 0.2678, Time: 5.24s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2412\n",
      "Epoch 23/190, Train Loss: 0.0705, Eval RMSE on combined data: 0.2279, Time: 5.26s\n",
      "--- New best final GNN model saved at epoch 23 with RMSE: 0.2279 ---\n",
      "Epoch 24/190, Train Loss: 0.0684, Eval RMSE on combined data: 0.2399, Time: 5.36s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2279\n",
      "Epoch 25/190, Train Loss: 0.0683, Eval RMSE on combined data: 0.2485, Time: 5.15s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2279\n",
      "Epoch 26/190, Train Loss: 0.0647, Eval RMSE on combined data: 0.2276, Time: 5.13s\n",
      "--- New best final GNN model saved at epoch 26 with RMSE: 0.2276 ---\n",
      "Epoch 27/190, Train Loss: 0.0635, Eval RMSE on combined data: 0.2360, Time: 5.20s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2276\n",
      "Epoch 28/190, Train Loss: 0.0656, Eval RMSE on combined data: 0.2271, Time: 5.26s\n",
      "--- New best final GNN model saved at epoch 28 with RMSE: 0.2271 ---\n",
      "Epoch 29/190, Train Loss: 0.0578, Eval RMSE on combined data: 0.2273, Time: 5.21s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2271\n",
      "Epoch 30/190, Train Loss: 0.0587, Eval RMSE on combined data: 0.2151, Time: 5.28s\n",
      "--- New best final GNN model saved at epoch 30 with RMSE: 0.2151 ---\n",
      "Epoch 31/190, Train Loss: 0.0567, Eval RMSE on combined data: 0.2175, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2151\n",
      "Epoch 32/190, Train Loss: 0.0571, Eval RMSE on combined data: 0.2178, Time: 5.25s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2151\n",
      "Epoch 33/190, Train Loss: 0.0535, Eval RMSE on combined data: 0.2185, Time: 5.24s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.2151\n",
      "Epoch 34/190, Train Loss: 0.0598, Eval RMSE on combined data: 0.2136, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 34 with RMSE: 0.2136 ---\n",
      "Epoch 35/190, Train Loss: 0.0594, Eval RMSE on combined data: 0.2239, Time: 5.21s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2136\n",
      "Epoch 36/190, Train Loss: 0.0563, Eval RMSE on combined data: 0.2287, Time: 5.25s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2136\n",
      "Epoch 37/190, Train Loss: 0.0510, Eval RMSE on combined data: 0.2163, Time: 5.36s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.2136\n",
      "Epoch 38/190, Train Loss: 0.0469, Eval RMSE on combined data: 0.2457, Time: 5.18s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.2136\n",
      "Epoch 39/190, Train Loss: 0.0513, Eval RMSE on combined data: 0.1922, Time: 5.19s\n",
      "--- New best final GNN model saved at epoch 39 with RMSE: 0.1922 ---\n",
      "Epoch 40/190, Train Loss: 0.0509, Eval RMSE on combined data: 0.2118, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1922\n",
      "Epoch 41/190, Train Loss: 0.0490, Eval RMSE on combined data: 0.1918, Time: 5.18s\n",
      "--- New best final GNN model saved at epoch 41 with RMSE: 0.1918 ---\n",
      "Epoch 42/190, Train Loss: 0.0497, Eval RMSE on combined data: 0.2024, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 43/190, Train Loss: 0.0473, Eval RMSE on combined data: 0.2047, Time: 5.23s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 44/190, Train Loss: 0.0494, Eval RMSE on combined data: 0.1976, Time: 5.23s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 45/190, Train Loss: 0.0451, Eval RMSE on combined data: 0.2235, Time: 5.35s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 46/190, Train Loss: 0.0445, Eval RMSE on combined data: 0.1975, Time: 5.24s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 47/190, Train Loss: 0.0418, Eval RMSE on combined data: 0.2367, Time: 5.19s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 48/190, Train Loss: 0.0428, Eval RMSE on combined data: 0.2130, Time: 5.27s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 49/190, Train Loss: 0.0449, Eval RMSE on combined data: 0.1830, Time: 5.28s\n",
      "--- New best final GNN model saved at epoch 49 with RMSE: 0.1830 ---\n",
      "Epoch 50/190, Train Loss: 0.0415, Eval RMSE on combined data: 0.1860, Time: 5.27s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1830\n",
      "Epoch 51/190, Train Loss: 0.0421, Eval RMSE on combined data: 0.1924, Time: 5.26s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1830\n",
      "Epoch 52/190, Train Loss: 0.0432, Eval RMSE on combined data: 0.2530, Time: 5.28s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1830\n",
      "Epoch 53/190, Train Loss: 0.0413, Eval RMSE on combined data: 0.1810, Time: 5.36s\n",
      "--- New best final GNN model saved at epoch 53 with RMSE: 0.1810 ---\n",
      "Epoch 54/190, Train Loss: 0.0420, Eval RMSE on combined data: 0.2102, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1810\n",
      "Epoch 55/190, Train Loss: 0.0472, Eval RMSE on combined data: 0.2054, Time: 5.29s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1810\n",
      "Epoch 56/190, Train Loss: 0.0420, Eval RMSE on combined data: 0.1844, Time: 5.23s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1810\n",
      "Epoch 57/190, Train Loss: 0.0411, Eval RMSE on combined data: 0.1790, Time: 5.27s\n",
      "--- New best final GNN model saved at epoch 57 with RMSE: 0.1790 ---\n",
      "Epoch 58/190, Train Loss: 0.0420, Eval RMSE on combined data: 0.2392, Time: 5.24s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1790\n",
      "Epoch 59/190, Train Loss: 0.0415, Eval RMSE on combined data: 0.2041, Time: 5.39s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1790\n",
      "Epoch 60/190, Train Loss: 0.0402, Eval RMSE on combined data: 0.1770, Time: 5.20s\n",
      "--- New best final GNN model saved at epoch 60 with RMSE: 0.1770 ---\n",
      "Epoch 61/190, Train Loss: 0.0391, Eval RMSE on combined data: 0.1865, Time: 5.23s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1770\n",
      "Epoch 62/190, Train Loss: 0.0389, Eval RMSE on combined data: 0.1852, Time: 5.25s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1770\n",
      "Epoch 63/190, Train Loss: 0.0417, Eval RMSE on combined data: 0.1909, Time: 5.19s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1770\n",
      "Epoch 64/190, Train Loss: 0.0421, Eval RMSE on combined data: 0.2106, Time: 5.24s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1770\n",
      "Epoch 65/190, Train Loss: 0.0390, Eval RMSE on combined data: 0.1717, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 65 with RMSE: 0.1717 ---\n",
      "Epoch 66/190, Train Loss: 0.0346, Eval RMSE on combined data: 0.2088, Time: 5.27s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 67/190, Train Loss: 0.0359, Eval RMSE on combined data: 0.1785, Time: 5.18s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 68/190, Train Loss: 0.0396, Eval RMSE on combined data: 0.2076, Time: 5.21s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 69/190, Train Loss: 0.0382, Eval RMSE on combined data: 0.1929, Time: 5.15s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 70/190, Train Loss: 0.0387, Eval RMSE on combined data: 0.1752, Time: 5.21s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 71/190, Train Loss: 0.0369, Eval RMSE on combined data: 0.2039, Time: 5.16s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 72/190, Train Loss: 0.0374, Eval RMSE on combined data: 0.1794, Time: 5.20s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 73/190, Train Loss: 0.0370, Eval RMSE on combined data: 0.2252, Time: 5.25s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 74/190, Train Loss: 0.0370, Eval RMSE on combined data: 0.1766, Time: 5.24s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 75/190, Train Loss: 0.0346, Eval RMSE on combined data: 0.1783, Time: 5.23s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 76/190, Train Loss: 0.0362, Eval RMSE on combined data: 0.1809, Time: 5.28s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 77/190, Train Loss: 0.0338, Eval RMSE on combined data: 0.1822, Time: 5.31s\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 78/190, Train Loss: 0.0322, Eval RMSE on combined data: 0.1765, Time: 5.28s\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 79/190, Train Loss: 0.0341, Eval RMSE on combined data: 0.1866, Time: 5.28s\n",
      "No improvement for 14 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 80/190, Train Loss: 0.0364, Eval RMSE on combined data: 0.1707, Time: 5.20s\n",
      "--- New best final GNN model saved at epoch 80 with RMSE: 0.1707 ---\n",
      "Epoch 81/190, Train Loss: 0.0338, Eval RMSE on combined data: 0.1663, Time: 5.30s\n",
      "--- New best final GNN model saved at epoch 81 with RMSE: 0.1663 ---\n",
      "Epoch 82/190, Train Loss: 0.0326, Eval RMSE on combined data: 0.2065, Time: 5.25s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1663\n",
      "Epoch 83/190, Train Loss: 0.0326, Eval RMSE on combined data: 0.1697, Time: 5.20s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1663\n",
      "Epoch 84/190, Train Loss: 0.0328, Eval RMSE on combined data: 0.1730, Time: 5.20s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1663\n",
      "Epoch 85/190, Train Loss: 0.0347, Eval RMSE on combined data: 0.1917, Time: 5.24s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1663\n",
      "Epoch 86/190, Train Loss: 0.0360, Eval RMSE on combined data: 0.1611, Time: 5.13s\n",
      "--- New best final GNN model saved at epoch 86 with RMSE: 0.1611 ---\n",
      "Epoch 87/190, Train Loss: 0.0373, Eval RMSE on combined data: 0.1752, Time: 5.19s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 88/190, Train Loss: 0.0388, Eval RMSE on combined data: 0.2025, Time: 5.16s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 89/190, Train Loss: 0.0361, Eval RMSE on combined data: 0.1634, Time: 5.44s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 90/190, Train Loss: 0.0342, Eval RMSE on combined data: 0.1653, Time: 5.21s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 91/190, Train Loss: 0.0351, Eval RMSE on combined data: 0.1834, Time: 5.20s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 92/190, Train Loss: 0.0328, Eval RMSE on combined data: 0.1691, Time: 5.28s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 93/190, Train Loss: 0.0303, Eval RMSE on combined data: 0.1812, Time: 5.38s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 94/190, Train Loss: 0.0337, Eval RMSE on combined data: 0.1532, Time: 5.37s\n",
      "--- New best final GNN model saved at epoch 94 with RMSE: 0.1532 ---\n",
      "Epoch 95/190, Train Loss: 0.0331, Eval RMSE on combined data: 0.1778, Time: 5.41s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 96/190, Train Loss: 0.0317, Eval RMSE on combined data: 0.1557, Time: 5.23s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 97/190, Train Loss: 0.0299, Eval RMSE on combined data: 0.1576, Time: 5.39s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 98/190, Train Loss: 0.0310, Eval RMSE on combined data: 0.1727, Time: 5.33s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 99/190, Train Loss: 0.0332, Eval RMSE on combined data: 0.1926, Time: 5.38s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 100/190, Train Loss: 0.0339, Eval RMSE on combined data: 0.1796, Time: 5.29s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 101/190, Train Loss: 0.0332, Eval RMSE on combined data: 0.1595, Time: 5.33s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 102/190, Train Loss: 0.0310, Eval RMSE on combined data: 0.1588, Time: 5.32s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 103/190, Train Loss: 0.0298, Eval RMSE on combined data: 0.1646, Time: 5.27s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 104/190, Train Loss: 0.0307, Eval RMSE on combined data: 0.1546, Time: 5.24s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 105/190, Train Loss: 0.0285, Eval RMSE on combined data: 0.1678, Time: 5.34s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 106/190, Train Loss: 0.0273, Eval RMSE on combined data: 0.1821, Time: 5.26s\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 107/190, Train Loss: 0.0320, Eval RMSE on combined data: 0.1580, Time: 5.26s\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 108/190, Train Loss: 0.0288, Eval RMSE on combined data: 0.1756, Time: 5.65s\n",
      "No improvement for 14 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 109/190, Train Loss: 0.0264, Eval RMSE on combined data: 0.1471, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 109 with RMSE: 0.1471 ---\n",
      "Epoch 110/190, Train Loss: 0.0296, Eval RMSE on combined data: 0.1517, Time: 5.32s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 111/190, Train Loss: 0.0300, Eval RMSE on combined data: 0.1728, Time: 5.22s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 112/190, Train Loss: 0.0281, Eval RMSE on combined data: 0.1561, Time: 5.27s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 113/190, Train Loss: 0.0291, Eval RMSE on combined data: 0.1686, Time: 5.20s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 114/190, Train Loss: 0.0305, Eval RMSE on combined data: 0.1699, Time: 5.27s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 115/190, Train Loss: 0.0310, Eval RMSE on combined data: 0.1544, Time: 5.22s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 116/190, Train Loss: 0.0298, Eval RMSE on combined data: 0.1759, Time: 5.21s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 117/190, Train Loss: 0.0290, Eval RMSE on combined data: 0.1398, Time: 5.24s\n",
      "--- New best final GNN model saved at epoch 117 with RMSE: 0.1398 ---\n",
      "Epoch 118/190, Train Loss: 0.0288, Eval RMSE on combined data: 0.1867, Time: 5.29s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 119/190, Train Loss: 0.0304, Eval RMSE on combined data: 0.1645, Time: 5.31s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 120/190, Train Loss: 0.0293, Eval RMSE on combined data: 0.1483, Time: 5.28s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 121/190, Train Loss: 0.0273, Eval RMSE on combined data: 0.1455, Time: 5.21s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 122/190, Train Loss: 0.0279, Eval RMSE on combined data: 0.1616, Time: 5.34s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 123/190, Train Loss: 0.0292, Eval RMSE on combined data: 0.1573, Time: 5.24s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 124/190, Train Loss: 0.0271, Eval RMSE on combined data: 0.1614, Time: 5.31s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 125/190, Train Loss: 0.0296, Eval RMSE on combined data: 0.1557, Time: 5.25s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 126/190, Train Loss: 0.0291, Eval RMSE on combined data: 0.1652, Time: 5.35s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 127/190, Train Loss: 0.0274, Eval RMSE on combined data: 0.1514, Time: 5.32s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 128/190, Train Loss: 0.0267, Eval RMSE on combined data: 0.1593, Time: 5.28s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 129/190, Train Loss: 0.0291, Eval RMSE on combined data: 0.1550, Time: 5.18s\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 130/190, Train Loss: 0.0269, Eval RMSE on combined data: 0.1660, Time: 5.22s\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 131/190, Train Loss: 0.0268, Eval RMSE on combined data: 0.1544, Time: 5.21s\n",
      "No improvement for 14 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 132/190, Train Loss: 0.0294, Eval RMSE on combined data: 0.1471, Time: 5.20s\n",
      "No improvement for 15 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 133/190, Train Loss: 0.0268, Eval RMSE on combined data: 0.1524, Time: 5.23s\n",
      "No improvement for 16 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 134/190, Train Loss: 0.0270, Eval RMSE on combined data: 0.1577, Time: 5.26s\n",
      "No improvement for 17 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 135/190, Train Loss: 0.0268, Eval RMSE on combined data: 0.1533, Time: 5.25s\n",
      "No improvement for 18 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 136/190, Train Loss: 0.0257, Eval RMSE on combined data: 0.1479, Time: 5.28s\n",
      "No improvement for 19 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 137/190, Train Loss: 0.0246, Eval RMSE on combined data: 0.1462, Time: 5.21s\n",
      "No improvement for 20 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 138/190, Train Loss: 0.0278, Eval RMSE on combined data: 0.1439, Time: 5.34s\n",
      "No improvement for 21 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 139/190, Train Loss: 0.0254, Eval RMSE on combined data: 0.1558, Time: 5.68s\n",
      "No improvement for 22 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 140/190, Train Loss: 0.0265, Eval RMSE on combined data: 0.1470, Time: 5.24s\n",
      "No improvement for 23 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 141/190, Train Loss: 0.0265, Eval RMSE on combined data: 0.1546, Time: 5.44s\n",
      "No improvement for 24 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 142/190, Train Loss: 0.0259, Eval RMSE on combined data: 0.1482, Time: 5.27s\n",
      "No improvement for 25 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 143/190, Train Loss: 0.0258, Eval RMSE on combined data: 0.1469, Time: 5.33s\n",
      "No improvement for 26 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 144/190, Train Loss: 0.0248, Eval RMSE on combined data: 0.1528, Time: 5.29s\n",
      "No improvement for 27 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 145/190, Train Loss: 0.0248, Eval RMSE on combined data: 0.1568, Time: 5.36s\n",
      "No improvement for 28 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 146/190, Train Loss: 0.0273, Eval RMSE on combined data: 0.1485, Time: 5.23s\n",
      "No improvement for 29 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 147/190, Train Loss: 0.0258, Eval RMSE on combined data: 0.1520, Time: 5.26s\n",
      "No improvement for 30 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 148/190, Train Loss: 0.0266, Eval RMSE on combined data: 0.1779, Time: 5.22s\n",
      "No improvement for 31 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 149/190, Train Loss: 0.0269, Eval RMSE on combined data: 0.1590, Time: 5.24s\n",
      "No improvement for 32 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 150/190, Train Loss: 0.0254, Eval RMSE on combined data: 0.1461, Time: 5.26s\n",
      "No improvement for 33 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 151/190, Train Loss: 0.0251, Eval RMSE on combined data: 0.1497, Time: 5.21s\n",
      "No improvement for 34 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 152/190, Train Loss: 0.0275, Eval RMSE on combined data: 0.1657, Time: 5.20s\n",
      "No improvement for 35 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 153/190, Train Loss: 0.0251, Eval RMSE on combined data: 0.1595, Time: 5.39s\n",
      "No improvement for 36 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 154/190, Train Loss: 0.0241, Eval RMSE on combined data: 0.1377, Time: 5.25s\n",
      "--- New best final GNN model saved at epoch 154 with RMSE: 0.1377 ---\n",
      "Epoch 155/190, Train Loss: 0.0248, Eval RMSE on combined data: 0.1570, Time: 5.33s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 156/190, Train Loss: 0.0246, Eval RMSE on combined data: 0.1560, Time: 5.27s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 157/190, Train Loss: 0.0255, Eval RMSE on combined data: 0.1385, Time: 5.30s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 158/190, Train Loss: 0.0247, Eval RMSE on combined data: 0.1515, Time: 5.29s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 159/190, Train Loss: 0.0254, Eval RMSE on combined data: 0.1688, Time: 5.39s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 160/190, Train Loss: 0.0223, Eval RMSE on combined data: 0.1580, Time: 5.24s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 161/190, Train Loss: 0.0242, Eval RMSE on combined data: 0.1351, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 161 with RMSE: 0.1351 ---\n",
      "Epoch 162/190, Train Loss: 0.0238, Eval RMSE on combined data: 0.1439, Time: 5.25s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1351\n",
      "Epoch 163/190, Train Loss: 0.0240, Eval RMSE on combined data: 0.1457, Time: 5.33s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1351\n",
      "Epoch 164/190, Train Loss: 0.0255, Eval RMSE on combined data: 0.1727, Time: 5.26s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1351\n",
      "Epoch 165/190, Train Loss: 0.0265, Eval RMSE on combined data: 0.1341, Time: 5.34s\n",
      "--- New best final GNN model saved at epoch 165 with RMSE: 0.1341 ---\n",
      "Epoch 166/190, Train Loss: 0.0234, Eval RMSE on combined data: 0.1473, Time: 5.27s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 167/190, Train Loss: 0.0231, Eval RMSE on combined data: 0.1392, Time: 5.34s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 168/190, Train Loss: 0.0224, Eval RMSE on combined data: 0.1425, Time: 5.33s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 169/190, Train Loss: 0.0244, Eval RMSE on combined data: 0.1692, Time: 5.27s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 170/190, Train Loss: 0.0242, Eval RMSE on combined data: 0.1497, Time: 5.22s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 171/190, Train Loss: 0.0218, Eval RMSE on combined data: 0.1501, Time: 5.27s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 172/190, Train Loss: 0.0217, Eval RMSE on combined data: 0.1494, Time: 5.24s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 173/190, Train Loss: 0.0217, Eval RMSE on combined data: 0.1546, Time: 5.30s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 174/190, Train Loss: 0.0229, Eval RMSE on combined data: 0.1429, Time: 5.26s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 175/190, Train Loss: 0.0228, Eval RMSE on combined data: 0.1407, Time: 5.31s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 176/190, Train Loss: 0.0239, Eval RMSE on combined data: 0.1485, Time: 5.15s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 177/190, Train Loss: 0.0244, Eval RMSE on combined data: 0.1430, Time: 5.24s\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 178/190, Train Loss: 0.0251, Eval RMSE on combined data: 0.1466, Time: 5.25s\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 179/190, Train Loss: 0.0228, Eval RMSE on combined data: 0.1321, Time: 5.31s\n",
      "--- New best final GNN model saved at epoch 179 with RMSE: 0.1321 ---\n",
      "Epoch 180/190, Train Loss: 0.0212, Eval RMSE on combined data: 0.1386, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 181/190, Train Loss: 0.0208, Eval RMSE on combined data: 0.1373, Time: 5.29s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 182/190, Train Loss: 0.0238, Eval RMSE on combined data: 0.1519, Time: 5.27s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 183/190, Train Loss: 0.0225, Eval RMSE on combined data: 0.1480, Time: 5.23s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 184/190, Train Loss: 0.0234, Eval RMSE on combined data: 0.1454, Time: 5.21s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 185/190, Train Loss: 0.0229, Eval RMSE on combined data: 0.1382, Time: 5.22s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 186/190, Train Loss: 0.0237, Eval RMSE on combined data: 0.1436, Time: 5.25s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 187/190, Train Loss: 0.0234, Eval RMSE on combined data: 0.1430, Time: 5.26s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 188/190, Train Loss: 0.0216, Eval RMSE on combined data: 0.1441, Time: 5.22s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 189/190, Train Loss: 0.0233, Eval RMSE on combined data: 0.1747, Time: 5.25s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 190/190, Train Loss: 0.0255, Eval RMSE on combined data: 0.1542, Time: 5.25s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1321\n",
      "Final model training complete.\n"
     ]
    }
   ],
   "source": [
    "best_final_val_rmse = float('inf')\n",
    "patience_counter_final = 0\n",
    "final_patience = 50\n",
    "\n",
    "current_commit_hash = get_git_commit_hash()\n",
    "model_filename = f\"final_best_gnn_model_{current_commit_hash}.pt\" # Pre-define filename\n",
    "\n",
    "print(f\"Retraining final GNN model for {best_n_epochs} epochs with best parameters...\")\n",
    "print(f\"Associated Git Commit ID for saved model: {current_commit_hash}\")\n",
    "\n",
    "for epoch in range(best_n_epochs):\n",
    "    start_epoch_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    final_gnn_model.train()\n",
    "    total_train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    for data_batch in final_train_val_loader:\n",
    "        # Move data to device\n",
    "        data_batch = data_batch.to(device)\n",
    "        \n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_gnn_model(data_batch)\n",
    "        \n",
    "        # Ensure outputs and target are of same shape for loss calculation\n",
    "        loss = final_criterion(outputs.view(-1), data_batch.y.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_gnn_model.parameters(), max_norm=1.0)\n",
    "        final_optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_train_batches\n",
    "\n",
    "    # Evaluation on combined data\n",
    "    final_gnn_model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data_batch_eval in final_train_val_loader:\n",
    "            data_batch_eval = data_batch_eval.to(device)\n",
    "            \n",
    "            val_outputs = final_gnn_model(data_batch_eval)\n",
    "            val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "            val_targets.extend(data_batch_eval.y.cpu().numpy().flatten())\n",
    "\n",
    "    current_val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "\n",
    "    if device.type == 'cuda': # Ensure GPU operations are finished before timing an epoch\n",
    "        torch.cuda.synchronize()\n",
    "    end_epoch_time = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{best_n_epochs}, Train Loss: {avg_train_loss:.4f}, Eval RMSE on combined data: {current_val_rmse:.4f}, Time: {end_epoch_time - start_epoch_time:.2f}s\")\n",
    "\n",
    "    # Dynamic Best Model Saving & Early Stopping\n",
    "    if current_val_rmse < best_final_val_rmse:\n",
    "        best_final_val_rmse = current_val_rmse\n",
    "        # Save the GNN model state dict\n",
    "        torch.save(final_gnn_model.state_dict(), gnn_models_base_dir / model_filename)\n",
    "        patience_counter_final = 0 # Reset patience counter if performance improved\n",
    "        print(f\"--- New best final GNN model saved at epoch {epoch+1} with RMSE: {current_val_rmse:.4f} ---\")\n",
    "    else:\n",
    "        patience_counter_final += 1 # Increment patience counter if no improvement\n",
    "        print(f\"No improvement for {patience_counter_final} epochs. Best RMSE so far: {best_final_val_rmse:.4f}\")\n",
    "\n",
    "    if patience_counter_final >= final_patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "print(\"Final model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f489f4-d72a-4453-a196-7d2635b2d8d6",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f551353f-28f6-4118-b459-651f371428d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best saved GNN model from '..\\models\\gnn\\final_best_gnn_model_271d2f4.pt' for final test evaluation...\n",
      "\n",
      "Starting final evaluation on test set for GNN...\n",
      "Final GNN Model Test RMSE: 0.6114\n",
      "Final GNN Model Test R2: 0.6100\n"
     ]
    }
   ],
   "source": [
    "# Load the best state dict model\n",
    "print(f\"Loading best saved GNN model from '{gnn_models_base_dir / model_filename}' for final test evaluation...\")\n",
    "path_to_saved_model = gnn_models_base_dir / model_filename\n",
    "loaded_model_state_dict = torch.load(path_to_saved_model)\n",
    "final_gnn_model.load_state_dict(loaded_model_state_dict)\n",
    "final_gnn_model.eval()\n",
    "\n",
    "print(\"\\nStarting final evaluation on test set for GNN...\")\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for data_batch_test in final_test_loader:\n",
    "        data_batch_test = data_batch_test.to(device)\n",
    "\n",
    "        test_outputs = final_gnn_model(data_batch_test)\n",
    "\n",
    "        # Collect predictions and targets\n",
    "        test_predictions.extend(test_outputs.cpu().numpy().flatten())\n",
    "        test_targets.extend(data_batch_test.y.cpu().numpy().flatten())\n",
    "\n",
    "final_test_rmse = np.sqrt(mean_squared_error(test_targets, test_predictions))\n",
    "final_test_r2 = r2_score(test_targets, test_predictions)\n",
    "\n",
    "print(f\"Final GNN Model Test RMSE: {final_test_rmse:.4f}\")\n",
    "print(f\"Final GNN Model Test R2: {final_test_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
