{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e338a562",
   "metadata": {},
   "source": [
    "# <center>**Build GNN Model**</center>  \n",
    "**Author**: Shirshak Aryal  \n",
    "**Last Updated**: 18 July 2025\n",
    "\n",
    "---\n",
    "**Purpose:** This notebook is dedicated to the development, training, and evaluation of a Graph Neural Network (GNN) model for `pGI50` prediction. It covers creating the graph data objects (and saving them), defining the GNN architecture, optimizing hyperparameters, training the final model with optimal parameters, and comprehensively evaluating its performance on unseen test data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd2555-5c6e-4a73-be36-c637e121dbde",
   "metadata": {},
   "source": [
    "## 1. Setup Notebook\n",
    "This section initializes the notebook environment by importing all necessary libraries, configuring system and PyTorch-specific settings for performance, defining the project path for module imports, and establishing global parameters and file paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba522b06-3bb8-4944-88b5-ec728a3dd895",
   "metadata": {},
   "source": [
    "### 1.1. Configure Environment\n",
    "This sub-section configures environment variables for CPU usage optimization and sets up PyTorch-specific thread management. It also ensures the project's root directory is added to the system path for proper module imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf49df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch threads: 16\n",
      "PyTorch interop threads: 16\n",
      "Project root added to sys.path: C:\\Users\\Acer\\Desktop\\Projects for Data Science\\Drug Gi50 Value Prediction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# General CPU Usage Optimization\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"16\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"16\"\n",
    "\n",
    "# PyTorch-specific CPU Usage Optimization (if not using GPU exclusively)\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    torch.set_num_threads(16)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: Could not set torch.set_num_threads.\\n{e}\")\n",
    "\n",
    "try:\n",
    "    torch.set_num_interop_threads(16)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: Could not set torch.set_num_interop_threads.\\n{e}\")\n",
    "\n",
    "print(f\"PyTorch threads: {torch.get_num_threads()}\")\n",
    "print(f\"PyTorch interop threads: {torch.get_num_interop_threads()}\")\n",
    "\n",
    "\n",
    "# Configure Project Path\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate up to the project root directory\n",
    "project_root = Path(current_dir).parent.resolve()\n",
    "\n",
    "# Add the project root to sys.path if it's not already there\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"Project root added to sys.path: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecfe5b7",
   "metadata": {},
   "source": [
    "### 1.2. Import Libraries\n",
    "All required Python libraries for data manipulation, molecular handling, PyTorch core functionalities, PyTorch Geometric, machine learning utilities, hyperparameter optimization, and general utilities are imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10e8a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tqdm.notebook found and enabled for pandas.\n"
     ]
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "from datetime import datetime\n",
    "import subprocess  # For getting Git commit ID\n",
    "\n",
    "# Core Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Molecular Handling (RDKit) Libraries\n",
    "from rdkit import Chem  # For basic molecule handling\n",
    "from rdkit.Chem import (\n",
    "    AllChem,\n",
    ")  # For atom features like Gasteiger charges, and other utilities\n",
    "\n",
    "# PyTorch Core Libraries\n",
    "import torch.nn as nn  # Neural network modules like Linear, ReLU, MSELoss\n",
    "import torch.nn.functional as F  # Functional interface for activations, e.g. F.ReLU\n",
    "import torch.optim as optim  # Optimization functions like Adam, AdamW, etc.\n",
    "from torch.optim import lr_scheduler  # Learning rate scheduling\n",
    "\n",
    "# PyTorch Geometric (PyG) Libraries\n",
    "from torch_geometric.data import Data  # The graph data object in PyG\n",
    "from torch_geometric.loader import (\n",
    "    DataLoader as PyGDataLoader,\n",
    ")  # PyG DataLoader for graphs\n",
    "import torch_geometric.nn as pyg_nn  # Common GNN layers (e.g., GCNConv, GraphSAGEConv)\n",
    "import torch_geometric.utils as pyg_utils  # Utility functions for graph manipulation\n",
    "\n",
    "# Machine Learning Utilities\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # For model evaluation metrics\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    ")  # For feature scaling\n",
    "\n",
    "# Hyperparameter Optimization Libraries\n",
    "import optuna\n",
    "\n",
    "# Conditional import for progress bars (tqdm)\n",
    "tqdm_notebook_available = False  # Initialize flag\n",
    "try:\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    tqdm.pandas()  # Enable tqdm for pandas apply\n",
    "    tqdm_notebook_available = True\n",
    "    print(\"tqdm.notebook found and enabled for pandas.\")\n",
    "except ImportError:\n",
    "    print(\"tqdm.notebook not found. Install with 'pip install tqdm'.\")\n",
    "\n",
    "# Local Project Imports\n",
    "from src.models.gnn_models import GNN  # Import your custom GNN model class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afdaa5-b8f4-4c1c-9231-edf19fbc344c",
   "metadata": {},
   "source": [
    "### 1.3. Define Device (GPU/CPU)\n",
    "This sub-section defines the computational device (GPU if available, otherwise CPU) for PyTorch operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "385334b2-5c91-4353-a650-b5e6c9b74825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939c1c3-4217-4c59-837e-0f59067088b6",
   "metadata": {},
   "source": [
    "### 1.4. Set Final Model Save Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efd70cd4-d396-4621-a342-7d6876c6f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best final GNN model will be saved in: ..\\models\\gnn\n"
     ]
    }
   ],
   "source": [
    "gnn_models_base_dir = Path(\"../models/gnn\")\n",
    "gnn_models_base_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"The best final GNN model will be saved in: {gnn_models_base_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af314e-5d67-4625-a656-cfb57a00d6e4",
   "metadata": {},
   "source": [
    "## 2. Load Data Splits\n",
    "This section loads the pre-engineered and split datasets (training, validation, and test sets for both features and target variable) that were prepared in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d4c581-37f9-42ef-a421-e67646669758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data splits from ..\\data\\splits...\n",
      "Data splits loaded successfully.\n",
      "X_train shape: (13119, 2268)\n",
      "X_val shape: (2812, 2268)\n",
      "X_test shape: (2812, 2268)\n",
      "y_train shape: (13119, 1)\n",
      "y_val shape: (2812, 1)\n",
      "y_test shape: (2812, 1)\n",
      "\n",
      "First 5 rows of X_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molregno</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307646</td>\n",
       "      <td>COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C</td>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081122</td>\n",
       "      <td>COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC</td>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2199496</td>\n",
       "      <td>COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2221960</td>\n",
       "      <td>O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO</td>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2879093</td>\n",
       "      <td>Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...</td>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   molregno                                   canonical_smiles  \\\n",
       "0   2307646               COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C   \n",
       "1   2081122       COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC   \n",
       "2   2199496  COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...   \n",
       "3   2221960           O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO   \n",
       "4   2879093  Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...   \n",
       "\n",
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  ...  morgan_fp_2038  \\\n",
       "0        0.494176  0.476742  12.560000  328.371  ...               0   \n",
       "1        0.459195  0.604738  12.923077  353.374  ...               0   \n",
       "2       -0.173158  0.359463  15.909091  447.535  ...               0   \n",
       "3       -0.686457  0.479732  11.217391  375.222  ...               0   \n",
       "4       -3.116139  0.437556  16.121212  472.879  ...               0   \n",
       "\n",
       "   morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  morgan_fp_2042  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  morgan_fp_2046  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2047  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 2268 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of y_train:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pGI50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>5.734742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>7.164746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>4.928428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>6.882724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>6.094208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pGI50\n",
       "14387  5.734742\n",
       "12543  7.164746\n",
       "12810  4.928428\n",
       "13172  6.882724\n",
       "18712  6.094208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits_dir = Path(\"../data/splits\")\n",
    "print(f\"\\nLoading data splits from {splits_dir}...\")\n",
    "\n",
    "try:\n",
    "    X_train = pd.read_parquet(splits_dir / \"X_train.parquet\")\n",
    "    X_val = pd.read_parquet(splits_dir / \"X_val.parquet\")\n",
    "    X_test = pd.read_parquet(splits_dir / \"X_test.parquet\")\n",
    "    \n",
    "    y_train = pd.read_parquet(splits_dir / \"y_train.parquet\")\n",
    "    y_val = pd.read_parquet(splits_dir / \"y_val.parquet\")\n",
    "    y_test = pd.read_parquet(splits_dir / \"y_test.parquet\")\n",
    "    print(\"Data splits loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: One or more split files not found in '{splits_dir}'.\")\n",
    "    print(\"Please ensure you have run '02_Split_Features.ipynb' to generate and save the splits.\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Display first few rows to verify data\n",
    "print(\"\\nFirst 5 rows of X_train:\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of y_train:\")\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c86cd-f9d1-4b3f-ac55-99cae504d03a",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for GNN\n",
    "This section performs specific data preparation steps required to transform the molecular data into PyTorch Geometric graph objects, suitable for GNN input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf767878-0371-4b4e-acb6-63085a6ef138",
   "metadata": {},
   "source": [
    "### 3.1. Extract Global Features of Each Molecule\n",
    "This sub-section extracts global molecular features (i.e., all the RDKit descriptors and Morgan fingerprints) from the dataset, which will be incorporated into the graph objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d4eecff-86f8-4cdc-b34d-d67ac68c7702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Excracting global features of each molecule ---\n",
      "Identified 2266 global feature columns for GNN.\n",
      "Global feature columns: ['num_activities', 'MaxAbsEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MinEStateIndex', 'qed', 'SPS', 'MolWt', 'HeavyAtomMolWt', 'ExactMolWt', 'NumValenceElectrons', 'NumRadicalElectrons', 'MaxPartialCharge', 'MinPartialCharge', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'HallKierAlpha', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'SMR_VSA1', 'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'FractionCSP3', 'HeavyAtomCount', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAmideBonds', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumAromaticRings', 'NumAtomStereoCenters', 'NumBridgeheadAtoms', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumHeterocycles', 'NumRotatableBonds', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', 'NumSpiroAtoms', 'NumUnspecifiedAtomStereoCenters', 'Phi', 'RingCount', 'MolLogP', 'MolMR', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', 'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', 'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', 'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', 'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', 'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', 'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', 'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', 'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', 'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', 'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', 'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', 'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', 'fr_unbrch_alkane', 'fr_urea', 'morgan_fp_0', 'morgan_fp_1', 'morgan_fp_2', 'morgan_fp_3', 'morgan_fp_4', 'morgan_fp_5', 'morgan_fp_6', 'morgan_fp_7', 'morgan_fp_8', 'morgan_fp_9', 'morgan_fp_10', 'morgan_fp_11', 'morgan_fp_12', 'morgan_fp_13', 'morgan_fp_14', 'morgan_fp_15', 'morgan_fp_16', 'morgan_fp_17', 'morgan_fp_18', 'morgan_fp_19', 'morgan_fp_20', 'morgan_fp_21', 'morgan_fp_22', 'morgan_fp_23', 'morgan_fp_24', 'morgan_fp_25', 'morgan_fp_26', 'morgan_fp_27', 'morgan_fp_28', 'morgan_fp_29', 'morgan_fp_30', 'morgan_fp_31', 'morgan_fp_32', 'morgan_fp_33', 'morgan_fp_34', 'morgan_fp_35', 'morgan_fp_36', 'morgan_fp_37', 'morgan_fp_38', 'morgan_fp_39', 'morgan_fp_40', 'morgan_fp_41', 'morgan_fp_42', 'morgan_fp_43', 'morgan_fp_44', 'morgan_fp_45', 'morgan_fp_46', 'morgan_fp_47', 'morgan_fp_48', 'morgan_fp_49', 'morgan_fp_50', 'morgan_fp_51', 'morgan_fp_52', 'morgan_fp_53', 'morgan_fp_54', 'morgan_fp_55', 'morgan_fp_56', 'morgan_fp_57', 'morgan_fp_58', 'morgan_fp_59', 'morgan_fp_60', 'morgan_fp_61', 'morgan_fp_62', 'morgan_fp_63', 'morgan_fp_64', 'morgan_fp_65', 'morgan_fp_66', 'morgan_fp_67', 'morgan_fp_68', 'morgan_fp_69', 'morgan_fp_70', 'morgan_fp_71', 'morgan_fp_72', 'morgan_fp_73', 'morgan_fp_74', 'morgan_fp_75', 'morgan_fp_76', 'morgan_fp_77', 'morgan_fp_78', 'morgan_fp_79', 'morgan_fp_80', 'morgan_fp_81', 'morgan_fp_82', 'morgan_fp_83', 'morgan_fp_84', 'morgan_fp_85', 'morgan_fp_86', 'morgan_fp_87', 'morgan_fp_88', 'morgan_fp_89', 'morgan_fp_90', 'morgan_fp_91', 'morgan_fp_92', 'morgan_fp_93', 'morgan_fp_94', 'morgan_fp_95', 'morgan_fp_96', 'morgan_fp_97', 'morgan_fp_98', 'morgan_fp_99', 'morgan_fp_100', 'morgan_fp_101', 'morgan_fp_102', 'morgan_fp_103', 'morgan_fp_104', 'morgan_fp_105', 'morgan_fp_106', 'morgan_fp_107', 'morgan_fp_108', 'morgan_fp_109', 'morgan_fp_110', 'morgan_fp_111', 'morgan_fp_112', 'morgan_fp_113', 'morgan_fp_114', 'morgan_fp_115', 'morgan_fp_116', 'morgan_fp_117', 'morgan_fp_118', 'morgan_fp_119', 'morgan_fp_120', 'morgan_fp_121', 'morgan_fp_122', 'morgan_fp_123', 'morgan_fp_124', 'morgan_fp_125', 'morgan_fp_126', 'morgan_fp_127', 'morgan_fp_128', 'morgan_fp_129', 'morgan_fp_130', 'morgan_fp_131', 'morgan_fp_132', 'morgan_fp_133', 'morgan_fp_134', 'morgan_fp_135', 'morgan_fp_136', 'morgan_fp_137', 'morgan_fp_138', 'morgan_fp_139', 'morgan_fp_140', 'morgan_fp_141', 'morgan_fp_142', 'morgan_fp_143', 'morgan_fp_144', 'morgan_fp_145', 'morgan_fp_146', 'morgan_fp_147', 'morgan_fp_148', 'morgan_fp_149', 'morgan_fp_150', 'morgan_fp_151', 'morgan_fp_152', 'morgan_fp_153', 'morgan_fp_154', 'morgan_fp_155', 'morgan_fp_156', 'morgan_fp_157', 'morgan_fp_158', 'morgan_fp_159', 'morgan_fp_160', 'morgan_fp_161', 'morgan_fp_162', 'morgan_fp_163', 'morgan_fp_164', 'morgan_fp_165', 'morgan_fp_166', 'morgan_fp_167', 'morgan_fp_168', 'morgan_fp_169', 'morgan_fp_170', 'morgan_fp_171', 'morgan_fp_172', 'morgan_fp_173', 'morgan_fp_174', 'morgan_fp_175', 'morgan_fp_176', 'morgan_fp_177', 'morgan_fp_178', 'morgan_fp_179', 'morgan_fp_180', 'morgan_fp_181', 'morgan_fp_182', 'morgan_fp_183', 'morgan_fp_184', 'morgan_fp_185', 'morgan_fp_186', 'morgan_fp_187', 'morgan_fp_188', 'morgan_fp_189', 'morgan_fp_190', 'morgan_fp_191', 'morgan_fp_192', 'morgan_fp_193', 'morgan_fp_194', 'morgan_fp_195', 'morgan_fp_196', 'morgan_fp_197', 'morgan_fp_198', 'morgan_fp_199', 'morgan_fp_200', 'morgan_fp_201', 'morgan_fp_202', 'morgan_fp_203', 'morgan_fp_204', 'morgan_fp_205', 'morgan_fp_206', 'morgan_fp_207', 'morgan_fp_208', 'morgan_fp_209', 'morgan_fp_210', 'morgan_fp_211', 'morgan_fp_212', 'morgan_fp_213', 'morgan_fp_214', 'morgan_fp_215', 'morgan_fp_216', 'morgan_fp_217', 'morgan_fp_218', 'morgan_fp_219', 'morgan_fp_220', 'morgan_fp_221', 'morgan_fp_222', 'morgan_fp_223', 'morgan_fp_224', 'morgan_fp_225', 'morgan_fp_226', 'morgan_fp_227', 'morgan_fp_228', 'morgan_fp_229', 'morgan_fp_230', 'morgan_fp_231', 'morgan_fp_232', 'morgan_fp_233', 'morgan_fp_234', 'morgan_fp_235', 'morgan_fp_236', 'morgan_fp_237', 'morgan_fp_238', 'morgan_fp_239', 'morgan_fp_240', 'morgan_fp_241', 'morgan_fp_242', 'morgan_fp_243', 'morgan_fp_244', 'morgan_fp_245', 'morgan_fp_246', 'morgan_fp_247', 'morgan_fp_248', 'morgan_fp_249', 'morgan_fp_250', 'morgan_fp_251', 'morgan_fp_252', 'morgan_fp_253', 'morgan_fp_254', 'morgan_fp_255', 'morgan_fp_256', 'morgan_fp_257', 'morgan_fp_258', 'morgan_fp_259', 'morgan_fp_260', 'morgan_fp_261', 'morgan_fp_262', 'morgan_fp_263', 'morgan_fp_264', 'morgan_fp_265', 'morgan_fp_266', 'morgan_fp_267', 'morgan_fp_268', 'morgan_fp_269', 'morgan_fp_270', 'morgan_fp_271', 'morgan_fp_272', 'morgan_fp_273', 'morgan_fp_274', 'morgan_fp_275', 'morgan_fp_276', 'morgan_fp_277', 'morgan_fp_278', 'morgan_fp_279', 'morgan_fp_280', 'morgan_fp_281', 'morgan_fp_282', 'morgan_fp_283', 'morgan_fp_284', 'morgan_fp_285', 'morgan_fp_286', 'morgan_fp_287', 'morgan_fp_288', 'morgan_fp_289', 'morgan_fp_290', 'morgan_fp_291', 'morgan_fp_292', 'morgan_fp_293', 'morgan_fp_294', 'morgan_fp_295', 'morgan_fp_296', 'morgan_fp_297', 'morgan_fp_298', 'morgan_fp_299', 'morgan_fp_300', 'morgan_fp_301', 'morgan_fp_302', 'morgan_fp_303', 'morgan_fp_304', 'morgan_fp_305', 'morgan_fp_306', 'morgan_fp_307', 'morgan_fp_308', 'morgan_fp_309', 'morgan_fp_310', 'morgan_fp_311', 'morgan_fp_312', 'morgan_fp_313', 'morgan_fp_314', 'morgan_fp_315', 'morgan_fp_316', 'morgan_fp_317', 'morgan_fp_318', 'morgan_fp_319', 'morgan_fp_320', 'morgan_fp_321', 'morgan_fp_322', 'morgan_fp_323', 'morgan_fp_324', 'morgan_fp_325', 'morgan_fp_326', 'morgan_fp_327', 'morgan_fp_328', 'morgan_fp_329', 'morgan_fp_330', 'morgan_fp_331', 'morgan_fp_332', 'morgan_fp_333', 'morgan_fp_334', 'morgan_fp_335', 'morgan_fp_336', 'morgan_fp_337', 'morgan_fp_338', 'morgan_fp_339', 'morgan_fp_340', 'morgan_fp_341', 'morgan_fp_342', 'morgan_fp_343', 'morgan_fp_344', 'morgan_fp_345', 'morgan_fp_346', 'morgan_fp_347', 'morgan_fp_348', 'morgan_fp_349', 'morgan_fp_350', 'morgan_fp_351', 'morgan_fp_352', 'morgan_fp_353', 'morgan_fp_354', 'morgan_fp_355', 'morgan_fp_356', 'morgan_fp_357', 'morgan_fp_358', 'morgan_fp_359', 'morgan_fp_360', 'morgan_fp_361', 'morgan_fp_362', 'morgan_fp_363', 'morgan_fp_364', 'morgan_fp_365', 'morgan_fp_366', 'morgan_fp_367', 'morgan_fp_368', 'morgan_fp_369', 'morgan_fp_370', 'morgan_fp_371', 'morgan_fp_372', 'morgan_fp_373', 'morgan_fp_374', 'morgan_fp_375', 'morgan_fp_376', 'morgan_fp_377', 'morgan_fp_378', 'morgan_fp_379', 'morgan_fp_380', 'morgan_fp_381', 'morgan_fp_382', 'morgan_fp_383', 'morgan_fp_384', 'morgan_fp_385', 'morgan_fp_386', 'morgan_fp_387', 'morgan_fp_388', 'morgan_fp_389', 'morgan_fp_390', 'morgan_fp_391', 'morgan_fp_392', 'morgan_fp_393', 'morgan_fp_394', 'morgan_fp_395', 'morgan_fp_396', 'morgan_fp_397', 'morgan_fp_398', 'morgan_fp_399', 'morgan_fp_400', 'morgan_fp_401', 'morgan_fp_402', 'morgan_fp_403', 'morgan_fp_404', 'morgan_fp_405', 'morgan_fp_406', 'morgan_fp_407', 'morgan_fp_408', 'morgan_fp_409', 'morgan_fp_410', 'morgan_fp_411', 'morgan_fp_412', 'morgan_fp_413', 'morgan_fp_414', 'morgan_fp_415', 'morgan_fp_416', 'morgan_fp_417', 'morgan_fp_418', 'morgan_fp_419', 'morgan_fp_420', 'morgan_fp_421', 'morgan_fp_422', 'morgan_fp_423', 'morgan_fp_424', 'morgan_fp_425', 'morgan_fp_426', 'morgan_fp_427', 'morgan_fp_428', 'morgan_fp_429', 'morgan_fp_430', 'morgan_fp_431', 'morgan_fp_432', 'morgan_fp_433', 'morgan_fp_434', 'morgan_fp_435', 'morgan_fp_436', 'morgan_fp_437', 'morgan_fp_438', 'morgan_fp_439', 'morgan_fp_440', 'morgan_fp_441', 'morgan_fp_442', 'morgan_fp_443', 'morgan_fp_444', 'morgan_fp_445', 'morgan_fp_446', 'morgan_fp_447', 'morgan_fp_448', 'morgan_fp_449', 'morgan_fp_450', 'morgan_fp_451', 'morgan_fp_452', 'morgan_fp_453', 'morgan_fp_454', 'morgan_fp_455', 'morgan_fp_456', 'morgan_fp_457', 'morgan_fp_458', 'morgan_fp_459', 'morgan_fp_460', 'morgan_fp_461', 'morgan_fp_462', 'morgan_fp_463', 'morgan_fp_464', 'morgan_fp_465', 'morgan_fp_466', 'morgan_fp_467', 'morgan_fp_468', 'morgan_fp_469', 'morgan_fp_470', 'morgan_fp_471', 'morgan_fp_472', 'morgan_fp_473', 'morgan_fp_474', 'morgan_fp_475', 'morgan_fp_476', 'morgan_fp_477', 'morgan_fp_478', 'morgan_fp_479', 'morgan_fp_480', 'morgan_fp_481', 'morgan_fp_482', 'morgan_fp_483', 'morgan_fp_484', 'morgan_fp_485', 'morgan_fp_486', 'morgan_fp_487', 'morgan_fp_488', 'morgan_fp_489', 'morgan_fp_490', 'morgan_fp_491', 'morgan_fp_492', 'morgan_fp_493', 'morgan_fp_494', 'morgan_fp_495', 'morgan_fp_496', 'morgan_fp_497', 'morgan_fp_498', 'morgan_fp_499', 'morgan_fp_500', 'morgan_fp_501', 'morgan_fp_502', 'morgan_fp_503', 'morgan_fp_504', 'morgan_fp_505', 'morgan_fp_506', 'morgan_fp_507', 'morgan_fp_508', 'morgan_fp_509', 'morgan_fp_510', 'morgan_fp_511', 'morgan_fp_512', 'morgan_fp_513', 'morgan_fp_514', 'morgan_fp_515', 'morgan_fp_516', 'morgan_fp_517', 'morgan_fp_518', 'morgan_fp_519', 'morgan_fp_520', 'morgan_fp_521', 'morgan_fp_522', 'morgan_fp_523', 'morgan_fp_524', 'morgan_fp_525', 'morgan_fp_526', 'morgan_fp_527', 'morgan_fp_528', 'morgan_fp_529', 'morgan_fp_530', 'morgan_fp_531', 'morgan_fp_532', 'morgan_fp_533', 'morgan_fp_534', 'morgan_fp_535', 'morgan_fp_536', 'morgan_fp_537', 'morgan_fp_538', 'morgan_fp_539', 'morgan_fp_540', 'morgan_fp_541', 'morgan_fp_542', 'morgan_fp_543', 'morgan_fp_544', 'morgan_fp_545', 'morgan_fp_546', 'morgan_fp_547', 'morgan_fp_548', 'morgan_fp_549', 'morgan_fp_550', 'morgan_fp_551', 'morgan_fp_552', 'morgan_fp_553', 'morgan_fp_554', 'morgan_fp_555', 'morgan_fp_556', 'morgan_fp_557', 'morgan_fp_558', 'morgan_fp_559', 'morgan_fp_560', 'morgan_fp_561', 'morgan_fp_562', 'morgan_fp_563', 'morgan_fp_564', 'morgan_fp_565', 'morgan_fp_566', 'morgan_fp_567', 'morgan_fp_568', 'morgan_fp_569', 'morgan_fp_570', 'morgan_fp_571', 'morgan_fp_572', 'morgan_fp_573', 'morgan_fp_574', 'morgan_fp_575', 'morgan_fp_576', 'morgan_fp_577', 'morgan_fp_578', 'morgan_fp_579', 'morgan_fp_580', 'morgan_fp_581', 'morgan_fp_582', 'morgan_fp_583', 'morgan_fp_584', 'morgan_fp_585', 'morgan_fp_586', 'morgan_fp_587', 'morgan_fp_588', 'morgan_fp_589', 'morgan_fp_590', 'morgan_fp_591', 'morgan_fp_592', 'morgan_fp_593', 'morgan_fp_594', 'morgan_fp_595', 'morgan_fp_596', 'morgan_fp_597', 'morgan_fp_598', 'morgan_fp_599', 'morgan_fp_600', 'morgan_fp_601', 'morgan_fp_602', 'morgan_fp_603', 'morgan_fp_604', 'morgan_fp_605', 'morgan_fp_606', 'morgan_fp_607', 'morgan_fp_608', 'morgan_fp_609', 'morgan_fp_610', 'morgan_fp_611', 'morgan_fp_612', 'morgan_fp_613', 'morgan_fp_614', 'morgan_fp_615', 'morgan_fp_616', 'morgan_fp_617', 'morgan_fp_618', 'morgan_fp_619', 'morgan_fp_620', 'morgan_fp_621', 'morgan_fp_622', 'morgan_fp_623', 'morgan_fp_624', 'morgan_fp_625', 'morgan_fp_626', 'morgan_fp_627', 'morgan_fp_628', 'morgan_fp_629', 'morgan_fp_630', 'morgan_fp_631', 'morgan_fp_632', 'morgan_fp_633', 'morgan_fp_634', 'morgan_fp_635', 'morgan_fp_636', 'morgan_fp_637', 'morgan_fp_638', 'morgan_fp_639', 'morgan_fp_640', 'morgan_fp_641', 'morgan_fp_642', 'morgan_fp_643', 'morgan_fp_644', 'morgan_fp_645', 'morgan_fp_646', 'morgan_fp_647', 'morgan_fp_648', 'morgan_fp_649', 'morgan_fp_650', 'morgan_fp_651', 'morgan_fp_652', 'morgan_fp_653', 'morgan_fp_654', 'morgan_fp_655', 'morgan_fp_656', 'morgan_fp_657', 'morgan_fp_658', 'morgan_fp_659', 'morgan_fp_660', 'morgan_fp_661', 'morgan_fp_662', 'morgan_fp_663', 'morgan_fp_664', 'morgan_fp_665', 'morgan_fp_666', 'morgan_fp_667', 'morgan_fp_668', 'morgan_fp_669', 'morgan_fp_670', 'morgan_fp_671', 'morgan_fp_672', 'morgan_fp_673', 'morgan_fp_674', 'morgan_fp_675', 'morgan_fp_676', 'morgan_fp_677', 'morgan_fp_678', 'morgan_fp_679', 'morgan_fp_680', 'morgan_fp_681', 'morgan_fp_682', 'morgan_fp_683', 'morgan_fp_684', 'morgan_fp_685', 'morgan_fp_686', 'morgan_fp_687', 'morgan_fp_688', 'morgan_fp_689', 'morgan_fp_690', 'morgan_fp_691', 'morgan_fp_692', 'morgan_fp_693', 'morgan_fp_694', 'morgan_fp_695', 'morgan_fp_696', 'morgan_fp_697', 'morgan_fp_698', 'morgan_fp_699', 'morgan_fp_700', 'morgan_fp_701', 'morgan_fp_702', 'morgan_fp_703', 'morgan_fp_704', 'morgan_fp_705', 'morgan_fp_706', 'morgan_fp_707', 'morgan_fp_708', 'morgan_fp_709', 'morgan_fp_710', 'morgan_fp_711', 'morgan_fp_712', 'morgan_fp_713', 'morgan_fp_714', 'morgan_fp_715', 'morgan_fp_716', 'morgan_fp_717', 'morgan_fp_718', 'morgan_fp_719', 'morgan_fp_720', 'morgan_fp_721', 'morgan_fp_722', 'morgan_fp_723', 'morgan_fp_724', 'morgan_fp_725', 'morgan_fp_726', 'morgan_fp_727', 'morgan_fp_728', 'morgan_fp_729', 'morgan_fp_730', 'morgan_fp_731', 'morgan_fp_732', 'morgan_fp_733', 'morgan_fp_734', 'morgan_fp_735', 'morgan_fp_736', 'morgan_fp_737', 'morgan_fp_738', 'morgan_fp_739', 'morgan_fp_740', 'morgan_fp_741', 'morgan_fp_742', 'morgan_fp_743', 'morgan_fp_744', 'morgan_fp_745', 'morgan_fp_746', 'morgan_fp_747', 'morgan_fp_748', 'morgan_fp_749', 'morgan_fp_750', 'morgan_fp_751', 'morgan_fp_752', 'morgan_fp_753', 'morgan_fp_754', 'morgan_fp_755', 'morgan_fp_756', 'morgan_fp_757', 'morgan_fp_758', 'morgan_fp_759', 'morgan_fp_760', 'morgan_fp_761', 'morgan_fp_762', 'morgan_fp_763', 'morgan_fp_764', 'morgan_fp_765', 'morgan_fp_766', 'morgan_fp_767', 'morgan_fp_768', 'morgan_fp_769', 'morgan_fp_770', 'morgan_fp_771', 'morgan_fp_772', 'morgan_fp_773', 'morgan_fp_774', 'morgan_fp_775', 'morgan_fp_776', 'morgan_fp_777', 'morgan_fp_778', 'morgan_fp_779', 'morgan_fp_780', 'morgan_fp_781', 'morgan_fp_782', 'morgan_fp_783', 'morgan_fp_784', 'morgan_fp_785', 'morgan_fp_786', 'morgan_fp_787', 'morgan_fp_788', 'morgan_fp_789', 'morgan_fp_790', 'morgan_fp_791', 'morgan_fp_792', 'morgan_fp_793', 'morgan_fp_794', 'morgan_fp_795', 'morgan_fp_796', 'morgan_fp_797', 'morgan_fp_798', 'morgan_fp_799', 'morgan_fp_800', 'morgan_fp_801', 'morgan_fp_802', 'morgan_fp_803', 'morgan_fp_804', 'morgan_fp_805', 'morgan_fp_806', 'morgan_fp_807', 'morgan_fp_808', 'morgan_fp_809', 'morgan_fp_810', 'morgan_fp_811', 'morgan_fp_812', 'morgan_fp_813', 'morgan_fp_814', 'morgan_fp_815', 'morgan_fp_816', 'morgan_fp_817', 'morgan_fp_818', 'morgan_fp_819', 'morgan_fp_820', 'morgan_fp_821', 'morgan_fp_822', 'morgan_fp_823', 'morgan_fp_824', 'morgan_fp_825', 'morgan_fp_826', 'morgan_fp_827', 'morgan_fp_828', 'morgan_fp_829', 'morgan_fp_830', 'morgan_fp_831', 'morgan_fp_832', 'morgan_fp_833', 'morgan_fp_834', 'morgan_fp_835', 'morgan_fp_836', 'morgan_fp_837', 'morgan_fp_838', 'morgan_fp_839', 'morgan_fp_840', 'morgan_fp_841', 'morgan_fp_842', 'morgan_fp_843', 'morgan_fp_844', 'morgan_fp_845', 'morgan_fp_846', 'morgan_fp_847', 'morgan_fp_848', 'morgan_fp_849', 'morgan_fp_850', 'morgan_fp_851', 'morgan_fp_852', 'morgan_fp_853', 'morgan_fp_854', 'morgan_fp_855', 'morgan_fp_856', 'morgan_fp_857', 'morgan_fp_858', 'morgan_fp_859', 'morgan_fp_860', 'morgan_fp_861', 'morgan_fp_862', 'morgan_fp_863', 'morgan_fp_864', 'morgan_fp_865', 'morgan_fp_866', 'morgan_fp_867', 'morgan_fp_868', 'morgan_fp_869', 'morgan_fp_870', 'morgan_fp_871', 'morgan_fp_872', 'morgan_fp_873', 'morgan_fp_874', 'morgan_fp_875', 'morgan_fp_876', 'morgan_fp_877', 'morgan_fp_878', 'morgan_fp_879', 'morgan_fp_880', 'morgan_fp_881', 'morgan_fp_882', 'morgan_fp_883', 'morgan_fp_884', 'morgan_fp_885', 'morgan_fp_886', 'morgan_fp_887', 'morgan_fp_888', 'morgan_fp_889', 'morgan_fp_890', 'morgan_fp_891', 'morgan_fp_892', 'morgan_fp_893', 'morgan_fp_894', 'morgan_fp_895', 'morgan_fp_896', 'morgan_fp_897', 'morgan_fp_898', 'morgan_fp_899', 'morgan_fp_900', 'morgan_fp_901', 'morgan_fp_902', 'morgan_fp_903', 'morgan_fp_904', 'morgan_fp_905', 'morgan_fp_906', 'morgan_fp_907', 'morgan_fp_908', 'morgan_fp_909', 'morgan_fp_910', 'morgan_fp_911', 'morgan_fp_912', 'morgan_fp_913', 'morgan_fp_914', 'morgan_fp_915', 'morgan_fp_916', 'morgan_fp_917', 'morgan_fp_918', 'morgan_fp_919', 'morgan_fp_920', 'morgan_fp_921', 'morgan_fp_922', 'morgan_fp_923', 'morgan_fp_924', 'morgan_fp_925', 'morgan_fp_926', 'morgan_fp_927', 'morgan_fp_928', 'morgan_fp_929', 'morgan_fp_930', 'morgan_fp_931', 'morgan_fp_932', 'morgan_fp_933', 'morgan_fp_934', 'morgan_fp_935', 'morgan_fp_936', 'morgan_fp_937', 'morgan_fp_938', 'morgan_fp_939', 'morgan_fp_940', 'morgan_fp_941', 'morgan_fp_942', 'morgan_fp_943', 'morgan_fp_944', 'morgan_fp_945', 'morgan_fp_946', 'morgan_fp_947', 'morgan_fp_948', 'morgan_fp_949', 'morgan_fp_950', 'morgan_fp_951', 'morgan_fp_952', 'morgan_fp_953', 'morgan_fp_954', 'morgan_fp_955', 'morgan_fp_956', 'morgan_fp_957', 'morgan_fp_958', 'morgan_fp_959', 'morgan_fp_960', 'morgan_fp_961', 'morgan_fp_962', 'morgan_fp_963', 'morgan_fp_964', 'morgan_fp_965', 'morgan_fp_966', 'morgan_fp_967', 'morgan_fp_968', 'morgan_fp_969', 'morgan_fp_970', 'morgan_fp_971', 'morgan_fp_972', 'morgan_fp_973', 'morgan_fp_974', 'morgan_fp_975', 'morgan_fp_976', 'morgan_fp_977', 'morgan_fp_978', 'morgan_fp_979', 'morgan_fp_980', 'morgan_fp_981', 'morgan_fp_982', 'morgan_fp_983', 'morgan_fp_984', 'morgan_fp_985', 'morgan_fp_986', 'morgan_fp_987', 'morgan_fp_988', 'morgan_fp_989', 'morgan_fp_990', 'morgan_fp_991', 'morgan_fp_992', 'morgan_fp_993', 'morgan_fp_994', 'morgan_fp_995', 'morgan_fp_996', 'morgan_fp_997', 'morgan_fp_998', 'morgan_fp_999', 'morgan_fp_1000', 'morgan_fp_1001', 'morgan_fp_1002', 'morgan_fp_1003', 'morgan_fp_1004', 'morgan_fp_1005', 'morgan_fp_1006', 'morgan_fp_1007', 'morgan_fp_1008', 'morgan_fp_1009', 'morgan_fp_1010', 'morgan_fp_1011', 'morgan_fp_1012', 'morgan_fp_1013', 'morgan_fp_1014', 'morgan_fp_1015', 'morgan_fp_1016', 'morgan_fp_1017', 'morgan_fp_1018', 'morgan_fp_1019', 'morgan_fp_1020', 'morgan_fp_1021', 'morgan_fp_1022', 'morgan_fp_1023', 'morgan_fp_1024', 'morgan_fp_1025', 'morgan_fp_1026', 'morgan_fp_1027', 'morgan_fp_1028', 'morgan_fp_1029', 'morgan_fp_1030', 'morgan_fp_1031', 'morgan_fp_1032', 'morgan_fp_1033', 'morgan_fp_1034', 'morgan_fp_1035', 'morgan_fp_1036', 'morgan_fp_1037', 'morgan_fp_1038', 'morgan_fp_1039', 'morgan_fp_1040', 'morgan_fp_1041', 'morgan_fp_1042', 'morgan_fp_1043', 'morgan_fp_1044', 'morgan_fp_1045', 'morgan_fp_1046', 'morgan_fp_1047', 'morgan_fp_1048', 'morgan_fp_1049', 'morgan_fp_1050', 'morgan_fp_1051', 'morgan_fp_1052', 'morgan_fp_1053', 'morgan_fp_1054', 'morgan_fp_1055', 'morgan_fp_1056', 'morgan_fp_1057', 'morgan_fp_1058', 'morgan_fp_1059', 'morgan_fp_1060', 'morgan_fp_1061', 'morgan_fp_1062', 'morgan_fp_1063', 'morgan_fp_1064', 'morgan_fp_1065', 'morgan_fp_1066', 'morgan_fp_1067', 'morgan_fp_1068', 'morgan_fp_1069', 'morgan_fp_1070', 'morgan_fp_1071', 'morgan_fp_1072', 'morgan_fp_1073', 'morgan_fp_1074', 'morgan_fp_1075', 'morgan_fp_1076', 'morgan_fp_1077', 'morgan_fp_1078', 'morgan_fp_1079', 'morgan_fp_1080', 'morgan_fp_1081', 'morgan_fp_1082', 'morgan_fp_1083', 'morgan_fp_1084', 'morgan_fp_1085', 'morgan_fp_1086', 'morgan_fp_1087', 'morgan_fp_1088', 'morgan_fp_1089', 'morgan_fp_1090', 'morgan_fp_1091', 'morgan_fp_1092', 'morgan_fp_1093', 'morgan_fp_1094', 'morgan_fp_1095', 'morgan_fp_1096', 'morgan_fp_1097', 'morgan_fp_1098', 'morgan_fp_1099', 'morgan_fp_1100', 'morgan_fp_1101', 'morgan_fp_1102', 'morgan_fp_1103', 'morgan_fp_1104', 'morgan_fp_1105', 'morgan_fp_1106', 'morgan_fp_1107', 'morgan_fp_1108', 'morgan_fp_1109', 'morgan_fp_1110', 'morgan_fp_1111', 'morgan_fp_1112', 'morgan_fp_1113', 'morgan_fp_1114', 'morgan_fp_1115', 'morgan_fp_1116', 'morgan_fp_1117', 'morgan_fp_1118', 'morgan_fp_1119', 'morgan_fp_1120', 'morgan_fp_1121', 'morgan_fp_1122', 'morgan_fp_1123', 'morgan_fp_1124', 'morgan_fp_1125', 'morgan_fp_1126', 'morgan_fp_1127', 'morgan_fp_1128', 'morgan_fp_1129', 'morgan_fp_1130', 'morgan_fp_1131', 'morgan_fp_1132', 'morgan_fp_1133', 'morgan_fp_1134', 'morgan_fp_1135', 'morgan_fp_1136', 'morgan_fp_1137', 'morgan_fp_1138', 'morgan_fp_1139', 'morgan_fp_1140', 'morgan_fp_1141', 'morgan_fp_1142', 'morgan_fp_1143', 'morgan_fp_1144', 'morgan_fp_1145', 'morgan_fp_1146', 'morgan_fp_1147', 'morgan_fp_1148', 'morgan_fp_1149', 'morgan_fp_1150', 'morgan_fp_1151', 'morgan_fp_1152', 'morgan_fp_1153', 'morgan_fp_1154', 'morgan_fp_1155', 'morgan_fp_1156', 'morgan_fp_1157', 'morgan_fp_1158', 'morgan_fp_1159', 'morgan_fp_1160', 'morgan_fp_1161', 'morgan_fp_1162', 'morgan_fp_1163', 'morgan_fp_1164', 'morgan_fp_1165', 'morgan_fp_1166', 'morgan_fp_1167', 'morgan_fp_1168', 'morgan_fp_1169', 'morgan_fp_1170', 'morgan_fp_1171', 'morgan_fp_1172', 'morgan_fp_1173', 'morgan_fp_1174', 'morgan_fp_1175', 'morgan_fp_1176', 'morgan_fp_1177', 'morgan_fp_1178', 'morgan_fp_1179', 'morgan_fp_1180', 'morgan_fp_1181', 'morgan_fp_1182', 'morgan_fp_1183', 'morgan_fp_1184', 'morgan_fp_1185', 'morgan_fp_1186', 'morgan_fp_1187', 'morgan_fp_1188', 'morgan_fp_1189', 'morgan_fp_1190', 'morgan_fp_1191', 'morgan_fp_1192', 'morgan_fp_1193', 'morgan_fp_1194', 'morgan_fp_1195', 'morgan_fp_1196', 'morgan_fp_1197', 'morgan_fp_1198', 'morgan_fp_1199', 'morgan_fp_1200', 'morgan_fp_1201', 'morgan_fp_1202', 'morgan_fp_1203', 'morgan_fp_1204', 'morgan_fp_1205', 'morgan_fp_1206', 'morgan_fp_1207', 'morgan_fp_1208', 'morgan_fp_1209', 'morgan_fp_1210', 'morgan_fp_1211', 'morgan_fp_1212', 'morgan_fp_1213', 'morgan_fp_1214', 'morgan_fp_1215', 'morgan_fp_1216', 'morgan_fp_1217', 'morgan_fp_1218', 'morgan_fp_1219', 'morgan_fp_1220', 'morgan_fp_1221', 'morgan_fp_1222', 'morgan_fp_1223', 'morgan_fp_1224', 'morgan_fp_1225', 'morgan_fp_1226', 'morgan_fp_1227', 'morgan_fp_1228', 'morgan_fp_1229', 'morgan_fp_1230', 'morgan_fp_1231', 'morgan_fp_1232', 'morgan_fp_1233', 'morgan_fp_1234', 'morgan_fp_1235', 'morgan_fp_1236', 'morgan_fp_1237', 'morgan_fp_1238', 'morgan_fp_1239', 'morgan_fp_1240', 'morgan_fp_1241', 'morgan_fp_1242', 'morgan_fp_1243', 'morgan_fp_1244', 'morgan_fp_1245', 'morgan_fp_1246', 'morgan_fp_1247', 'morgan_fp_1248', 'morgan_fp_1249', 'morgan_fp_1250', 'morgan_fp_1251', 'morgan_fp_1252', 'morgan_fp_1253', 'morgan_fp_1254', 'morgan_fp_1255', 'morgan_fp_1256', 'morgan_fp_1257', 'morgan_fp_1258', 'morgan_fp_1259', 'morgan_fp_1260', 'morgan_fp_1261', 'morgan_fp_1262', 'morgan_fp_1263', 'morgan_fp_1264', 'morgan_fp_1265', 'morgan_fp_1266', 'morgan_fp_1267', 'morgan_fp_1268', 'morgan_fp_1269', 'morgan_fp_1270', 'morgan_fp_1271', 'morgan_fp_1272', 'morgan_fp_1273', 'morgan_fp_1274', 'morgan_fp_1275', 'morgan_fp_1276', 'morgan_fp_1277', 'morgan_fp_1278', 'morgan_fp_1279', 'morgan_fp_1280', 'morgan_fp_1281', 'morgan_fp_1282', 'morgan_fp_1283', 'morgan_fp_1284', 'morgan_fp_1285', 'morgan_fp_1286', 'morgan_fp_1287', 'morgan_fp_1288', 'morgan_fp_1289', 'morgan_fp_1290', 'morgan_fp_1291', 'morgan_fp_1292', 'morgan_fp_1293', 'morgan_fp_1294', 'morgan_fp_1295', 'morgan_fp_1296', 'morgan_fp_1297', 'morgan_fp_1298', 'morgan_fp_1299', 'morgan_fp_1300', 'morgan_fp_1301', 'morgan_fp_1302', 'morgan_fp_1303', 'morgan_fp_1304', 'morgan_fp_1305', 'morgan_fp_1306', 'morgan_fp_1307', 'morgan_fp_1308', 'morgan_fp_1309', 'morgan_fp_1310', 'morgan_fp_1311', 'morgan_fp_1312', 'morgan_fp_1313', 'morgan_fp_1314', 'morgan_fp_1315', 'morgan_fp_1316', 'morgan_fp_1317', 'morgan_fp_1318', 'morgan_fp_1319', 'morgan_fp_1320', 'morgan_fp_1321', 'morgan_fp_1322', 'morgan_fp_1323', 'morgan_fp_1324', 'morgan_fp_1325', 'morgan_fp_1326', 'morgan_fp_1327', 'morgan_fp_1328', 'morgan_fp_1329', 'morgan_fp_1330', 'morgan_fp_1331', 'morgan_fp_1332', 'morgan_fp_1333', 'morgan_fp_1334', 'morgan_fp_1335', 'morgan_fp_1336', 'morgan_fp_1337', 'morgan_fp_1338', 'morgan_fp_1339', 'morgan_fp_1340', 'morgan_fp_1341', 'morgan_fp_1342', 'morgan_fp_1343', 'morgan_fp_1344', 'morgan_fp_1345', 'morgan_fp_1346', 'morgan_fp_1347', 'morgan_fp_1348', 'morgan_fp_1349', 'morgan_fp_1350', 'morgan_fp_1351', 'morgan_fp_1352', 'morgan_fp_1353', 'morgan_fp_1354', 'morgan_fp_1355', 'morgan_fp_1356', 'morgan_fp_1357', 'morgan_fp_1358', 'morgan_fp_1359', 'morgan_fp_1360', 'morgan_fp_1361', 'morgan_fp_1362', 'morgan_fp_1363', 'morgan_fp_1364', 'morgan_fp_1365', 'morgan_fp_1366', 'morgan_fp_1367', 'morgan_fp_1368', 'morgan_fp_1369', 'morgan_fp_1370', 'morgan_fp_1371', 'morgan_fp_1372', 'morgan_fp_1373', 'morgan_fp_1374', 'morgan_fp_1375', 'morgan_fp_1376', 'morgan_fp_1377', 'morgan_fp_1378', 'morgan_fp_1379', 'morgan_fp_1380', 'morgan_fp_1381', 'morgan_fp_1382', 'morgan_fp_1383', 'morgan_fp_1384', 'morgan_fp_1385', 'morgan_fp_1386', 'morgan_fp_1387', 'morgan_fp_1388', 'morgan_fp_1389', 'morgan_fp_1390', 'morgan_fp_1391', 'morgan_fp_1392', 'morgan_fp_1393', 'morgan_fp_1394', 'morgan_fp_1395', 'morgan_fp_1396', 'morgan_fp_1397', 'morgan_fp_1398', 'morgan_fp_1399', 'morgan_fp_1400', 'morgan_fp_1401', 'morgan_fp_1402', 'morgan_fp_1403', 'morgan_fp_1404', 'morgan_fp_1405', 'morgan_fp_1406', 'morgan_fp_1407', 'morgan_fp_1408', 'morgan_fp_1409', 'morgan_fp_1410', 'morgan_fp_1411', 'morgan_fp_1412', 'morgan_fp_1413', 'morgan_fp_1414', 'morgan_fp_1415', 'morgan_fp_1416', 'morgan_fp_1417', 'morgan_fp_1418', 'morgan_fp_1419', 'morgan_fp_1420', 'morgan_fp_1421', 'morgan_fp_1422', 'morgan_fp_1423', 'morgan_fp_1424', 'morgan_fp_1425', 'morgan_fp_1426', 'morgan_fp_1427', 'morgan_fp_1428', 'morgan_fp_1429', 'morgan_fp_1430', 'morgan_fp_1431', 'morgan_fp_1432', 'morgan_fp_1433', 'morgan_fp_1434', 'morgan_fp_1435', 'morgan_fp_1436', 'morgan_fp_1437', 'morgan_fp_1438', 'morgan_fp_1439', 'morgan_fp_1440', 'morgan_fp_1441', 'morgan_fp_1442', 'morgan_fp_1443', 'morgan_fp_1444', 'morgan_fp_1445', 'morgan_fp_1446', 'morgan_fp_1447', 'morgan_fp_1448', 'morgan_fp_1449', 'morgan_fp_1450', 'morgan_fp_1451', 'morgan_fp_1452', 'morgan_fp_1453', 'morgan_fp_1454', 'morgan_fp_1455', 'morgan_fp_1456', 'morgan_fp_1457', 'morgan_fp_1458', 'morgan_fp_1459', 'morgan_fp_1460', 'morgan_fp_1461', 'morgan_fp_1462', 'morgan_fp_1463', 'morgan_fp_1464', 'morgan_fp_1465', 'morgan_fp_1466', 'morgan_fp_1467', 'morgan_fp_1468', 'morgan_fp_1469', 'morgan_fp_1470', 'morgan_fp_1471', 'morgan_fp_1472', 'morgan_fp_1473', 'morgan_fp_1474', 'morgan_fp_1475', 'morgan_fp_1476', 'morgan_fp_1477', 'morgan_fp_1478', 'morgan_fp_1479', 'morgan_fp_1480', 'morgan_fp_1481', 'morgan_fp_1482', 'morgan_fp_1483', 'morgan_fp_1484', 'morgan_fp_1485', 'morgan_fp_1486', 'morgan_fp_1487', 'morgan_fp_1488', 'morgan_fp_1489', 'morgan_fp_1490', 'morgan_fp_1491', 'morgan_fp_1492', 'morgan_fp_1493', 'morgan_fp_1494', 'morgan_fp_1495', 'morgan_fp_1496', 'morgan_fp_1497', 'morgan_fp_1498', 'morgan_fp_1499', 'morgan_fp_1500', 'morgan_fp_1501', 'morgan_fp_1502', 'morgan_fp_1503', 'morgan_fp_1504', 'morgan_fp_1505', 'morgan_fp_1506', 'morgan_fp_1507', 'morgan_fp_1508', 'morgan_fp_1509', 'morgan_fp_1510', 'morgan_fp_1511', 'morgan_fp_1512', 'morgan_fp_1513', 'morgan_fp_1514', 'morgan_fp_1515', 'morgan_fp_1516', 'morgan_fp_1517', 'morgan_fp_1518', 'morgan_fp_1519', 'morgan_fp_1520', 'morgan_fp_1521', 'morgan_fp_1522', 'morgan_fp_1523', 'morgan_fp_1524', 'morgan_fp_1525', 'morgan_fp_1526', 'morgan_fp_1527', 'morgan_fp_1528', 'morgan_fp_1529', 'morgan_fp_1530', 'morgan_fp_1531', 'morgan_fp_1532', 'morgan_fp_1533', 'morgan_fp_1534', 'morgan_fp_1535', 'morgan_fp_1536', 'morgan_fp_1537', 'morgan_fp_1538', 'morgan_fp_1539', 'morgan_fp_1540', 'morgan_fp_1541', 'morgan_fp_1542', 'morgan_fp_1543', 'morgan_fp_1544', 'morgan_fp_1545', 'morgan_fp_1546', 'morgan_fp_1547', 'morgan_fp_1548', 'morgan_fp_1549', 'morgan_fp_1550', 'morgan_fp_1551', 'morgan_fp_1552', 'morgan_fp_1553', 'morgan_fp_1554', 'morgan_fp_1555', 'morgan_fp_1556', 'morgan_fp_1557', 'morgan_fp_1558', 'morgan_fp_1559', 'morgan_fp_1560', 'morgan_fp_1561', 'morgan_fp_1562', 'morgan_fp_1563', 'morgan_fp_1564', 'morgan_fp_1565', 'morgan_fp_1566', 'morgan_fp_1567', 'morgan_fp_1568', 'morgan_fp_1569', 'morgan_fp_1570', 'morgan_fp_1571', 'morgan_fp_1572', 'morgan_fp_1573', 'morgan_fp_1574', 'morgan_fp_1575', 'morgan_fp_1576', 'morgan_fp_1577', 'morgan_fp_1578', 'morgan_fp_1579', 'morgan_fp_1580', 'morgan_fp_1581', 'morgan_fp_1582', 'morgan_fp_1583', 'morgan_fp_1584', 'morgan_fp_1585', 'morgan_fp_1586', 'morgan_fp_1587', 'morgan_fp_1588', 'morgan_fp_1589', 'morgan_fp_1590', 'morgan_fp_1591', 'morgan_fp_1592', 'morgan_fp_1593', 'morgan_fp_1594', 'morgan_fp_1595', 'morgan_fp_1596', 'morgan_fp_1597', 'morgan_fp_1598', 'morgan_fp_1599', 'morgan_fp_1600', 'morgan_fp_1601', 'morgan_fp_1602', 'morgan_fp_1603', 'morgan_fp_1604', 'morgan_fp_1605', 'morgan_fp_1606', 'morgan_fp_1607', 'morgan_fp_1608', 'morgan_fp_1609', 'morgan_fp_1610', 'morgan_fp_1611', 'morgan_fp_1612', 'morgan_fp_1613', 'morgan_fp_1614', 'morgan_fp_1615', 'morgan_fp_1616', 'morgan_fp_1617', 'morgan_fp_1618', 'morgan_fp_1619', 'morgan_fp_1620', 'morgan_fp_1621', 'morgan_fp_1622', 'morgan_fp_1623', 'morgan_fp_1624', 'morgan_fp_1625', 'morgan_fp_1626', 'morgan_fp_1627', 'morgan_fp_1628', 'morgan_fp_1629', 'morgan_fp_1630', 'morgan_fp_1631', 'morgan_fp_1632', 'morgan_fp_1633', 'morgan_fp_1634', 'morgan_fp_1635', 'morgan_fp_1636', 'morgan_fp_1637', 'morgan_fp_1638', 'morgan_fp_1639', 'morgan_fp_1640', 'morgan_fp_1641', 'morgan_fp_1642', 'morgan_fp_1643', 'morgan_fp_1644', 'morgan_fp_1645', 'morgan_fp_1646', 'morgan_fp_1647', 'morgan_fp_1648', 'morgan_fp_1649', 'morgan_fp_1650', 'morgan_fp_1651', 'morgan_fp_1652', 'morgan_fp_1653', 'morgan_fp_1654', 'morgan_fp_1655', 'morgan_fp_1656', 'morgan_fp_1657', 'morgan_fp_1658', 'morgan_fp_1659', 'morgan_fp_1660', 'morgan_fp_1661', 'morgan_fp_1662', 'morgan_fp_1663', 'morgan_fp_1664', 'morgan_fp_1665', 'morgan_fp_1666', 'morgan_fp_1667', 'morgan_fp_1668', 'morgan_fp_1669', 'morgan_fp_1670', 'morgan_fp_1671', 'morgan_fp_1672', 'morgan_fp_1673', 'morgan_fp_1674', 'morgan_fp_1675', 'morgan_fp_1676', 'morgan_fp_1677', 'morgan_fp_1678', 'morgan_fp_1679', 'morgan_fp_1680', 'morgan_fp_1681', 'morgan_fp_1682', 'morgan_fp_1683', 'morgan_fp_1684', 'morgan_fp_1685', 'morgan_fp_1686', 'morgan_fp_1687', 'morgan_fp_1688', 'morgan_fp_1689', 'morgan_fp_1690', 'morgan_fp_1691', 'morgan_fp_1692', 'morgan_fp_1693', 'morgan_fp_1694', 'morgan_fp_1695', 'morgan_fp_1696', 'morgan_fp_1697', 'morgan_fp_1698', 'morgan_fp_1699', 'morgan_fp_1700', 'morgan_fp_1701', 'morgan_fp_1702', 'morgan_fp_1703', 'morgan_fp_1704', 'morgan_fp_1705', 'morgan_fp_1706', 'morgan_fp_1707', 'morgan_fp_1708', 'morgan_fp_1709', 'morgan_fp_1710', 'morgan_fp_1711', 'morgan_fp_1712', 'morgan_fp_1713', 'morgan_fp_1714', 'morgan_fp_1715', 'morgan_fp_1716', 'morgan_fp_1717', 'morgan_fp_1718', 'morgan_fp_1719', 'morgan_fp_1720', 'morgan_fp_1721', 'morgan_fp_1722', 'morgan_fp_1723', 'morgan_fp_1724', 'morgan_fp_1725', 'morgan_fp_1726', 'morgan_fp_1727', 'morgan_fp_1728', 'morgan_fp_1729', 'morgan_fp_1730', 'morgan_fp_1731', 'morgan_fp_1732', 'morgan_fp_1733', 'morgan_fp_1734', 'morgan_fp_1735', 'morgan_fp_1736', 'morgan_fp_1737', 'morgan_fp_1738', 'morgan_fp_1739', 'morgan_fp_1740', 'morgan_fp_1741', 'morgan_fp_1742', 'morgan_fp_1743', 'morgan_fp_1744', 'morgan_fp_1745', 'morgan_fp_1746', 'morgan_fp_1747', 'morgan_fp_1748', 'morgan_fp_1749', 'morgan_fp_1750', 'morgan_fp_1751', 'morgan_fp_1752', 'morgan_fp_1753', 'morgan_fp_1754', 'morgan_fp_1755', 'morgan_fp_1756', 'morgan_fp_1757', 'morgan_fp_1758', 'morgan_fp_1759', 'morgan_fp_1760', 'morgan_fp_1761', 'morgan_fp_1762', 'morgan_fp_1763', 'morgan_fp_1764', 'morgan_fp_1765', 'morgan_fp_1766', 'morgan_fp_1767', 'morgan_fp_1768', 'morgan_fp_1769', 'morgan_fp_1770', 'morgan_fp_1771', 'morgan_fp_1772', 'morgan_fp_1773', 'morgan_fp_1774', 'morgan_fp_1775', 'morgan_fp_1776', 'morgan_fp_1777', 'morgan_fp_1778', 'morgan_fp_1779', 'morgan_fp_1780', 'morgan_fp_1781', 'morgan_fp_1782', 'morgan_fp_1783', 'morgan_fp_1784', 'morgan_fp_1785', 'morgan_fp_1786', 'morgan_fp_1787', 'morgan_fp_1788', 'morgan_fp_1789', 'morgan_fp_1790', 'morgan_fp_1791', 'morgan_fp_1792', 'morgan_fp_1793', 'morgan_fp_1794', 'morgan_fp_1795', 'morgan_fp_1796', 'morgan_fp_1797', 'morgan_fp_1798', 'morgan_fp_1799', 'morgan_fp_1800', 'morgan_fp_1801', 'morgan_fp_1802', 'morgan_fp_1803', 'morgan_fp_1804', 'morgan_fp_1805', 'morgan_fp_1806', 'morgan_fp_1807', 'morgan_fp_1808', 'morgan_fp_1809', 'morgan_fp_1810', 'morgan_fp_1811', 'morgan_fp_1812', 'morgan_fp_1813', 'morgan_fp_1814', 'morgan_fp_1815', 'morgan_fp_1816', 'morgan_fp_1817', 'morgan_fp_1818', 'morgan_fp_1819', 'morgan_fp_1820', 'morgan_fp_1821', 'morgan_fp_1822', 'morgan_fp_1823', 'morgan_fp_1824', 'morgan_fp_1825', 'morgan_fp_1826', 'morgan_fp_1827', 'morgan_fp_1828', 'morgan_fp_1829', 'morgan_fp_1830', 'morgan_fp_1831', 'morgan_fp_1832', 'morgan_fp_1833', 'morgan_fp_1834', 'morgan_fp_1835', 'morgan_fp_1836', 'morgan_fp_1837', 'morgan_fp_1838', 'morgan_fp_1839', 'morgan_fp_1840', 'morgan_fp_1841', 'morgan_fp_1842', 'morgan_fp_1843', 'morgan_fp_1844', 'morgan_fp_1845', 'morgan_fp_1846', 'morgan_fp_1847', 'morgan_fp_1848', 'morgan_fp_1849', 'morgan_fp_1850', 'morgan_fp_1851', 'morgan_fp_1852', 'morgan_fp_1853', 'morgan_fp_1854', 'morgan_fp_1855', 'morgan_fp_1856', 'morgan_fp_1857', 'morgan_fp_1858', 'morgan_fp_1859', 'morgan_fp_1860', 'morgan_fp_1861', 'morgan_fp_1862', 'morgan_fp_1863', 'morgan_fp_1864', 'morgan_fp_1865', 'morgan_fp_1866', 'morgan_fp_1867', 'morgan_fp_1868', 'morgan_fp_1869', 'morgan_fp_1870', 'morgan_fp_1871', 'morgan_fp_1872', 'morgan_fp_1873', 'morgan_fp_1874', 'morgan_fp_1875', 'morgan_fp_1876', 'morgan_fp_1877', 'morgan_fp_1878', 'morgan_fp_1879', 'morgan_fp_1880', 'morgan_fp_1881', 'morgan_fp_1882', 'morgan_fp_1883', 'morgan_fp_1884', 'morgan_fp_1885', 'morgan_fp_1886', 'morgan_fp_1887', 'morgan_fp_1888', 'morgan_fp_1889', 'morgan_fp_1890', 'morgan_fp_1891', 'morgan_fp_1892', 'morgan_fp_1893', 'morgan_fp_1894', 'morgan_fp_1895', 'morgan_fp_1896', 'morgan_fp_1897', 'morgan_fp_1898', 'morgan_fp_1899', 'morgan_fp_1900', 'morgan_fp_1901', 'morgan_fp_1902', 'morgan_fp_1903', 'morgan_fp_1904', 'morgan_fp_1905', 'morgan_fp_1906', 'morgan_fp_1907', 'morgan_fp_1908', 'morgan_fp_1909', 'morgan_fp_1910', 'morgan_fp_1911', 'morgan_fp_1912', 'morgan_fp_1913', 'morgan_fp_1914', 'morgan_fp_1915', 'morgan_fp_1916', 'morgan_fp_1917', 'morgan_fp_1918', 'morgan_fp_1919', 'morgan_fp_1920', 'morgan_fp_1921', 'morgan_fp_1922', 'morgan_fp_1923', 'morgan_fp_1924', 'morgan_fp_1925', 'morgan_fp_1926', 'morgan_fp_1927', 'morgan_fp_1928', 'morgan_fp_1929', 'morgan_fp_1930', 'morgan_fp_1931', 'morgan_fp_1932', 'morgan_fp_1933', 'morgan_fp_1934', 'morgan_fp_1935', 'morgan_fp_1936', 'morgan_fp_1937', 'morgan_fp_1938', 'morgan_fp_1939', 'morgan_fp_1940', 'morgan_fp_1941', 'morgan_fp_1942', 'morgan_fp_1943', 'morgan_fp_1944', 'morgan_fp_1945', 'morgan_fp_1946', 'morgan_fp_1947', 'morgan_fp_1948', 'morgan_fp_1949', 'morgan_fp_1950', 'morgan_fp_1951', 'morgan_fp_1952', 'morgan_fp_1953', 'morgan_fp_1954', 'morgan_fp_1955', 'morgan_fp_1956', 'morgan_fp_1957', 'morgan_fp_1958', 'morgan_fp_1959', 'morgan_fp_1960', 'morgan_fp_1961', 'morgan_fp_1962', 'morgan_fp_1963', 'morgan_fp_1964', 'morgan_fp_1965', 'morgan_fp_1966', 'morgan_fp_1967', 'morgan_fp_1968', 'morgan_fp_1969', 'morgan_fp_1970', 'morgan_fp_1971', 'morgan_fp_1972', 'morgan_fp_1973', 'morgan_fp_1974', 'morgan_fp_1975', 'morgan_fp_1976', 'morgan_fp_1977', 'morgan_fp_1978', 'morgan_fp_1979', 'morgan_fp_1980', 'morgan_fp_1981', 'morgan_fp_1982', 'morgan_fp_1983', 'morgan_fp_1984', 'morgan_fp_1985', 'morgan_fp_1986', 'morgan_fp_1987', 'morgan_fp_1988', 'morgan_fp_1989', 'morgan_fp_1990', 'morgan_fp_1991', 'morgan_fp_1992', 'morgan_fp_1993', 'morgan_fp_1994', 'morgan_fp_1995', 'morgan_fp_1996', 'morgan_fp_1997', 'morgan_fp_1998', 'morgan_fp_1999', 'morgan_fp_2000', 'morgan_fp_2001', 'morgan_fp_2002', 'morgan_fp_2003', 'morgan_fp_2004', 'morgan_fp_2005', 'morgan_fp_2006', 'morgan_fp_2007', 'morgan_fp_2008', 'morgan_fp_2009', 'morgan_fp_2010', 'morgan_fp_2011', 'morgan_fp_2012', 'morgan_fp_2013', 'morgan_fp_2014', 'morgan_fp_2015', 'morgan_fp_2016', 'morgan_fp_2017', 'morgan_fp_2018', 'morgan_fp_2019', 'morgan_fp_2020', 'morgan_fp_2021', 'morgan_fp_2022', 'morgan_fp_2023', 'morgan_fp_2024', 'morgan_fp_2025', 'morgan_fp_2026', 'morgan_fp_2027', 'morgan_fp_2028', 'morgan_fp_2029', 'morgan_fp_2030', 'morgan_fp_2031', 'morgan_fp_2032', 'morgan_fp_2033', 'morgan_fp_2034', 'morgan_fp_2035', 'morgan_fp_2036', 'morgan_fp_2037', 'morgan_fp_2038', 'morgan_fp_2039', 'morgan_fp_2040', 'morgan_fp_2041', 'morgan_fp_2042', 'morgan_fp_2043', 'morgan_fp_2044', 'morgan_fp_2045', 'morgan_fp_2046', 'morgan_fp_2047']\n",
      "\n",
      "Extracted global features for each split:\n",
      "X_train_global_features shape: (13119, 2266)\n",
      "X_val_global_features shape: (2812, 2266)\n",
      "X_test_global_features shape: (2812, 2266)\n",
      "\n",
      "First 5 rows of X_train_global_features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>HeavyAtomMolWt</th>\n",
       "      <th>ExactMolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>312.243</td>\n",
       "      <td>328.121178</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>334.222</td>\n",
       "      <td>353.126323</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>418.303</td>\n",
       "      <td>447.215806</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>360.102</td>\n",
       "      <td>374.026604</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>453.727</td>\n",
       "      <td>472.111375</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  HeavyAtomMolWt  ExactMolWt  \\\n",
       "0        0.494176  0.476742  12.560000  328.371         312.243  328.121178   \n",
       "1        0.459195  0.604738  12.923077  353.374         334.222  353.126323   \n",
       "2       -0.173158  0.359463  15.909091  447.535         418.303  447.215806   \n",
       "3       -0.686457  0.479732  11.217391  375.222         360.102  374.026604   \n",
       "4       -3.116139  0.437556  16.121212  472.879         453.727  472.111375   \n",
       "\n",
       "   ...  morgan_fp_2038  morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  \\\n",
       "0  ...               0               0               0               0   \n",
       "1  ...               0               0               0               0   \n",
       "2  ...               0               0               0               0   \n",
       "3  ...               0               0               0               0   \n",
       "4  ...               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2042  morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2046  morgan_fp_2047  \n",
       "0               0               0  \n",
       "1               0               0  \n",
       "2               0               0  \n",
       "3               0               0  \n",
       "4               0               0  \n",
       "\n",
       "[5 rows x 2266 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original X DataFrames (with molregno and canonical_smiles) are retained and ready for graph construction:\n",
      "X_train shape: (13119, 2268)\n",
      "X_val shape: (2812, 2268)\n",
      "X_test shape: (2812, 2268)\n",
      "y_train shape: (13119, 1), y_val shape: (2812, 1), y_test shape: (2812, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molregno</th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>num_activities</th>\n",
       "      <th>MaxAbsEStateIndex</th>\n",
       "      <th>MaxEStateIndex</th>\n",
       "      <th>MinAbsEStateIndex</th>\n",
       "      <th>MinEStateIndex</th>\n",
       "      <th>qed</th>\n",
       "      <th>SPS</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>...</th>\n",
       "      <th>morgan_fp_2038</th>\n",
       "      <th>morgan_fp_2039</th>\n",
       "      <th>morgan_fp_2040</th>\n",
       "      <th>morgan_fp_2041</th>\n",
       "      <th>morgan_fp_2042</th>\n",
       "      <th>morgan_fp_2043</th>\n",
       "      <th>morgan_fp_2044</th>\n",
       "      <th>morgan_fp_2045</th>\n",
       "      <th>morgan_fp_2046</th>\n",
       "      <th>morgan_fp_2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307646</td>\n",
       "      <td>COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C</td>\n",
       "      <td>6</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>6.033142</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.494176</td>\n",
       "      <td>0.476742</td>\n",
       "      <td>12.560000</td>\n",
       "      <td>328.371</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2081122</td>\n",
       "      <td>COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC</td>\n",
       "      <td>9</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>9.645791</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.459195</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>12.923077</td>\n",
       "      <td>353.374</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2199496</td>\n",
       "      <td>COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>11.953178</td>\n",
       "      <td>0.169552</td>\n",
       "      <td>-0.173158</td>\n",
       "      <td>0.359463</td>\n",
       "      <td>15.909091</td>\n",
       "      <td>447.535</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2221960</td>\n",
       "      <td>O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO</td>\n",
       "      <td>4</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>12.253458</td>\n",
       "      <td>0.216419</td>\n",
       "      <td>-0.686457</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>11.217391</td>\n",
       "      <td>375.222</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2879093</td>\n",
       "      <td>Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...</td>\n",
       "      <td>2</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>14.128489</td>\n",
       "      <td>0.124437</td>\n",
       "      <td>-3.116139</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>16.121212</td>\n",
       "      <td>472.879</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   molregno                                   canonical_smiles  \\\n",
       "0   2307646               COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C   \n",
       "1   2081122       COc1cc(/C(C#N)=C/c2ccc3c(c2)OCCO3)cc(OC)c1OC   \n",
       "2   2199496  COC(=O)[C@@H]1CCCN1Cc1ccc(-c2ncc(-c3ccc(OCC=C(...   \n",
       "3   2221960           O=C(/C=C/c1cccn(C/C=C/c2ccccc2Br)c1=O)NO   \n",
       "4   2879093  Cc1cc(C2c3c(-c4cccc5[nH]c(=O)oc45)n[nH]c3C(=O)...   \n",
       "\n",
       "   num_activities  MaxAbsEStateIndex  MaxEStateIndex  MinAbsEStateIndex  \\\n",
       "0               6           6.033142        6.033142           0.494176   \n",
       "1               9           9.645791        9.645791           0.459195   \n",
       "2               6          11.953178       11.953178           0.169552   \n",
       "3               4          12.253458       12.253458           0.216419   \n",
       "4               2          14.128489       14.128489           0.124437   \n",
       "\n",
       "   MinEStateIndex       qed        SPS    MolWt  ...  morgan_fp_2038  \\\n",
       "0        0.494176  0.476742  12.560000  328.371  ...               0   \n",
       "1        0.459195  0.604738  12.923077  353.374  ...               0   \n",
       "2       -0.173158  0.359463  15.909091  447.535  ...               0   \n",
       "3       -0.686457  0.479732  11.217391  375.222  ...               0   \n",
       "4       -3.116139  0.437556  16.121212  472.879  ...               0   \n",
       "\n",
       "   morgan_fp_2039  morgan_fp_2040  morgan_fp_2041  morgan_fp_2042  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2043  morgan_fp_2044  morgan_fp_2045  morgan_fp_2046  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   morgan_fp_2047  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 2268 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pGI50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14387</th>\n",
       "      <td>5.734742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12543</th>\n",
       "      <td>7.164746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12810</th>\n",
       "      <td>4.928428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>6.882724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>6.094208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pGI50\n",
       "14387  5.734742\n",
       "12543  7.164746\n",
       "12810  4.928428\n",
       "13172  6.882724\n",
       "18712  6.094208"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n--- Excracting global features of each molecule ---\")\n",
    "\n",
    "# Identify columns for global features\n",
    "# These are all columns in X_train EXCEPT 'molregno' and 'canonical_smiles'\n",
    "global_feature_columns = X_train.drop(columns=['molregno', 'canonical_smiles'], errors='ignore').columns.tolist()\n",
    "\n",
    "print(f\"Identified {len(global_feature_columns)} global feature columns for GNN.\")\n",
    "print(f\"Global feature columns: {global_feature_columns}\")\n",
    "\n",
    "# Extract global features into new DataFrames\n",
    "# These DataFrames will be the source for data.global_features in GNN Data objects\n",
    "X_train_global_features = X_train[global_feature_columns]\n",
    "X_val_global_features = X_val[global_feature_columns]\n",
    "X_test_global_features = X_test[global_feature_columns]\n",
    "\n",
    "print(\"\\nExtracted global features for each split:\")\n",
    "print(f\"X_train_global_features shape: {X_train_global_features.shape}\")\n",
    "print(f\"X_val_global_features shape: {X_val_global_features.shape}\")\n",
    "print(f\"X_test_global_features shape: {X_test_global_features.shape}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of X_train_global_features:\")\n",
    "display(X_train_global_features.head())\n",
    "\n",
    "\n",
    "# Confirm original X DataFrames (with molregno and canonical_smiles) are still available\n",
    "# These will be used for iterating and building individual graph objects.\n",
    "print(\"\\nOriginal X DataFrames (with molregno and canonical_smiles) are retained and ready for graph construction:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_val shape: {y_val.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "display(X_train.head()) # Show that original X_train still has molregno and canonical_smiles\n",
    "display(y_train.head()) # Show the pGI50 target values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff4a6c-ef8d-4aef-863c-22bc039fe563",
   "metadata": {},
   "source": [
    "### 3.2. Create Graph Objects of Each Molecule\n",
    "This step transforms each molecule's SMILES string into a `torch_geometric.data.Data` object, incorporating atom features, bond features, and the extracted global features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8982d07-be78-4701-a6bc-45ebcd87fd11",
   "metadata": {},
   "source": [
    "#### 3.2.1. Define Helper Function to Create Graph Object\n",
    "A helper function is defined here to encapsulate the logic for converting a single molecule's SMILES and its corresponding features into a PyTorch Geometric `Data` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c331d7-1a8a-47dc-b1f4-99b7429ae398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_pyg_data(mol, pgi50_value, global_features_vector, molregno, smiles_string):\n",
    "    if mol is None:\n",
    "        return None  # Handle cases where SMILES parsing fails\n",
    "\n",
    "    # Compute Gasteiger charges (how electron-dense the area occupied by this atom is, crucial for interactions)\n",
    "    try:\n",
    "        AllChem.ComputeGasteigerCharges(mol)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not compute Gasteiger charges for molregno {molregno}: {e}\")\n",
    "        # If computation fails, atoms will default to 0.0 for this property\n",
    "        pass\n",
    "\n",
    "    # Node Features (x): Atom Properties\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        # Initialize a list for this atom's features\n",
    "        features = []\n",
    "\n",
    "        # Atomic Number (int, not one-hot coded)\n",
    "        features.append(atom.GetAtomicNum())\n",
    "\n",
    "        # Basic Connectivity\n",
    "        features.append(atom.GetDegree())  # Num of directly-bonded heavy (non-Hydrogen) atoms\n",
    "        features.append(atom.GetTotalDegree())  # Total numb of neighbors (including all Hydrogens)\n",
    "\n",
    "        # Charge and Valence\n",
    "        features.append(atom.GetFormalCharge())  # Formal charge (integer charge based on bonding rules)\n",
    "        features.append(atom.GetNumExplicitHs())  # Number of explicitly defined hydrogens attached\n",
    "        features.append(atom.GetNumImplicitHs())  # Number of hydrogens implicitly defined by valence\n",
    "        features.append(atom.GetTotalNumHs())  # Total number of hydrogens attached (explicit + implicit)\n",
    "        features.append(atom.GetValence(Chem.ValenceType.IMPLICIT))  # Implicit Valence: Number of bonds formed by implicit hydrogens\n",
    "        features.append(atom.GetValence(Chem.ValenceType.EXPLICIT))  # Explicit Valence: Sum of bond orders (1 for single, 2 for double, etc.) to explicitly defined atoms\n",
    "        features.append(atom.GetTotalValence())  # Total Valence: Total number of bonds (sum of explicit & implicit valence)\n",
    "\n",
    "        # Hybridization (convert enum to int) (e.g., sp3, sp2)\n",
    "        features.append(int(atom.GetHybridization()))\n",
    "\n",
    "        # Aromaticity and Ring Information (boolean converted to int)\n",
    "        features.append(int(atom.GetIsAromatic()))        # Whether the atom is part of an aromatic system\n",
    "        features.append(int(atom.IsInRing()))             # Whether the atom is in ANY ring structure\n",
    "        features.append(int(atom.IsInRingSize(3)))        # Whether the atom is in a 3-membered ring\n",
    "        features.append(int(atom.IsInRingSize(4)))        # Whether the atom is in a 4-membered ring\n",
    "        features.append(int(atom.IsInRingSize(5)))        # Whether the atom is in a 5-membered ring\n",
    "        features.append(int(atom.IsInRingSize(6)))        # Whether the atom is in a 6-membered ring\n",
    "        features.append(int(atom.IsInRingSize(7)))        # Whether the atom is in a 7-membered ring\n",
    "        features.append(int(atom.IsInRingSize(8)))        # Whether the atom is in an 8-membered ring\n",
    "\n",
    "        # Chirality (convert enum to int)(stereochemical information, crucial for biological activity)\n",
    "        features.append(int(atom.GetChiralTag()))\n",
    "\n",
    "         # Partial Charges (from Gasteiger calculation)\n",
    "        gasteiger_charge = 0.0\n",
    "        if atom.HasProp('_GasteigerCharge'):\n",
    "            try:\n",
    "                gasteiger_charge = float(atom.GetProp('_GasteigerCharge'))\n",
    "            except ValueError:\n",
    "                pass # Handle potential 'nan' or non-float values gracefully\n",
    "        features.append(gasteiger_charge)\n",
    "\n",
    "        # Add to the list of all atom features for this molecule\n",
    "        atom_features.append(features)\n",
    "        \n",
    "    # Convert the list of lists to a PyTorch tensor\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "\n",
    "    # Edge Index (edge_index): Bond connectivity\n",
    "    edge_indices = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append([i, j])\n",
    "        edge_indices.append([j, i]) # Add reverse edge for undirected graph\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Handle molecules with no bonds (single atom, e.g., for [Ne])\n",
    "    if edge_index.numel() == 0:\n",
    "        edge_index = torch.empty((2, 0), dtype=torch.long) # Create an empty edge_index tensor\n",
    "\n",
    "    # Graph-level Target (y): pGI50\n",
    "    y = torch.tensor([pgi50_value], dtype=torch.float)\n",
    "\n",
    "    # Global Features (global_features)\n",
    "    try:\n",
    "        global_features_vector = global_features_vector.astype(float)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error converting global_features_vector to float for molregno {molregno}: {e}\")\n",
    "        \n",
    "    global_features_tensor = torch.tensor(global_features_vector, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    # Create the PyTorch Geometric Data object\n",
    "    data = Data(x=x,\n",
    "                edge_index=edge_index,\n",
    "                y=y,\n",
    "                global_features=global_features_tensor,\n",
    "                molregno=molregno,\n",
    "                smiles=smiles_string)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3b691-faa4-494d-b48b-49685b04d746",
   "metadata": {},
   "source": [
    "#### 3.2.2. Apply Helper Function to Create Graph Objects\n",
    "The defined helper function is applied across the entire dataset to generate a list of PyTorch Geometric `Data` objects for each molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c56a6f9-50ad-4596-8935-38c2af5e1a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating PyG Data objects for Training Set ---\n",
      "Type of X_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of y_train: <class 'pandas.core.frame.DataFrame'>\n",
      "Type of X_train_global_features: <class 'pandas.core.frame.DataFrame'>\n",
      "Length of train_df after concatenation: 13119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5877529cbb9425bbf9c93953bdf1ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Train Molecules:   0%|          | 0/13119 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 13119 / 13119 graph objects for the training set.\n",
      "Total training graphs: 13119\n",
      "\n",
      "--- Creating PyG Data objects for Validation Set ---\n",
      "Length of val_df after concatenation: 2812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1fb6b11cca64aeaa2dbf8ba24e0c4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Validation Molecules:   0%|          | 0/2812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 2812 / 2812 graph objects for the validation set.\n",
      "Total validation graphs: 2812\n",
      "\n",
      "--- Creating PyG Data objects for Test Set ---\n",
      "Length of test_df after concatenation: 2812\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8517eba41bb47308aac7a64d6d55d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Test Molecules:   0%|          | 0/2812 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 2812 / 2812 graph objects for the test set.\n",
      "Total test graphs: 2812\n"
     ]
    }
   ],
   "source": [
    "train_data_list = []\n",
    "val_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# Process Training Data\n",
    "print(\"\\n--- Creating PyG Data objects for Training Set ---\")\n",
    "\n",
    "print(f\"Type of X_train: {type(X_train)}\")\n",
    "print(f\"Type of y_train: {type(y_train)}\")\n",
    "print(f\"Type of X_train_global_features: {type(X_train_global_features)}\")\n",
    "\n",
    "# Ensure X_train, y_train, X_train_global_features have the same index for alignment\n",
    "train_df = pd.concat([X_train.reset_index(drop=True),\n",
    "                      y_train.reset_index(drop=True),\n",
    "                      X_train_global_features.reset_index(drop=True)],\n",
    "                     axis=1)\n",
    "print(f\"Length of train_df after concatenation: {len(train_df)}\")\n",
    "\n",
    "successful_train_graphs = 0\n",
    "for index, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Processing Train Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    \n",
    "    # Extract global features based on the column names extracted after loading data splits\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    # Convert SMILES to RDKit Mol object\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Create PyG Data object\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0: # Ensure valid mol and has nodes\n",
    "        train_data_list.append(pyg_data)\n",
    "        successful_train_graphs += 1\n",
    "    else:\n",
    "        print(f\"Warning: Could not process SMILES: {smiles} (Molregno: {molregno})\")\n",
    "\n",
    "print(f\"Successfully created {successful_train_graphs} / {len(train_df)} graph objects for the training set.\")\n",
    "print(f\"Total training graphs: {len(train_data_list)}\")\n",
    "\n",
    "\n",
    "# Process Validation Data\n",
    "print(\"\\n--- Creating PyG Data objects for Validation Set ---\")\n",
    "val_df = pd.concat([X_val.reset_index(drop=True),\n",
    "                    y_val.reset_index(drop=True),\n",
    "                    X_val_global_features.reset_index(drop=True)],\n",
    "                   axis=1)\n",
    "print(f\"Length of val_df after concatenation: {len(val_df)}\")\n",
    "\n",
    "successful_val_graphs = 0\n",
    "for index, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Processing Validation Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0:\n",
    "        val_data_list.append(pyg_data)\n",
    "        successful_val_graphs += 1\n",
    "\n",
    "print(f\"Successfully created {successful_val_graphs} / {len(val_df)} graph objects for the validation set.\")\n",
    "print(f\"Total validation graphs: {len(val_data_list)}\")\n",
    "\n",
    "\n",
    "# Process Test Data\n",
    "print(\"\\n--- Creating PyG Data objects for Test Set ---\")\n",
    "test_df = pd.concat([X_test.reset_index(drop=True),\n",
    "                     y_test.reset_index(drop=True),\n",
    "                     X_test_global_features.reset_index(drop=True)],\n",
    "                    axis=1)\n",
    "print(f\"Length of test_df after concatenation: {len(test_df)}\")\n",
    "\n",
    "successful_test_graphs = 0\n",
    "for index, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Test Molecules\"):\n",
    "    smiles = row['canonical_smiles']\n",
    "    molregno = row['molregno']\n",
    "    pgi50 = row['pGI50']\n",
    "    global_features_vector = row[global_feature_columns].values\n",
    "\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    pyg_data = mol_to_pyg_data(mol, pgi50, global_features_vector, molregno, smiles)\n",
    "\n",
    "    if pyg_data is not None and pyg_data.x.numel() > 0:\n",
    "        test_data_list.append(pyg_data)\n",
    "        successful_test_graphs += 1\n",
    "\n",
    "print(f\"Successfully created {successful_test_graphs} / {len(test_df)} graph objects for the test set.\")\n",
    "print(f\"Total test graphs: {len(test_data_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835fa21-03a3-42ff-9df3-4d33a258ea09",
   "metadata": {},
   "source": [
    "##### 3.2.2.1. Verify Creation of Graph Objects\n",
    "Basic checks are performed to verify that the graph objects have been correctly created (by checking the number of nodes, edges, and features for a sample object)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9ff332-5b18-4665-8b6c-dcd4948bb7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [6.0, 6.0, 6.033141613006592, 6.033141613006592, 6.033141613006592]\n",
      "    Global features (data.global_features) min: -3.1400\n",
      "    Global features (data.global_features) max: 1072410.2500\n",
      "    Global features (data.global_features) mean: 474.7916\n",
      "    Global features (data.global_features) std: 22525.9316\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2ba18-2eac-40de-abed-6e864d14f225",
   "metadata": {},
   "source": [
    "#### 3.2.3. Standardize Global Features of Each Molecule\n",
    "The global features (RDKit descriptors, Morgan fingerprints) within each graph object are standardized (using `StandardScaler`) to normalize their scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4617613f-0ddd-4ca9-b99b-005acf0e4878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Global features in torch_geometric.data.Data objects have been scaled!\n"
     ]
    }
   ],
   "source": [
    "# Collect all global features to fit the scaler\n",
    "# Concatenate all global_features tensors. Each data.global_features is already (1, feature_dim),\n",
    "# so torch.cat(..., dim=0) will result in (num_total_graphs, feature_dim).\n",
    "list_of_global_features_tensors = [data.global_features for data in train_data_list + val_data_list]\n",
    "all_global_features_combined = torch.cat(list_of_global_features_tensors, dim=0).cpu().numpy()\n",
    "\n",
    "# Initialize and fit the scaler on the combined global features from training and validation sets\n",
    "global_feature_scaler = StandardScaler()\n",
    "global_feature_scaler.fit(all_global_features_combined)\n",
    "\n",
    "# Apply scaling to the 'global_features' in Data objects for all splits\n",
    "for data_list in [train_data_list, val_data_list, test_data_list]:\n",
    "    for data in data_list:\n",
    "        # Ensure it's numpy before scaling, then back to torch\n",
    "        original_global_features_np = data.global_features.cpu().numpy()\n",
    "        scaled_global_features_np = global_feature_scaler.transform(original_global_features_np)\n",
    "        # Put it back on the correct device\n",
    "        data.global_features = torch.tensor(scaled_global_features_np, dtype=torch.float32).to(data.global_features.device)\n",
    "\n",
    "print(\"\\nGlobal features in torch_geometric.data.Data objects have been scaled!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c07769-10ea-426a-810d-8cb48c2b0c03",
   "metadata": {},
   "source": [
    "##### 3.2.3.1. Verify Scaling of Global Features\n",
    "A quick check is performed to confirm that the global features within the graph objects have been successfully scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42567868-2a41-42be-af81-96be63072dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [0.038460150361061096, 0.038460150361061096, -2.2056941986083984, -2.2056941986083984, -2.2056941986083984]\n",
      "    Global features (data.global_features) min: -2.2057\n",
      "    Global features (data.global_features) max: 16.9898\n",
      "    Global features (data.global_features) mean: -0.0328\n",
      "    Global features (data.global_features) std: 0.9213\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd7f3f-57a4-47d2-85df-3485e21df7a2",
   "metadata": {},
   "source": [
    "#### 3.2.4. Save Graph Objects\n",
    "The list of generated PyTorch Geometric `Data` objects with the scaled global features is saved locally to avoid regenerating them in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d05e3-8fe7-4ec2-a05c-71d6dc567cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed graph data saved to: ..\\data\\splits\\pyg_data_graphs\n",
      "Train data list size: 13119\n",
      "Validation data list size: 2812\n",
      "Test data list size: 2812\n"
     ]
    }
   ],
   "source": [
    "# Directory for saving the processed graph data\n",
    "save_dir = Path('../data/pyg_data_graphs')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the full file paths\n",
    "train_data_path = save_dir / 'train_data_list.pt'\n",
    "val_data_path = save_dir / 'val_data_list.pt'\n",
    "test_data_path = save_dir / 'test_data_list.pt'\n",
    "\n",
    "# Save the lists of Data objects\n",
    "torch.save(train_data_list, train_data_path)\n",
    "torch.save(val_data_list, val_data_path)\n",
    "torch.save(test_data_list, test_data_path)\n",
    "\n",
    "print(f\"Processed graph data saved to: {save_dir}\")\n",
    "print(f\"Train data list size: {len(train_data_list)}\")\n",
    "print(f\"Validation data list size: {len(val_data_list)}\")\n",
    "print(f\"Test data list size: {len(test_data_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046262d-755c-4d54-86e8-33b883e70dce",
   "metadata": {},
   "source": [
    "## 4. Optimize Hyperparameters\n",
    "This section utilizes Optuna to systematically search for the optimal set of hyperparameters for the GNN model, aiming to minimize prediction error on the validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f189570-12e8-4380-8cd2-ccd3c52985f8",
   "metadata": {},
   "source": [
    "### 4.1. Load Graph Objects\n",
    "The previously saved PyTorch Geometric `Data` graph objects are loaded, serving as input for the hyperparameter optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6919bfb2-bb97-4f6a-995f-25842c26ef90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13119 training graphs.\n",
      "Loaded 2812 validation graphs.\n",
      "Loaded 2812 test graphs.\n"
     ]
    }
   ],
   "source": [
    "# Directory where the graph objects are saved\n",
    "load_dir = Path('../data/pyg_data_graphs')\n",
    "\n",
    "# Define the full file paths\n",
    "train_data_path = load_dir / 'train_data_list.pt'\n",
    "val_data_path = load_dir / 'val_data_list.pt'\n",
    "test_data_path = load_dir / 'test_data_list.pt'\n",
    "\n",
    "# Load the lists of Data objects\n",
    "try:\n",
    "    train_data_list = torch.load(train_data_path, weights_only=False)\n",
    "    val_data_list = torch.load(val_data_path, weights_only=False)\n",
    "    test_data_list = torch.load(test_data_path, weights_only=False)\n",
    "\n",
    "    print(f\"Loaded {len(train_data_list)} training graphs.\")\n",
    "    print(f\"Loaded {len(val_data_list)} validation graphs.\")\n",
    "    print(f\"Loaded {len(test_data_list)} test graphs.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Processed data not found in {load_dir}. Please run the data processing and saving step first.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25775b7b-f6bf-487c-bd1d-afa380af03c7",
   "metadata": {},
   "source": [
    "#### 4.1.1. Verify Loading of Graph Objects\n",
    "Basic checks are performed to ensure the graph objects have been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62061207-247d-4bd6-a197-daaa1a6fb4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [0.038460150361061096, 0.038460150361061096, -2.2056941986083984, -2.2056941986083984, -2.2056941986083984]\n",
      "    Global features (data.global_features) min: -2.2057\n",
      "    Global features (data.global_features) max: 16.9898\n",
      "    Global features (data.global_features) mean: -0.0328\n",
      "    Global features (data.global_features) std: 0.9213\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df52b2f3-fb35-4d0d-b181-c7af49150abd",
   "metadata": {},
   "source": [
    "### 4.2. Define Optuna Objective Function\n",
    "The Optuna objective function is defined here. This function instantiates and trains a GNN model with a given set of hyperparameters, returning its performance (i.e., RMSE) on the validation set, which Optuna aims to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30682f4-705d-48b2-8da1-24a8afa2325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_channels = trial.suggest_int(\"hidden_channels\", 128, 1024, log=True) # Number of neurons in hidden layer\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256]) # Batch size for DataLoaders\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 150, 600)  # Number of training epochs\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 4) # Number of GNN layers\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5) # Dropout rate\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-3, log=True)\n",
    "\n",
    "    # Determine feature dimensions dynamically from loaded/created graph objects\n",
    "    # Ensure train_data_list is not empty before accessing its first element\n",
    "    if not train_data_list:\n",
    "        raise ValueError(\"train_data_list is empty. Cannot determine feature dimensions.\")\n",
    "\n",
    "    # node_feature_dim: Number of features per atom\n",
    "    # global_feature_dim: Number of global features per molecule\n",
    "    node_feature_dim = train_data_list[0].x.shape[1]\n",
    "    global_feature_dim = train_data_list[0].global_features.shape[1]\n",
    "\n",
    "    # Initialize model\n",
    "    model = GNN(\n",
    "        node_feature_dim=node_feature_dim,\n",
    "        global_feature_dim=global_feature_dim,\n",
    "        hidden_channels=hidden_channels,  # From Optuna trial\n",
    "        num_layers=num_layers,  # From Optuna trial\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss function and Optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # PyTorch Geometric DataLoaders\n",
    "    num_workers = 0\n",
    "    train_loader = PyGDataLoader(train_data_list, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = PyGDataLoader(val_data_list, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    best_val_rmse = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 50 # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        start_epoch_time = time.time()\n",
    "        for batch_idx, data_batch in enumerate(train_loader):\n",
    "            data_batch = data_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch)\n",
    "                \n",
    "            # Ensure outputs and target are same shape for loss calculation\n",
    "            loss = criterion(outputs.view(-1), data_batch.y.view(-1)) # .view(-1) flattens to ensure shape compatibility\n",
    "\n",
    "            if torch.isnan(outputs).any() or torch.isinf(outputs).any():\n",
    "                print(f\"!!! WARNING: NaN/Inf in model outputs at epoch {epoch+1}, batch {batch_idx+1}\")\n",
    "            if torch.isnan(loss).any() or torch.isinf(loss).any():\n",
    "                print(f\"!!! WARNING: NaN/Inf in loss at epoch {epoch+1}, batch {batch_idx+1}\")\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
    "                        print(f\"!!! CRITICAL: NaN/Inf in gradient of {name} - Epoch {epoch+1}, Batch {batch_idx+1}\")\n",
    "                        # Add a break here for deeper inspection if this happens\n",
    "                        # import sys; sys.exit(\"Gradient instability detected.\")\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad(): # Disable gradient calculations for validation\n",
    "            for data_batch in val_loader:\n",
    "                data_batch = data_batch.to(device)\n",
    "                val_outputs = model(data_batch)\n",
    "                val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "                val_targets.extend(data_batch.y.cpu().numpy().flatten()) # Extract y from PyG Data object\n",
    "\n",
    "        val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "\n",
    "        if device.type == 'cuda': # Ensure GPU operations are finished before timing an epoch\n",
    "            torch.cuda.synchronize()\n",
    "        end_epoch_time = time.time()\n",
    "\n",
    "        print(f\"Trial {trial.number}, Epoch {epoch+1}/{n_epochs}, Val RMSE: {val_rmse:.4f}, Time: {end_epoch_time - start_epoch_time:.2f}s\")\n",
    "\n",
    "        # Optuna Pruning: Report current validation RMSE to Optuna\n",
    "        trial.report(val_rmse, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # Manual Early Stopping Check\n",
    "        if val_rmse < best_val_rmse:\n",
    "            best_val_rmse = val_rmse\n",
    "            patience_counter = 0 # Reset patience if improvement is found\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} for trial {trial.number}\")\n",
    "                break # Exit training loop for current trial\n",
    "\n",
    "    # Final evaluation on validation set after training (or early stopping)\n",
    "    model.eval()\n",
    "    final_val_predictions = []\n",
    "    final_val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data_batch in val_loader:\n",
    "            data_batch = data_batch.to(device)\n",
    "            val_outputs = model(data_batch)\n",
    "            final_val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "            final_val_targets.extend(data_batch.y.cpu().numpy().flatten())\n",
    "\n",
    "    final_rmse = np.sqrt(mean_squared_error(final_val_targets, final_val_predictions))\n",
    "    final_r2 = r2_score(final_val_targets, final_val_predictions)\n",
    "\n",
    "    # Store R2 score as well in the study for later analysis\n",
    "    trial.set_user_attr(\"final_r2_score\", float(final_r2))\n",
    "\n",
    "    return final_rmse # Optuna minimizes this value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e4cec-523b-4256-9831-6625f53855f6",
   "metadata": {},
   "source": [
    "### 4.3. Run Optuna Study\n",
    "An Optuna study is created and executed to perform the hyperparameter optimization, iterating through various trials to find the best combination of GNN model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a66f01e-b7b3-4e2f-8704-f92e31c2d069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study for GNN will be stored at: sqlite:///..\\studies\\gnn_study\\gnn_optuna_study.db\n",
      "Loaded existing study 'gnn_regression_pGI50' from sqlite:///..\\studies\\gnn_study\\gnn_optuna_study.db. Resuming optimization.\n",
      "\n",
      "Starting Optuna optimization for GNN...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6a9db1c0a949cf972905a46d0659c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "   0%|          | 00:00/4:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-07-18 20:01:54,236] Trial 97 failed with parameters: {'hidden_channels': 959, 'lr': 0.000335866088243583, 'batch_size': 32, 'n_epochs': 295, 'num_layers': 3, 'dropout_rate': 0.2061426548243178, 'weight_decay': 1.092019611038451e-08} because of the following error: NameError(\"name 'train_data_list' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Acer\\Desktop\\Projects for Data Science\\Drug Gi50 Value Prediction\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_28584\\1816650752.py\", line 13, in objective\n",
      "    if not train_data_list:\n",
      "           ^^^^^^^^^^^^^^^\n",
      "NameError: name 'train_data_list' is not defined\n",
      "[W 2025-07-18 20:01:54,261] Trial 97 failed with value None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study_dir = Path(\"../studies/gnn_study\")\n",
    "study_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "study_db_path = f\"sqlite:///{study_dir / 'gnn_optuna_study.db'}\"\n",
    "study_name = \"gnn_regression_pGI50\"\n",
    "print(f\"Optuna study for GNN will be stored at: {study_db_path}\")\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(\n",
    "    n_startup_trials=10,  # Run at least these many trials completely before starting to prune\n",
    "    n_warmup_steps=20,    # Don't prune trials until they've completed these many epochs\n",
    "    interval_steps=10     # Check for pruning every these many epochs\n",
    ")\n",
    "# pruner = None\n",
    "\n",
    "# Check if a study with the same name already exists in the database\n",
    "# If it does, load it to resume the optimization.\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=study_db_path)\n",
    "    print(f\"Loaded existing study '{study_name}' from {study_db_path}. Resuming optimization.\")\n",
    "except KeyError:\n",
    "    # If the study does not exist, create a new one\n",
    "    print(f\"Creating new study '{study_name}' at {study_db_path}.\")\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction=\"minimize\",\n",
    "        storage=study_db_path,\n",
    "        pruner=pruner\n",
    "    )\n",
    "\n",
    "print(\"\\nStarting Optuna optimization for GNN...\")\n",
    "# Run for 'n_trial' trials or 'timeout' seconds, whichever completes first\n",
    "study.optimize(objective, n_trials=None, timeout=14400, show_progress_bar=True)\n",
    "print(\"\\nOptuna optimization finished for GNN.\")\n",
    "\n",
    "# Print best trial results\n",
    "print(\"\\n--- Best Trial Results for GNN ---\")\n",
    "print(f\"Best trial number: {study.best_trial.number}\")\n",
    "print(f\"Best RMSE (Validation): {study.best_value:.4f}\")\n",
    "print(\"Best hyperparameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "if \"final_r2_score\" in study.best_trial.user_attrs:\n",
    "    print(f\"Best R2 Score (Validation): {study.best_trial.user_attrs['final_r2_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773cb25-a8fb-4d90-96d7-8745c16b01be",
   "metadata": {},
   "source": [
    "## 5. Train Final Model\n",
    "This section trains the final GNN model using the best hyperparameters identified by Optuna and saves it for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075db4a0",
   "metadata": {},
   "source": [
    "### 5.1. Load Graph Objects\n",
    "The necessary PyTorch Geometric graph objects are reloaded to ensure a fresh start for final model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0bb25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13119 training graphs.\n",
      "Loaded 2812 validation graphs.\n",
      "Loaded 2812 test graphs.\n"
     ]
    }
   ],
   "source": [
    "# Directory where the graph objects are saved\n",
    "load_dir = Path(\"../data/pyg_data_graphs\")\n",
    "\n",
    "# Define the full file paths\n",
    "train_data_path = load_dir / \"train_data_list.pt\"\n",
    "val_data_path = load_dir / \"val_data_list.pt\"\n",
    "test_data_path = load_dir / \"test_data_list.pt\"\n",
    "\n",
    "# Load the lists of Data objects\n",
    "try:\n",
    "    train_data_list = torch.load(train_data_path, weights_only=False)\n",
    "    val_data_list = torch.load(val_data_path, weights_only=False)\n",
    "    test_data_list = torch.load(test_data_path, weights_only=False)\n",
    "\n",
    "    print(f\"Loaded {len(train_data_list)} training graphs.\")\n",
    "    print(f\"Loaded {len(val_data_list)} validation graphs.\")\n",
    "    print(f\"Loaded {len(test_data_list)} test graphs.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"Error: Processed data not found in {load_dir}. Please run the data processing and saving step first.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed54de",
   "metadata": {},
   "source": [
    "#### 5.1.1. Verify Loading of Graph Objects\n",
    "Basic checks are performed to ensure the graph objects have been loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744aa7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\n",
      "Data(x=[25, 21], edge_index=[2, 58], y=[1], global_features=[1, 4532], molregno=2307646, smiles='COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C')\n",
      "  Number of nodes (atoms): 25\n",
      "  Number of edges (bonds): 58\n",
      "\n",
      "  Node features (data.x) shape: torch.Size([25, 21])\n",
      "    Node features (data.x) sample (first 5 values): [6.0, 1.0, 4.0, 0.0, 0.0]\n",
      "    Node features (data.x) min: -0.4928\n",
      "    Node features (data.x) max: 8.0000\n",
      "    Node features (data.x) mean: 1.2363\n",
      "    Node features (data.x) std: 1.7344\n",
      "    Contains NaN in data.x: False\n",
      "    Contains Inf in data.x: False\n",
      "  Edge index (data.edge_index) shape: torch.Size([2, 58])\n",
      "  Target (data.y): 5.7347\n",
      "\n",
      "  Global features (data.global_features) shape: torch.Size([1, 4532])\n",
      "    Global features (data.global_features) sample (first 5 values): [0.038460150361061096, 0.038460150361061096, -2.2056941986083984, -2.2056941986083984, -2.2056941986083984]\n",
      "    Global features (data.global_features) min: -2.2057\n",
      "    Global features (data.global_features) max: 16.9898\n",
      "    Global features (data.global_features) mean: -0.0328\n",
      "    Global features (data.global_features) std: 0.9213\n",
      "    Contains NaN in global_features: False\n",
      "    Contains Inf in global_features: False\n",
      "\n",
      "  SMILES: COc1cccc2c1OCc1c-2nc2cnc3ccccc3c2c1C\n",
      "  Molregno: 2307646\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample PyTorch Geometric Data object (from train_data_list[0]) ---\")\n",
    "if len(train_data_list) > 0:\n",
    "    sample_data = train_data_list[0]\n",
    "    print(sample_data)\n",
    "    print(f\"  Number of nodes (atoms): {sample_data.num_nodes}\")\n",
    "    print(f\"  Number of edges (bonds): {sample_data.num_edges}\")\n",
    "    \n",
    "    # Node features (data.x) details\n",
    "    print(f\"\\n  Node features (data.x) shape: {sample_data.x.shape}\")\n",
    "    if sample_data.x.numel() > 0:\n",
    "        print(f\"    Node features (data.x) sample (first 5 values): {sample_data.x.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Node features (data.x) min: {sample_data.x.min().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) max: {sample_data.x.max().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) mean: {sample_data.x.float().mean().item():.4f}\")\n",
    "        print(f\"    Node features (data.x) std: {sample_data.x.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in data.x: {torch.isnan(sample_data.x).any().item()}\")\n",
    "        print(f\"    Contains Inf in data.x: {torch.isinf(sample_data.x).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Node features (data.x) is an empty tensor.\")\n",
    "\n",
    "    print(f\"  Edge index (data.edge_index) shape: {sample_data.edge_index.shape}\")\n",
    "    print(f\"  Target (data.y): {sample_data.y.item():.4f}\") # Display target with 4 decimal places\n",
    "    \n",
    "    # Global features (data.global_features) details (VERIFYING SCALING HERE)\n",
    "    print(f\"\\n  Global features (data.global_features) shape: {sample_data.global_features.shape}\")\n",
    "    if sample_data.global_features.numel() > 0:\n",
    "        print(f\"    Global features (data.global_features) sample (first 5 values): {sample_data.global_features.flatten()[:5].tolist()}\")\n",
    "        print(f\"    Global features (data.global_features) min: {sample_data.global_features.min().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) max: {sample_data.global_features.max().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) mean: {sample_data.global_features.float().mean().item():.4f}\")\n",
    "        print(f\"    Global features (data.global_features) std: {sample_data.global_features.float().std().item():.4f}\")\n",
    "        print(f\"    Contains NaN in global_features: {torch.isnan(sample_data.global_features).any().item()}\")\n",
    "        print(f\"    Contains Inf in global_features: {torch.isinf(sample_data.global_features).any().item()}\")\n",
    "    else:\n",
    "        print(\"    Global features (data.global_features) is an empty tensor.\")\n",
    "        \n",
    "    print(f\"\\n  SMILES: {sample_data.smiles}\")\n",
    "    print(f\"  Molregno: {sample_data.molregno}\")\n",
    "else:\n",
    "    print(\"No training data objects created to display sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19444a-e710-425d-ac80-0f42e0fd3ef6",
   "metadata": {},
   "source": [
    "### 5.2. Reinitialize Everything with Best Hyperparameters\n",
    "The GNN model is reinitialized with the optimal hyperparameters found during the Optuna study, along with the final training (training and validation data **combined**) and testing data, and their respective DataLoaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31078f14-daa7-4943-8423-dba53f579e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial parameters (GNN): {'hidden_channels': 999, 'lr': 0.0002654886343578734, 'batch_size': 128, 'n_epochs': 190, 'num_layers': 1, 'dropout_rate': 0.12998376396007172, 'weight_decay': 4.855663649252953e-08}\n",
      "Best hyperparameters from Optuna: {'hidden_channels': 999, 'lr': 0.0002654886343578734, 'batch_size': 128, 'n_epochs': 190, 'num_layers': 1, 'dropout_rate': 0.12998376396007172, 'weight_decay': 4.855663649252953e-08}\n",
      "Final GNN model, criterion, optimizer, and DataLoaders initialized with best parameters.\n",
      "Training on combined 15931 samples, testing on 2812 samples.\n"
     ]
    }
   ],
   "source": [
    "# Re-load the study to ensure the latest best parameters\n",
    "study_dir = Path(\"../studies/gnn_study\")\n",
    "study_db_path = f\"sqlite:///{study_dir / 'gnn_optuna_study.db'}\"\n",
    "study_name = \"gnn_regression_pGI50\"\n",
    "\n",
    "try:\n",
    "    study = optuna.load_study(study_name=study_name, storage=study_db_path)\n",
    "    print(\"Best trial parameters (GNN):\", study.best_trial.params)\n",
    "    best_params = study.best_trial.params\n",
    "except KeyError:\n",
    "    print(f\"Study '{study_name}' does not exist at {study_db_path}. Please make sure the GNN Optuna study cell has been run.\")\n",
    "\n",
    "best_hidden_channels = best_params[\"hidden_channels\"]\n",
    "best_learning_rate = best_params[\"lr\"]\n",
    "best_batch_size = best_params[\"batch_size\"]\n",
    "best_n_epochs = best_params[\"n_epochs\"]\n",
    "best_num_layers = best_params[\"num_layers\"]\n",
    "best_dropout_rate = best_params[\"dropout_rate\"]\n",
    "best_weight_decay = best_params[\"weight_decay\"]\n",
    "\n",
    "print(f\"Best hyperparameters from Optuna: {best_params}\")\n",
    "\n",
    "# Re-initialize the model with best hyperparameters\n",
    "if not train_data_list:\n",
    "    raise ValueError(\"train_data_list is empty. Cannot determine feature dimensions for GNN.\")\n",
    "\n",
    "node_feature_dim = train_data_list[0].x.shape[1]\n",
    "global_feature_dim = train_data_list[0].global_features.shape[1]\n",
    "\n",
    "final_gnn_model = GNN(\n",
    "    node_feature_dim=node_feature_dim,\n",
    "    global_feature_dim=global_feature_dim,\n",
    "    hidden_channels=best_hidden_channels,\n",
    "    num_layers=best_num_layers,\n",
    "    dropout_rate=best_dropout_rate\n",
    ").to(device)\n",
    "\n",
    "# Re-initialize criterion and optimizer\n",
    "final_criterion = nn.MSELoss()\n",
    "final_optimizer = optim.Adam(final_gnn_model.parameters(), lr=best_learning_rate, weight_decay=best_weight_decay)\n",
    "\n",
    "# Re-create DataLoaders with the best batch size (Training + Validation data COMBINED)\n",
    "final_train_val_data_list = train_data_list + val_data_list\n",
    "\n",
    "# Create DataLoaders with the best batch size\n",
    "num_workers = 0\n",
    "final_train_val_loader = PyGDataLoader(final_train_val_data_list, batch_size=best_batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Create the FINAL TEST DataLoader\n",
    "final_test_loader = PyGDataLoader(test_data_list, batch_size=best_batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"Final GNN model, criterion, optimizer, and DataLoaders initialized with best parameters.\")\n",
    "print(f\"Training on combined {len(final_train_val_data_list)} samples, testing on {len(test_data_list)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e421910-603a-469c-b7fe-133619e5eedc",
   "metadata": {},
   "source": [
    "### 5.3. Get Current Commit ID\n",
    "The current Git commit ID (hash) is programmatically retrieved. This commit ID will be incorporated into the final model's filename to ensure direct traceability and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "863c9398-d629-407f-a118-575c9d400651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_git_commit_hash():\n",
    "    try:\n",
    "        # Get the short commit hash\n",
    "        commit_hash = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD']).strip().decode('ascii')\n",
    "        return commit_hash\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return \"unknown_commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa70146-1e6a-44ce-84a8-dca9bcb04dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Git Commit ID: 271d2f4\n"
     ]
    }
   ],
   "source": [
    "# Optionally, see the current commit ID\n",
    "current_commit = get_git_commit_hash()\n",
    "print(f\"Current Git Commit ID: {current_commit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ed59c-cb77-4821-8773-4936603ac335",
   "metadata": {},
   "source": [
    "### 5.4. Train and Save Model\n",
    "The final GNN model is trained on the combined training and validation graph datasets and then saved locally with a filename that includes the Git commit ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc661f62-1bc3-47d7-9d83-41d64ff14e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining final GNN model for 190 epochs with best parameters...\n",
      "Associated Git Commit ID for saved model: 271d2f4\n",
      "Epoch 1/190, Train Loss: 2.0786, Eval RMSE on combined data: 0.7288, Time: 5.49s\n",
      "--- New best final GNN model saved at epoch 1 with RMSE: 0.7288 ---\n",
      "Epoch 2/190, Train Loss: 0.5506, Eval RMSE on combined data: 0.6565, Time: 5.28s\n",
      "--- New best final GNN model saved at epoch 2 with RMSE: 0.6565 ---\n",
      "Epoch 3/190, Train Loss: 0.4511, Eval RMSE on combined data: 0.5760, Time: 5.31s\n",
      "--- New best final GNN model saved at epoch 3 with RMSE: 0.5760 ---\n",
      "Epoch 4/190, Train Loss: 0.3592, Eval RMSE on combined data: 0.5163, Time: 5.25s\n",
      "--- New best final GNN model saved at epoch 4 with RMSE: 0.5163 ---\n",
      "Epoch 5/190, Train Loss: 0.2898, Eval RMSE on combined data: 0.4665, Time: 5.26s\n",
      "--- New best final GNN model saved at epoch 5 with RMSE: 0.4665 ---\n",
      "Epoch 6/190, Train Loss: 0.2443, Eval RMSE on combined data: 0.4380, Time: 5.23s\n",
      "--- New best final GNN model saved at epoch 6 with RMSE: 0.4380 ---\n",
      "Epoch 7/190, Train Loss: 0.2088, Eval RMSE on combined data: 0.4194, Time: 5.26s\n",
      "--- New best final GNN model saved at epoch 7 with RMSE: 0.4194 ---\n",
      "Epoch 8/190, Train Loss: 0.1873, Eval RMSE on combined data: 0.3901, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 8 with RMSE: 0.3901 ---\n",
      "Epoch 9/190, Train Loss: 0.1573, Eval RMSE on combined data: 0.3580, Time: 5.31s\n",
      "--- New best final GNN model saved at epoch 9 with RMSE: 0.3580 ---\n",
      "Epoch 10/190, Train Loss: 0.1475, Eval RMSE on combined data: 0.3388, Time: 5.25s\n",
      "--- New best final GNN model saved at epoch 10 with RMSE: 0.3388 ---\n",
      "Epoch 11/190, Train Loss: 0.1321, Eval RMSE on combined data: 0.3360, Time: 5.30s\n",
      "--- New best final GNN model saved at epoch 11 with RMSE: 0.3360 ---\n",
      "Epoch 12/190, Train Loss: 0.1251, Eval RMSE on combined data: 0.3491, Time: 5.23s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.3360\n",
      "Epoch 13/190, Train Loss: 0.1176, Eval RMSE on combined data: 0.3028, Time: 5.31s\n",
      "--- New best final GNN model saved at epoch 13 with RMSE: 0.3028 ---\n",
      "Epoch 14/190, Train Loss: 0.1047, Eval RMSE on combined data: 0.3119, Time: 5.22s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.3028\n",
      "Epoch 15/190, Train Loss: 0.0996, Eval RMSE on combined data: 0.3406, Time: 5.25s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.3028\n",
      "Epoch 16/190, Train Loss: 0.0940, Eval RMSE on combined data: 0.2665, Time: 5.25s\n",
      "--- New best final GNN model saved at epoch 16 with RMSE: 0.2665 ---\n",
      "Epoch 17/190, Train Loss: 0.0785, Eval RMSE on combined data: 0.2625, Time: 5.29s\n",
      "--- New best final GNN model saved at epoch 17 with RMSE: 0.2625 ---\n",
      "Epoch 18/190, Train Loss: 0.0802, Eval RMSE on combined data: 0.2483, Time: 5.24s\n",
      "--- New best final GNN model saved at epoch 18 with RMSE: 0.2483 ---\n",
      "Epoch 19/190, Train Loss: 0.0736, Eval RMSE on combined data: 0.2556, Time: 5.25s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2483\n",
      "Epoch 20/190, Train Loss: 0.0730, Eval RMSE on combined data: 0.2412, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 20 with RMSE: 0.2412 ---\n",
      "Epoch 21/190, Train Loss: 0.0761, Eval RMSE on combined data: 0.2623, Time: 5.21s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2412\n",
      "Epoch 22/190, Train Loss: 0.0752, Eval RMSE on combined data: 0.2678, Time: 5.24s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2412\n",
      "Epoch 23/190, Train Loss: 0.0705, Eval RMSE on combined data: 0.2279, Time: 5.26s\n",
      "--- New best final GNN model saved at epoch 23 with RMSE: 0.2279 ---\n",
      "Epoch 24/190, Train Loss: 0.0684, Eval RMSE on combined data: 0.2399, Time: 5.36s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2279\n",
      "Epoch 25/190, Train Loss: 0.0683, Eval RMSE on combined data: 0.2485, Time: 5.15s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2279\n",
      "Epoch 26/190, Train Loss: 0.0647, Eval RMSE on combined data: 0.2276, Time: 5.13s\n",
      "--- New best final GNN model saved at epoch 26 with RMSE: 0.2276 ---\n",
      "Epoch 27/190, Train Loss: 0.0635, Eval RMSE on combined data: 0.2360, Time: 5.20s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2276\n",
      "Epoch 28/190, Train Loss: 0.0656, Eval RMSE on combined data: 0.2271, Time: 5.26s\n",
      "--- New best final GNN model saved at epoch 28 with RMSE: 0.2271 ---\n",
      "Epoch 29/190, Train Loss: 0.0578, Eval RMSE on combined data: 0.2273, Time: 5.21s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2271\n",
      "Epoch 30/190, Train Loss: 0.0587, Eval RMSE on combined data: 0.2151, Time: 5.28s\n",
      "--- New best final GNN model saved at epoch 30 with RMSE: 0.2151 ---\n",
      "Epoch 31/190, Train Loss: 0.0567, Eval RMSE on combined data: 0.2175, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2151\n",
      "Epoch 32/190, Train Loss: 0.0571, Eval RMSE on combined data: 0.2178, Time: 5.25s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2151\n",
      "Epoch 33/190, Train Loss: 0.0535, Eval RMSE on combined data: 0.2185, Time: 5.24s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.2151\n",
      "Epoch 34/190, Train Loss: 0.0598, Eval RMSE on combined data: 0.2136, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 34 with RMSE: 0.2136 ---\n",
      "Epoch 35/190, Train Loss: 0.0594, Eval RMSE on combined data: 0.2239, Time: 5.21s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.2136\n",
      "Epoch 36/190, Train Loss: 0.0563, Eval RMSE on combined data: 0.2287, Time: 5.25s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.2136\n",
      "Epoch 37/190, Train Loss: 0.0510, Eval RMSE on combined data: 0.2163, Time: 5.36s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.2136\n",
      "Epoch 38/190, Train Loss: 0.0469, Eval RMSE on combined data: 0.2457, Time: 5.18s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.2136\n",
      "Epoch 39/190, Train Loss: 0.0513, Eval RMSE on combined data: 0.1922, Time: 5.19s\n",
      "--- New best final GNN model saved at epoch 39 with RMSE: 0.1922 ---\n",
      "Epoch 40/190, Train Loss: 0.0509, Eval RMSE on combined data: 0.2118, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1922\n",
      "Epoch 41/190, Train Loss: 0.0490, Eval RMSE on combined data: 0.1918, Time: 5.18s\n",
      "--- New best final GNN model saved at epoch 41 with RMSE: 0.1918 ---\n",
      "Epoch 42/190, Train Loss: 0.0497, Eval RMSE on combined data: 0.2024, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 43/190, Train Loss: 0.0473, Eval RMSE on combined data: 0.2047, Time: 5.23s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 44/190, Train Loss: 0.0494, Eval RMSE on combined data: 0.1976, Time: 5.23s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 45/190, Train Loss: 0.0451, Eval RMSE on combined data: 0.2235, Time: 5.35s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 46/190, Train Loss: 0.0445, Eval RMSE on combined data: 0.1975, Time: 5.24s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 47/190, Train Loss: 0.0418, Eval RMSE on combined data: 0.2367, Time: 5.19s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 48/190, Train Loss: 0.0428, Eval RMSE on combined data: 0.2130, Time: 5.27s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1918\n",
      "Epoch 49/190, Train Loss: 0.0449, Eval RMSE on combined data: 0.1830, Time: 5.28s\n",
      "--- New best final GNN model saved at epoch 49 with RMSE: 0.1830 ---\n",
      "Epoch 50/190, Train Loss: 0.0415, Eval RMSE on combined data: 0.1860, Time: 5.27s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1830\n",
      "Epoch 51/190, Train Loss: 0.0421, Eval RMSE on combined data: 0.1924, Time: 5.26s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1830\n",
      "Epoch 52/190, Train Loss: 0.0432, Eval RMSE on combined data: 0.2530, Time: 5.28s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1830\n",
      "Epoch 53/190, Train Loss: 0.0413, Eval RMSE on combined data: 0.1810, Time: 5.36s\n",
      "--- New best final GNN model saved at epoch 53 with RMSE: 0.1810 ---\n",
      "Epoch 54/190, Train Loss: 0.0420, Eval RMSE on combined data: 0.2102, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1810\n",
      "Epoch 55/190, Train Loss: 0.0472, Eval RMSE on combined data: 0.2054, Time: 5.29s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1810\n",
      "Epoch 56/190, Train Loss: 0.0420, Eval RMSE on combined data: 0.1844, Time: 5.23s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1810\n",
      "Epoch 57/190, Train Loss: 0.0411, Eval RMSE on combined data: 0.1790, Time: 5.27s\n",
      "--- New best final GNN model saved at epoch 57 with RMSE: 0.1790 ---\n",
      "Epoch 58/190, Train Loss: 0.0420, Eval RMSE on combined data: 0.2392, Time: 5.24s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1790\n",
      "Epoch 59/190, Train Loss: 0.0415, Eval RMSE on combined data: 0.2041, Time: 5.39s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1790\n",
      "Epoch 60/190, Train Loss: 0.0402, Eval RMSE on combined data: 0.1770, Time: 5.20s\n",
      "--- New best final GNN model saved at epoch 60 with RMSE: 0.1770 ---\n",
      "Epoch 61/190, Train Loss: 0.0391, Eval RMSE on combined data: 0.1865, Time: 5.23s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1770\n",
      "Epoch 62/190, Train Loss: 0.0389, Eval RMSE on combined data: 0.1852, Time: 5.25s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1770\n",
      "Epoch 63/190, Train Loss: 0.0417, Eval RMSE on combined data: 0.1909, Time: 5.19s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1770\n",
      "Epoch 64/190, Train Loss: 0.0421, Eval RMSE on combined data: 0.2106, Time: 5.24s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1770\n",
      "Epoch 65/190, Train Loss: 0.0390, Eval RMSE on combined data: 0.1717, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 65 with RMSE: 0.1717 ---\n",
      "Epoch 66/190, Train Loss: 0.0346, Eval RMSE on combined data: 0.2088, Time: 5.27s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 67/190, Train Loss: 0.0359, Eval RMSE on combined data: 0.1785, Time: 5.18s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 68/190, Train Loss: 0.0396, Eval RMSE on combined data: 0.2076, Time: 5.21s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 69/190, Train Loss: 0.0382, Eval RMSE on combined data: 0.1929, Time: 5.15s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 70/190, Train Loss: 0.0387, Eval RMSE on combined data: 0.1752, Time: 5.21s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 71/190, Train Loss: 0.0369, Eval RMSE on combined data: 0.2039, Time: 5.16s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 72/190, Train Loss: 0.0374, Eval RMSE on combined data: 0.1794, Time: 5.20s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 73/190, Train Loss: 0.0370, Eval RMSE on combined data: 0.2252, Time: 5.25s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 74/190, Train Loss: 0.0370, Eval RMSE on combined data: 0.1766, Time: 5.24s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 75/190, Train Loss: 0.0346, Eval RMSE on combined data: 0.1783, Time: 5.23s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 76/190, Train Loss: 0.0362, Eval RMSE on combined data: 0.1809, Time: 5.28s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 77/190, Train Loss: 0.0338, Eval RMSE on combined data: 0.1822, Time: 5.31s\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 78/190, Train Loss: 0.0322, Eval RMSE on combined data: 0.1765, Time: 5.28s\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 79/190, Train Loss: 0.0341, Eval RMSE on combined data: 0.1866, Time: 5.28s\n",
      "No improvement for 14 epochs. Best RMSE so far: 0.1717\n",
      "Epoch 80/190, Train Loss: 0.0364, Eval RMSE on combined data: 0.1707, Time: 5.20s\n",
      "--- New best final GNN model saved at epoch 80 with RMSE: 0.1707 ---\n",
      "Epoch 81/190, Train Loss: 0.0338, Eval RMSE on combined data: 0.1663, Time: 5.30s\n",
      "--- New best final GNN model saved at epoch 81 with RMSE: 0.1663 ---\n",
      "Epoch 82/190, Train Loss: 0.0326, Eval RMSE on combined data: 0.2065, Time: 5.25s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1663\n",
      "Epoch 83/190, Train Loss: 0.0326, Eval RMSE on combined data: 0.1697, Time: 5.20s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1663\n",
      "Epoch 84/190, Train Loss: 0.0328, Eval RMSE on combined data: 0.1730, Time: 5.20s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1663\n",
      "Epoch 85/190, Train Loss: 0.0347, Eval RMSE on combined data: 0.1917, Time: 5.24s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1663\n",
      "Epoch 86/190, Train Loss: 0.0360, Eval RMSE on combined data: 0.1611, Time: 5.13s\n",
      "--- New best final GNN model saved at epoch 86 with RMSE: 0.1611 ---\n",
      "Epoch 87/190, Train Loss: 0.0373, Eval RMSE on combined data: 0.1752, Time: 5.19s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 88/190, Train Loss: 0.0388, Eval RMSE on combined data: 0.2025, Time: 5.16s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 89/190, Train Loss: 0.0361, Eval RMSE on combined data: 0.1634, Time: 5.44s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 90/190, Train Loss: 0.0342, Eval RMSE on combined data: 0.1653, Time: 5.21s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 91/190, Train Loss: 0.0351, Eval RMSE on combined data: 0.1834, Time: 5.20s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 92/190, Train Loss: 0.0328, Eval RMSE on combined data: 0.1691, Time: 5.28s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 93/190, Train Loss: 0.0303, Eval RMSE on combined data: 0.1812, Time: 5.38s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1611\n",
      "Epoch 94/190, Train Loss: 0.0337, Eval RMSE on combined data: 0.1532, Time: 5.37s\n",
      "--- New best final GNN model saved at epoch 94 with RMSE: 0.1532 ---\n",
      "Epoch 95/190, Train Loss: 0.0331, Eval RMSE on combined data: 0.1778, Time: 5.41s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 96/190, Train Loss: 0.0317, Eval RMSE on combined data: 0.1557, Time: 5.23s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 97/190, Train Loss: 0.0299, Eval RMSE on combined data: 0.1576, Time: 5.39s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 98/190, Train Loss: 0.0310, Eval RMSE on combined data: 0.1727, Time: 5.33s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 99/190, Train Loss: 0.0332, Eval RMSE on combined data: 0.1926, Time: 5.38s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 100/190, Train Loss: 0.0339, Eval RMSE on combined data: 0.1796, Time: 5.29s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 101/190, Train Loss: 0.0332, Eval RMSE on combined data: 0.1595, Time: 5.33s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 102/190, Train Loss: 0.0310, Eval RMSE on combined data: 0.1588, Time: 5.32s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 103/190, Train Loss: 0.0298, Eval RMSE on combined data: 0.1646, Time: 5.27s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 104/190, Train Loss: 0.0307, Eval RMSE on combined data: 0.1546, Time: 5.24s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 105/190, Train Loss: 0.0285, Eval RMSE on combined data: 0.1678, Time: 5.34s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 106/190, Train Loss: 0.0273, Eval RMSE on combined data: 0.1821, Time: 5.26s\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 107/190, Train Loss: 0.0320, Eval RMSE on combined data: 0.1580, Time: 5.26s\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 108/190, Train Loss: 0.0288, Eval RMSE on combined data: 0.1756, Time: 5.65s\n",
      "No improvement for 14 epochs. Best RMSE so far: 0.1532\n",
      "Epoch 109/190, Train Loss: 0.0264, Eval RMSE on combined data: 0.1471, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 109 with RMSE: 0.1471 ---\n",
      "Epoch 110/190, Train Loss: 0.0296, Eval RMSE on combined data: 0.1517, Time: 5.32s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 111/190, Train Loss: 0.0300, Eval RMSE on combined data: 0.1728, Time: 5.22s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 112/190, Train Loss: 0.0281, Eval RMSE on combined data: 0.1561, Time: 5.27s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 113/190, Train Loss: 0.0291, Eval RMSE on combined data: 0.1686, Time: 5.20s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 114/190, Train Loss: 0.0305, Eval RMSE on combined data: 0.1699, Time: 5.27s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 115/190, Train Loss: 0.0310, Eval RMSE on combined data: 0.1544, Time: 5.22s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 116/190, Train Loss: 0.0298, Eval RMSE on combined data: 0.1759, Time: 5.21s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1471\n",
      "Epoch 117/190, Train Loss: 0.0290, Eval RMSE on combined data: 0.1398, Time: 5.24s\n",
      "--- New best final GNN model saved at epoch 117 with RMSE: 0.1398 ---\n",
      "Epoch 118/190, Train Loss: 0.0288, Eval RMSE on combined data: 0.1867, Time: 5.29s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 119/190, Train Loss: 0.0304, Eval RMSE on combined data: 0.1645, Time: 5.31s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 120/190, Train Loss: 0.0293, Eval RMSE on combined data: 0.1483, Time: 5.28s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 121/190, Train Loss: 0.0273, Eval RMSE on combined data: 0.1455, Time: 5.21s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 122/190, Train Loss: 0.0279, Eval RMSE on combined data: 0.1616, Time: 5.34s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 123/190, Train Loss: 0.0292, Eval RMSE on combined data: 0.1573, Time: 5.24s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 124/190, Train Loss: 0.0271, Eval RMSE on combined data: 0.1614, Time: 5.31s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 125/190, Train Loss: 0.0296, Eval RMSE on combined data: 0.1557, Time: 5.25s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 126/190, Train Loss: 0.0291, Eval RMSE on combined data: 0.1652, Time: 5.35s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 127/190, Train Loss: 0.0274, Eval RMSE on combined data: 0.1514, Time: 5.32s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 128/190, Train Loss: 0.0267, Eval RMSE on combined data: 0.1593, Time: 5.28s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 129/190, Train Loss: 0.0291, Eval RMSE on combined data: 0.1550, Time: 5.18s\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 130/190, Train Loss: 0.0269, Eval RMSE on combined data: 0.1660, Time: 5.22s\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 131/190, Train Loss: 0.0268, Eval RMSE on combined data: 0.1544, Time: 5.21s\n",
      "No improvement for 14 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 132/190, Train Loss: 0.0294, Eval RMSE on combined data: 0.1471, Time: 5.20s\n",
      "No improvement for 15 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 133/190, Train Loss: 0.0268, Eval RMSE on combined data: 0.1524, Time: 5.23s\n",
      "No improvement for 16 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 134/190, Train Loss: 0.0270, Eval RMSE on combined data: 0.1577, Time: 5.26s\n",
      "No improvement for 17 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 135/190, Train Loss: 0.0268, Eval RMSE on combined data: 0.1533, Time: 5.25s\n",
      "No improvement for 18 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 136/190, Train Loss: 0.0257, Eval RMSE on combined data: 0.1479, Time: 5.28s\n",
      "No improvement for 19 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 137/190, Train Loss: 0.0246, Eval RMSE on combined data: 0.1462, Time: 5.21s\n",
      "No improvement for 20 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 138/190, Train Loss: 0.0278, Eval RMSE on combined data: 0.1439, Time: 5.34s\n",
      "No improvement for 21 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 139/190, Train Loss: 0.0254, Eval RMSE on combined data: 0.1558, Time: 5.68s\n",
      "No improvement for 22 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 140/190, Train Loss: 0.0265, Eval RMSE on combined data: 0.1470, Time: 5.24s\n",
      "No improvement for 23 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 141/190, Train Loss: 0.0265, Eval RMSE on combined data: 0.1546, Time: 5.44s\n",
      "No improvement for 24 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 142/190, Train Loss: 0.0259, Eval RMSE on combined data: 0.1482, Time: 5.27s\n",
      "No improvement for 25 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 143/190, Train Loss: 0.0258, Eval RMSE on combined data: 0.1469, Time: 5.33s\n",
      "No improvement for 26 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 144/190, Train Loss: 0.0248, Eval RMSE on combined data: 0.1528, Time: 5.29s\n",
      "No improvement for 27 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 145/190, Train Loss: 0.0248, Eval RMSE on combined data: 0.1568, Time: 5.36s\n",
      "No improvement for 28 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 146/190, Train Loss: 0.0273, Eval RMSE on combined data: 0.1485, Time: 5.23s\n",
      "No improvement for 29 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 147/190, Train Loss: 0.0258, Eval RMSE on combined data: 0.1520, Time: 5.26s\n",
      "No improvement for 30 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 148/190, Train Loss: 0.0266, Eval RMSE on combined data: 0.1779, Time: 5.22s\n",
      "No improvement for 31 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 149/190, Train Loss: 0.0269, Eval RMSE on combined data: 0.1590, Time: 5.24s\n",
      "No improvement for 32 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 150/190, Train Loss: 0.0254, Eval RMSE on combined data: 0.1461, Time: 5.26s\n",
      "No improvement for 33 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 151/190, Train Loss: 0.0251, Eval RMSE on combined data: 0.1497, Time: 5.21s\n",
      "No improvement for 34 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 152/190, Train Loss: 0.0275, Eval RMSE on combined data: 0.1657, Time: 5.20s\n",
      "No improvement for 35 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 153/190, Train Loss: 0.0251, Eval RMSE on combined data: 0.1595, Time: 5.39s\n",
      "No improvement for 36 epochs. Best RMSE so far: 0.1398\n",
      "Epoch 154/190, Train Loss: 0.0241, Eval RMSE on combined data: 0.1377, Time: 5.25s\n",
      "--- New best final GNN model saved at epoch 154 with RMSE: 0.1377 ---\n",
      "Epoch 155/190, Train Loss: 0.0248, Eval RMSE on combined data: 0.1570, Time: 5.33s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 156/190, Train Loss: 0.0246, Eval RMSE on combined data: 0.1560, Time: 5.27s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 157/190, Train Loss: 0.0255, Eval RMSE on combined data: 0.1385, Time: 5.30s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 158/190, Train Loss: 0.0247, Eval RMSE on combined data: 0.1515, Time: 5.29s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 159/190, Train Loss: 0.0254, Eval RMSE on combined data: 0.1688, Time: 5.39s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 160/190, Train Loss: 0.0223, Eval RMSE on combined data: 0.1580, Time: 5.24s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1377\n",
      "Epoch 161/190, Train Loss: 0.0242, Eval RMSE on combined data: 0.1351, Time: 5.22s\n",
      "--- New best final GNN model saved at epoch 161 with RMSE: 0.1351 ---\n",
      "Epoch 162/190, Train Loss: 0.0238, Eval RMSE on combined data: 0.1439, Time: 5.25s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1351\n",
      "Epoch 163/190, Train Loss: 0.0240, Eval RMSE on combined data: 0.1457, Time: 5.33s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1351\n",
      "Epoch 164/190, Train Loss: 0.0255, Eval RMSE on combined data: 0.1727, Time: 5.26s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1351\n",
      "Epoch 165/190, Train Loss: 0.0265, Eval RMSE on combined data: 0.1341, Time: 5.34s\n",
      "--- New best final GNN model saved at epoch 165 with RMSE: 0.1341 ---\n",
      "Epoch 166/190, Train Loss: 0.0234, Eval RMSE on combined data: 0.1473, Time: 5.27s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 167/190, Train Loss: 0.0231, Eval RMSE on combined data: 0.1392, Time: 5.34s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 168/190, Train Loss: 0.0224, Eval RMSE on combined data: 0.1425, Time: 5.33s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 169/190, Train Loss: 0.0244, Eval RMSE on combined data: 0.1692, Time: 5.27s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 170/190, Train Loss: 0.0242, Eval RMSE on combined data: 0.1497, Time: 5.22s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 171/190, Train Loss: 0.0218, Eval RMSE on combined data: 0.1501, Time: 5.27s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 172/190, Train Loss: 0.0217, Eval RMSE on combined data: 0.1494, Time: 5.24s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 173/190, Train Loss: 0.0217, Eval RMSE on combined data: 0.1546, Time: 5.30s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 174/190, Train Loss: 0.0229, Eval RMSE on combined data: 0.1429, Time: 5.26s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 175/190, Train Loss: 0.0228, Eval RMSE on combined data: 0.1407, Time: 5.31s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 176/190, Train Loss: 0.0239, Eval RMSE on combined data: 0.1485, Time: 5.15s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 177/190, Train Loss: 0.0244, Eval RMSE on combined data: 0.1430, Time: 5.24s\n",
      "No improvement for 12 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 178/190, Train Loss: 0.0251, Eval RMSE on combined data: 0.1466, Time: 5.25s\n",
      "No improvement for 13 epochs. Best RMSE so far: 0.1341\n",
      "Epoch 179/190, Train Loss: 0.0228, Eval RMSE on combined data: 0.1321, Time: 5.31s\n",
      "--- New best final GNN model saved at epoch 179 with RMSE: 0.1321 ---\n",
      "Epoch 180/190, Train Loss: 0.0212, Eval RMSE on combined data: 0.1386, Time: 5.28s\n",
      "No improvement for 1 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 181/190, Train Loss: 0.0208, Eval RMSE on combined data: 0.1373, Time: 5.29s\n",
      "No improvement for 2 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 182/190, Train Loss: 0.0238, Eval RMSE on combined data: 0.1519, Time: 5.27s\n",
      "No improvement for 3 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 183/190, Train Loss: 0.0225, Eval RMSE on combined data: 0.1480, Time: 5.23s\n",
      "No improvement for 4 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 184/190, Train Loss: 0.0234, Eval RMSE on combined data: 0.1454, Time: 5.21s\n",
      "No improvement for 5 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 185/190, Train Loss: 0.0229, Eval RMSE on combined data: 0.1382, Time: 5.22s\n",
      "No improvement for 6 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 186/190, Train Loss: 0.0237, Eval RMSE on combined data: 0.1436, Time: 5.25s\n",
      "No improvement for 7 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 187/190, Train Loss: 0.0234, Eval RMSE on combined data: 0.1430, Time: 5.26s\n",
      "No improvement for 8 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 188/190, Train Loss: 0.0216, Eval RMSE on combined data: 0.1441, Time: 5.22s\n",
      "No improvement for 9 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 189/190, Train Loss: 0.0233, Eval RMSE on combined data: 0.1747, Time: 5.25s\n",
      "No improvement for 10 epochs. Best RMSE so far: 0.1321\n",
      "Epoch 190/190, Train Loss: 0.0255, Eval RMSE on combined data: 0.1542, Time: 5.25s\n",
      "No improvement for 11 epochs. Best RMSE so far: 0.1321\n",
      "Final model training complete.\n"
     ]
    }
   ],
   "source": [
    "best_final_val_rmse = float('inf')\n",
    "patience_counter_final = 0\n",
    "final_patience = 50\n",
    "\n",
    "current_commit_hash = get_git_commit_hash()\n",
    "model_filename = f\"final_best_gnn_model_{current_commit_hash}.pt\" # Pre-define filename\n",
    "\n",
    "print(f\"Retraining final GNN model for {best_n_epochs} epochs with best parameters...\")\n",
    "print(f\"Associated Git Commit ID for saved model: {current_commit_hash}\")\n",
    "\n",
    "for epoch in range(best_n_epochs):\n",
    "    start_epoch_time = time.time()\n",
    "    \n",
    "    # Training\n",
    "    final_gnn_model.train()\n",
    "    total_train_loss = 0\n",
    "    num_train_batches = 0\n",
    "    for data_batch in final_train_val_loader:\n",
    "        # Move data to device\n",
    "        data_batch = data_batch.to(device)\n",
    "        \n",
    "        final_optimizer.zero_grad()\n",
    "        outputs = final_gnn_model(data_batch)\n",
    "        \n",
    "        # Ensure outputs and target are of same shape for loss calculation\n",
    "        loss = final_criterion(outputs.view(-1), data_batch.y.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_gnn_model.parameters(), max_norm=1.0)\n",
    "        final_optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "    avg_train_loss = total_train_loss / num_train_batches\n",
    "\n",
    "    # Evaluation on combined data\n",
    "    final_gnn_model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data_batch_eval in final_train_val_loader:\n",
    "            data_batch_eval = data_batch_eval.to(device)\n",
    "            \n",
    "            val_outputs = final_gnn_model(data_batch_eval)\n",
    "            val_predictions.extend(val_outputs.cpu().numpy().flatten())\n",
    "            val_targets.extend(data_batch_eval.y.cpu().numpy().flatten())\n",
    "\n",
    "    current_val_rmse = np.sqrt(mean_squared_error(val_targets, val_predictions))\n",
    "\n",
    "    if device.type == 'cuda': # Ensure GPU operations are finished before timing an epoch\n",
    "        torch.cuda.synchronize()\n",
    "    end_epoch_time = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{best_n_epochs}, Train Loss: {avg_train_loss:.4f}, Eval RMSE on combined data: {current_val_rmse:.4f}, Time: {end_epoch_time - start_epoch_time:.2f}s\")\n",
    "\n",
    "    # Dynamic Best Model Saving & Early Stopping\n",
    "    if current_val_rmse < best_final_val_rmse:\n",
    "        best_final_val_rmse = current_val_rmse\n",
    "        # Save the GNN model state dict\n",
    "        torch.save(final_gnn_model.state_dict(), gnn_models_base_dir / model_filename)\n",
    "        patience_counter_final = 0 # Reset patience counter if performance improved\n",
    "        print(f\"--- New best final GNN model saved at epoch {epoch+1} with RMSE: {current_val_rmse:.4f} ---\")\n",
    "    else:\n",
    "        patience_counter_final += 1 # Increment patience counter if no improvement\n",
    "        print(f\"No improvement for {patience_counter_final} epochs. Best RMSE so far: {best_final_val_rmse:.4f}\")\n",
    "\n",
    "    if patience_counter_final >= final_patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "        break\n",
    "\n",
    "print(\"Final model training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f489f4-d72a-4453-a196-7d2635b2d8d6",
   "metadata": {},
   "source": [
    "### 5.5. Evaluate Model\n",
    "This section performs a final, unbiased evaluation of the trained GNN model's performance on the previously unseen test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f551353f-28f6-4118-b459-651f371428d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best saved GNN model from '..\\models\\gnn\\final_best_gnn_model_271d2f4.pt' for final test evaluation...\n",
      "\n",
      "Starting final evaluation on test set for GNN...\n",
      "Final GNN Model Test RMSE: 0.6114\n",
      "Final GNN Model Test R2: 0.6100\n"
     ]
    }
   ],
   "source": [
    "# Load the best state dict model\n",
    "print(f\"Loading best saved GNN model from '{gnn_models_base_dir / model_filename}' for final test evaluation...\")\n",
    "path_to_saved_model = gnn_models_base_dir / model_filename\n",
    "loaded_model_state_dict = torch.load(path_to_saved_model)\n",
    "final_gnn_model.load_state_dict(loaded_model_state_dict)\n",
    "final_gnn_model.eval()\n",
    "\n",
    "print(\"\\nStarting final evaluation on test set for GNN...\")\n",
    "test_predictions = []\n",
    "test_targets = []\n",
    "with torch.no_grad():\n",
    "    for data_batch_test in final_test_loader:\n",
    "        data_batch_test = data_batch_test.to(device)\n",
    "\n",
    "        test_outputs = final_gnn_model(data_batch_test)\n",
    "\n",
    "        # Collect predictions and targets\n",
    "        test_predictions.extend(test_outputs.cpu().numpy().flatten())\n",
    "        test_targets.extend(data_batch_test.y.cpu().numpy().flatten())\n",
    "\n",
    "final_test_rmse = np.sqrt(mean_squared_error(test_targets, test_predictions))\n",
    "final_test_r2 = r2_score(test_targets, test_predictions)\n",
    "\n",
    "print(f\"Final GNN Model Test RMSE: {final_test_rmse:.4f}\")\n",
    "print(f\"Final GNN Model Test R2: {final_test_r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
